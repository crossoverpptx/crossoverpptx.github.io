<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>crossoverpptx&#39;s Blog</title>
  
  <subtitle>极度的痛苦才是精神的最后解放者。唯有此种痛苦，才迫使我们大彻如悟。</subtitle>
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2023-08-23T07:24:58.965Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>crossoverpptx</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>C/C++数据库编程</title>
    <link href="http://yoursite.com/2023/08/23/C-C-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%96%E7%A8%8B/"/>
    <id>http://yoursite.com/2023/08/23/C-C-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%96%E7%A8%8B/</id>
    <published>2023-08-23T07:14:44.000Z</published>
    <updated>2023-08-23T07:24:58.965Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0-Mysql安装与开发环境配置"><a href="#0-Mysql安装与开发环境配置" class="headerlink" title="0. Mysql安装与开发环境配置"></a>0. Mysql安装与开发环境配置</h1><p><a href="https://blog.csdn.net/weixin_46030002/article/details/126719379" target="_blank" rel="noopener">MySQL安装_win10（超详细）</a></p><p><a href="https://blog.csdn.net/weixin_40582034/article/details/115562097?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169269796516800182788194%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=169269796516800182788194&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-115562097-null-null.268^v1^koosearch&amp;utm_term=mysql&amp;spm=1018.2226.3001.4450" target="_blank" rel="noopener">C/C++访问MySQL数据库</a></p><h1 id="1-win10-Navicat-连接虚拟机的MySQL需要关闭防火墙"><a href="#1-win10-Navicat-连接虚拟机的MySQL需要关闭防火墙" class="headerlink" title="1. win10 Navicat 连接虚拟机的MySQL需要关闭防火墙"></a>1. win10 Navicat 连接虚拟机的MySQL需要关闭防火墙</h1><ol><li>查看防火墙端口开放的情况：</li></ol><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service firewalld <span class="built_in">status</span></span><br></pre></td></tr></table></figure><ol><li>关闭防火墙：</li></ol><figure class="highlight vbscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="keyword">stop</span> firewalld</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="2-由于找不到libmysql-dIl-无法继续执行代码。重新安装程序可能会解决此问题。"><a href="#2-由于找不到libmysql-dIl-无法继续执行代码。重新安装程序可能会解决此问题。" class="headerlink" title="2. 由于找不到libmysql.dIl, 无法继续执行代码。重新安装程序可能会解决此问题。"></a>2. 由于找不到libmysql.dIl, 无法继续执行代码。重新安装程序可能会解决此问题。</h1><p>将<code>D:\MySQL\mysql-8.0.33-winx64\lib</code>目录下的<code>libmysql.dll</code>拷贝到<code>E:\Code\VS2022\student_manager\student_manager</code>。</p><h1 id="3-测试连接数据库，并插入数据"><a href="#3-测试连接数据库，并插入数据" class="headerlink" title="3. 测试连接数据库，并插入数据"></a>3. 测试连接数据库，并插入数据</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mysql.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* host = <span class="string">"127.0.0.1"</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* user = <span class="string">"root"</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* pw = <span class="string">"111111"</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* database_name = <span class="string">"student_manager"</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> port = <span class="number">3306</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Student</span> &#123;</span></span><br><span class="line"><span class="keyword">int</span> student_id;</span><br><span class="line"><span class="built_in">string</span> student_name;</span><br><span class="line"><span class="built_in">string</span> class_id;</span><br><span class="line">&#125;Student;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">MYSQL* con = mysql_init(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置字符编码</span></span><br><span class="line">mysql_options(con, MYSQL_SET_CHARSET_NAME, <span class="string">"GBK"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!mysql_real_connect(con, host, user, pw, database_name, port, <span class="literal">NULL</span>, <span class="number">0</span>)) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Failed to connect to database : Error:%s\n"</span>, mysql_errno(con));</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Student stu = &#123; <span class="number">1001</span>, <span class="string">"吴彦祖"</span>, <span class="string">"计算机2班"</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> sql[<span class="number">256</span>];</span><br><span class="line">sprintf_s(sql, <span class="string">"insert into students(student_id, student_name, class_id) values(%d, '%s', '%s');"</span>, </span><br><span class="line">stu.student_id, stu.student_name.c_str(), stu.class_id.c_str());</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (mysql_query(con, sql)) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Failed to insert data : Error:%s\n"</span>, mysql_errno(con));</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mysql_close(con);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="4-C-封装MySQL增删改查操作"><a href="#4-C-封装MySQL增删改查操作" class="headerlink" title="4. C++封装MySQL增删改查操作"></a>4. C++封装MySQL增删改查操作</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// StudentManager.h</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> STUDENTMANAGER_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> STUDENTMANAGER_H</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mysql.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Student</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> student_id;</span><br><span class="line">    <span class="built_in">string</span> student_name;</span><br><span class="line">    <span class="built_in">string</span> class_id;</span><br><span class="line">&#125;Student;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StudentManager</span> &#123;</span></span><br><span class="line">    StudentManager();</span><br><span class="line">    ~StudentManager();</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>: <span class="comment">// 单例模式：只创建一个实体，即只创建一个学生管理类即可</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> StudentManager* <span class="title">GetInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">static</span> StudentManager StudentManager;</span><br><span class="line">        <span class="keyword">return</span> &amp;StudentManager;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">insert_student</span><span class="params">(Student&amp; stu)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">update_student</span><span class="params">(Student&amp; stu)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">delete_student</span><span class="params">(<span class="keyword">int</span> student_id)</span></span>;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;Student&gt; <span class="title">get_students</span><span class="params">(<span class="built_in">string</span> condition = <span class="string">""</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    MYSQL* con;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span>* host = <span class="string">"127.0.0.1"</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span>* user = <span class="string">"root"</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span>* pw = <span class="string">"111111"</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span>* database_name = <span class="string">"student_manager"</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> port = <span class="number">3306</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span> <span class="comment">// STUDENTMANAGER_H</span></span></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// StudentManager.cpp</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"StudentManager.h"</span></span></span><br><span class="line"></span><br><span class="line">StudentManager::StudentManager() &#123;</span><br><span class="line">con = mysql_init(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置字符编码</span></span><br><span class="line">mysql_options(con, MYSQL_SET_CHARSET_NAME, <span class="string">"GBK"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!mysql_real_connect(con, host, user, pw, database_name, port, <span class="literal">NULL</span>, <span class="number">0</span>)) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Failed to connect to database : Error:%s\n"</span>, mysql_errno(con));</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">StudentManager::~StudentManager() &#123;</span><br><span class="line">mysql_close(con);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">StudentManager::insert_student</span><span class="params">(Student&amp; stu)</span> </span>&#123;</span><br><span class="line"><span class="keyword">char</span> sql[<span class="number">256</span>];</span><br><span class="line">sprintf_s(sql, <span class="string">"INSERT INTO students(student_id, student_name, class_id) values(%d, '%s', '%s');"</span>,</span><br><span class="line">stu.student_id, stu.student_name.c_str(), stu.class_id.c_str());</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (mysql_query(con, sql)) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Failed to insert data : Error:%s\n"</span>, mysql_errno(con));</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">StudentManager::update_student</span><span class="params">(Student&amp; stu)</span> </span>&#123;</span><br><span class="line"><span class="keyword">char</span> sql[<span class="number">256</span>];</span><br><span class="line">sprintf_s(sql, <span class="string">"UPDATE students SET student_name = '%s', class_id = '%s' WHERE student_id = %d"</span>, </span><br><span class="line">stu.student_name.c_str(), stu.class_id.c_str(), stu.student_id);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (mysql_query(con, sql)) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Failed to update data : Error:%s\n"</span>, mysql_errno(con));</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">StudentManager::delete_student</span><span class="params">(<span class="keyword">int</span> student_id)</span> </span>&#123;</span><br><span class="line"><span class="keyword">char</span> sql[<span class="number">256</span>];</span><br><span class="line">sprintf_s(sql, <span class="string">"DELETE FROM students WHERE student_id = %d"</span>, student_id);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (mysql_query(con, sql)) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Failed to delete data : Error:%s\n"</span>, mysql_errno(con));</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;Student&gt; <span class="title">StudentManager::get_students</span><span class="params">(<span class="built_in">string</span> condition)</span> </span>&#123;</span><br><span class="line"><span class="built_in">vector</span>&lt;Student&gt; stuList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> sql[<span class="number">256</span>];</span><br><span class="line">sprintf_s(sql, <span class="string">"SELECT * FROM students %s"</span>, condition.c_str());</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (mysql_query(con, sql)) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Failed to select data : Error:%s\n"</span>, mysql_errno(con));</span><br><span class="line"><span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MYSQL_RES* res = mysql_store_result(con);</span><br><span class="line">MYSQL_ROW row;</span><br><span class="line"><span class="keyword">while</span> (row = mysql_fetch_row(res)) &#123;</span><br><span class="line">Student stu;</span><br><span class="line">stu.student_id = atoi(row[<span class="number">0</span>]);</span><br><span class="line">stu.student_name = row[<span class="number">1</span>];</span><br><span class="line">stu.class_id = row[<span class="number">2</span>];</span><br><span class="line">stuList.emplace_back(stu);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> stuList;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// main.cpp</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"StudentManager.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Student stu&#123;<span class="number">999</span>, <span class="string">"彭于晏"</span>, <span class="string">"网工1班"</span>&#125;;</span><br><span class="line">    StudentManager::GetInstance()-&gt;insert_student(stu);</span><br><span class="line"></span><br><span class="line">    Student stu&#123;<span class="number">999</span>, <span class="string">"彭于晏"</span>, <span class="string">"网工3班"</span> &#125;;</span><br><span class="line">    StudentManager::GetInstance()-&gt;update_student(stu);</span><br><span class="line"></span><br><span class="line">    StudentManager::GetInstance()-&gt;delete_student(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;Student&gt; ret = StudentManager::GetInstance()-&gt;get_students();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; t : ret) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; t.student_id &lt;&lt; <span class="string">" "</span> &lt;&lt; t.student_name &lt;&lt; <span class="string">" "</span> &lt;&lt; t.class_id &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// main.cpp</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"StudentManager.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    StudentManager* studentManager = StudentManager::GetInstance();</span><br><span class="line">    Student stu1&#123;<span class="number">1009</span>, <span class="string">"彭于晏"</span>, <span class="string">"网工1班"</span>&#125;;</span><br><span class="line">    studentManager-&gt;insert_student(stu1);</span><br><span class="line"></span><br><span class="line">    Student stu2&#123;<span class="number">999</span>, <span class="string">"胡歌"</span>, <span class="string">"网工3班"</span> &#125;;</span><br><span class="line">    studentManager-&gt;update_student(stu2);</span><br><span class="line"></span><br><span class="line">    studentManager-&gt;delete_student(<span class="number">1001</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;Student&gt; ret = studentManager-&gt;get_students();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; t : ret) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; t.student_id &lt;&lt; <span class="string">" "</span> &lt;&lt; t.student_name &lt;&lt; <span class="string">" "</span> &lt;&lt; t.class_id &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;0-Mysql安装与开发环境配置&quot;&gt;&lt;a href=&quot;#0-Mysql安装与开发环境配置&quot; class=&quot;headerlink&quot; title=&quot;0. Mysql安装与开发环境配置&quot;&gt;&lt;/a&gt;0. Mysql安装与开发环境配置&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_46030002/article/details/126719379&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MySQL安装_win10（超详细）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_40582034/article/details/115562097?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169269796516800182788194%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;amp;request_id=169269796516800182788194&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-115562097-null-null.268^v1^koosearch&amp;amp;utm_term=mysql&amp;amp;spm=1018.2226.3001.4450&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;C/C++访问MySQL数据库&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-win10-Navicat-连接虚拟机的MySQL需要关闭防火墙&quot;&gt;&lt;a href=&quot;#1-win10-Navicat-连接虚拟机的MySQL需要关闭防火墙&quot; class=&quot;headerlink&quot; title=&quot;1. win10 Navicat 连接虚拟机的MySQL需要关闭防火墙&quot;&gt;&lt;/a&gt;1. win10 Navicat 连接虚拟机的MySQL需要关闭防火墙&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;查看防火墙端口开放的情况：&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight lua&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;service firewalld &lt;span class=&quot;built_in&quot;&gt;status&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ol&gt;
&lt;li&gt;关闭防火墙：&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight vbscript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;systemctl &lt;span class=&quot;keyword&quot;&gt;stop&lt;/span&gt; firewalld&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    <category term="C/C++" scheme="http://yoursite.com/categories/C-C/"/>
    
    
    <category term="C/C++" scheme="http://yoursite.com/tags/C-C/"/>
    
    <category term="MySQL" scheme="http://yoursite.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>Socket编程：基于TCP协议的客户端与服务端网络通信</title>
    <link href="http://yoursite.com/2023/08/11/Socket%E7%BC%96%E7%A8%8B%EF%BC%9A%E5%9F%BA%E4%BA%8ETCP%E5%8D%8F%E8%AE%AE%E7%9A%84%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/"/>
    <id>http://yoursite.com/2023/08/11/Socket%E7%BC%96%E7%A8%8B%EF%BC%9A%E5%9F%BA%E4%BA%8ETCP%E5%8D%8F%E8%AE%AE%E7%9A%84%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/</id>
    <published>2023-08-11T09:54:50.000Z</published>
    <updated>2023-08-11T10:02:17.811Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-服务端创建流程"><a href="#1-服务端创建流程" class="headerlink" title="1. 服务端创建流程"></a>1. 服务端创建流程</h1><ol><li>调用socket函数创建监听socket；</li><li>调用bind函数将socket绑定到某个IP和端口号组成的二元组上；</li><li>调用listen函数开启监听；</li><li>当有客户端连接请求时，调用accept函数接受连接，产生一个新的socket（与客户端通信的socket）；</li><li>基于新产生的socket调用send或recv函数开始与客户端进行数据交流；</li><li>通信结束后，调用close函数关闭socket。<a id="more"></a></li></ol><h1 id="2-客户端创建流程"><a href="#2-客户端创建流程" class="headerlink" title="2. 客户端创建流程"></a>2. 客户端创建流程</h1><ol><li>调用socket函数创建客户端socket；</li><li>调用connect函数尝试连接服务器；</li><li>连接成功后调用send或recv函数与服务器进行数据交流。</li><li>通信结束后，调用close函数关闭socket。</li></ol><h1 id="3-常用API"><a href="#3-常用API" class="headerlink" title="3. 常用API"></a>3. 常用API</h1><h2 id="struct-sockaddr-和-struct-sockaddr-in"><a href="#struct-sockaddr-和-struct-sockaddr-in" class="headerlink" title="struct sockaddr 和 struct sockaddr_in"></a>struct sockaddr 和 struct sockaddr_in</h2><p> 这两个结构体都是用来处理<strong>网络通信的地址</strong>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *此数据结构用做bind、connect、recvfrom、sendto等函数的参数，指明地址信息</span></span><br><span class="line"><span class="comment"> *note:</span></span><br><span class="line"><span class="comment"> *  目标地址和端口信息在一起</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="comment">// 地址家族，一般“AF_xxx”的形式，通常使用AF_INET</span></span><br><span class="line">    <span class="keyword">unsigned</span> short sa_family;</span><br><span class="line">    <span class="comment">// 14字节协议地址，目标地址和端口信息</span></span><br><span class="line">    <span class="keyword">char</span>           sa_data[<span class="number">14</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;netinet/in.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span>  <span class="title">sockaddr_in</span> </span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    short <span class="keyword">int</span>       sin_family;       <span class="comment">//协议族</span></span><br><span class="line">    <span class="keyword">unsigned</span> short  <span class="keyword">int</span>  sin_port;    <span class="comment">//端口号(使用网络字节顺序)</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">in_addr</span>  <span class="title">sin_addr</span>;</span>         <span class="comment">//IP地址        </span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span>   sin_zero[<span class="number">8</span>];      <span class="comment">//sockaddr与sockaddr_in 保持大小相同而保留的空字节</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span>  <span class="title">in_addr</span> </span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">unsigned</span>  <span class="keyword">long</span>  s_addr;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">in_addr</span> </span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">union</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span></span></span><br><span class="line"><span class="class">        &#123;</span></span><br><span class="line">            <span class="keyword">unsigned</span> <span class="keyword">char</span> s_b1,</span><br><span class="line">                          s_b2,</span><br><span class="line">                          s_b3,</span><br><span class="line">                          s_b4;</span><br><span class="line">        &#125; S_un_b;</span><br><span class="line"></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> </span></span><br><span class="line"><span class="class">        &#123;</span></span><br><span class="line">            <span class="keyword">unsigned</span> short s_w1,</span><br><span class="line">                           s_w2;</span><br><span class="line">        &#125; S_un_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">long</span> S_addr;</span><br><span class="line">    &#125; S_un;</span><br><span class="line">&#125; IN_ADDR;</span><br></pre></td></tr></table></figure><p><strong>sockaddr_in</strong>和<strong>sockaddr</strong>是<strong>并列</strong>的结构，指向<strong>sockaddr_in</strong>的结构体的指针，同样可以指向<strong>sockraddr</strong>的结构体，并代替它。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">mysock</span>;</span></span><br><span class="line"></span><br><span class="line">bind(sockfd, (struct sockaddr *)&amp;mysock, <span class="keyword">sizeof</span>(struct sockaddr); <span class="comment">/* bind的时候进行转化 */</span></span><br></pre></td></tr></table></figure><h2 id="net-core-somaxconn"><a href="#net-core-somaxconn" class="headerlink" title="net.core.somaxconn"></a>net.core.somaxconn</h2><p>net.core.somaxconn是Linux中的一个kernel参数，表示socket监听（listen）的backlog上限。backlog是socket的监听队列，当一个请求（request）尚未被处理或建立时，他会进入backlog。而socket server可以一次性处理backlog中的所有请求，处理后的请求不再位于监听队列中。当server处理请求较慢，以至于监听队列被填满后，新来的请求会被拒绝。</p><blockquote><p>在Hadoop 1.0中，参数ipc.server.listen.queue.size控制了服务端socket的监听队列长度，即backlog长度，默认值是128。而Linux的参数net.core.somaxconn默认值同样为128。当服务端繁忙时，如NameNode或JobTracker，128是远远不够的。这样就需要增大backlog，例如我们的3000台集群就将ipc.server.listen.queue.size设成了32768，为了使得整个参数达到预期效果，同样需要将kernel参数net.core.somaxconn设成一个大于等于32768的值。</p></blockquote><p>Linux中可以工具syctl来动态调整所有的kernel参数。所谓动态调整就是kernel参数值修改后即时生效。但是这个生效仅限于os层面，对于Hadoop来说，必须重启应用才能生效。</p><p>显示所有的kernel参数及值</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -a</span><br></pre></td></tr></table></figure><p>修改参数值的语法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.core.somaxconn=32768</span><br></pre></td></tr></table></figure><p>以上命令将kernel参数net.core.somaxconn的值改成了32768。这样的改动虽然可以立即生效，但是重启机器后会恢复默认值。为了永久保留改动，需要用vi在/etc/sysctl.conf中增加一行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.core.somaxconn=4000</span><br></pre></td></tr></table></figure><p>然后执行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><h2 id="int-socket-int-family-int-type-int-protocol"><a href="#int-socket-int-family-int-type-int-protocol" class="headerlink" title="int socket(int family, int type, int protocol);"></a>int socket(int family, int type, int protocol);</h2><ul><li>作用：创建一个套接字描述符。在Linux系统中，一切皆文件。为了表示和区分已经打开的文件，Unix/Linux会给文件分配一个ID，这个ID是一个整数，被称为文件描述符。因此，网络连接也是一个文件，它也有文件描述符。通过socket()函数来创建一个网络连接或者说打开一个网络文件，socket()函数的返回值就是文件描述符，通过这个文件描述符我们就可以使用普通的文件操作来传输数据了。</li><li>参数：</li></ul><p>​        family：指明了协议族/域，通常AF_INET、AF_INET6、AF_LOCAL等；<br>​        type：套接字类型，主要 SOCK_STREAM、SOCK_DGRAM、SOCK_RAW等；<br>​        protocol：一般取为0。</p><ul><li>返回值：ok(非负)，error(-1)。成功时，返回一个小的非负整数值，与文件描述符类似。</li></ul><h2 id="int-bind-int-sockfd-const-struct-sockaddr-addr-socklen-t-addrlen"><a href="#int-bind-int-sockfd-const-struct-sockaddr-addr-socklen-t-addrlen" class="headerlink" title="int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);"></a>int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);</h2><ul><li><p>作用：服务端把用于通信的地址和端口绑定到socket上。</p></li><li><p>参数：</p><p>sockfd：代表需要绑定的socket，即在创建socket套接字时返回的文件描述符；<br>addr：存放了服务端用于通信的地址和端口；<br>addrlen：代表addr结构体的大小。</p></li><li><p>返回值：当bind函数返回0时，为正确绑定；返回-1，则为绑定失败。</p></li></ul><h2 id="int-listen-int-sockfd-int-backlog"><a href="#int-listen-int-sockfd-int-backlog" class="headerlink" title="int listen(int sockfd, int backlog);"></a>int listen(int sockfd, int backlog);</h2><ul><li><p>作用：listen函数的功能并不是等待一个新的connect的到来，真正等待connect的是accept函数。listen的操作就是当有较多的client发起connect时，server端不能及时处理连接请求，这时就会将connect连接放在等待队列中缓存起来。这个等待队列的长度由listen中的backlog参数来设定。</p></li><li><p>参数：</p><p>sockfd：前面socket创建的文件描述符;<br>backlog：指server端可以缓存连接的最大个数，也就是等待队列的长度。</p></li><li><p>返回值：当listen运行成功时，返回0；运行失败时，返回-1。</p></li></ul><h2 id="int-accept-int-sockfd-struct-sockaddr-client-addr-socklen-t-addrlen"><a href="#int-accept-int-sockfd-struct-sockaddr-client-addr-socklen-t-addrlen" class="headerlink" title="int accept(int sockfd, struct sockaddr client_addr, socklen_t addrlen);"></a>int accept(int sockfd, struct sockaddr <em>client_addr, socklen_t </em>addrlen);</h2><ul><li><p>作用：accept函数等待客户端的连接，如果没有客户端连上来，它就一直等待，这种方式称为阻塞。accept等待到客户端的连接后，创建一个新的socket，函数返回值就是这个新的socket，服务端用这个新的socket和客户端进行消息的收发。</p></li><li><p>参数：</p><p>sockfd：已经被listen过的socket；<br>client_addr：用于存放客户端的地址信息，其中包含客户端的协议族，网络地址以及端口号。如果不需要客户端的地址，可以填0；<br>addrlen：用于存放参数二(client_addr)的长度。</p></li><li><p>返回值：ok(新的 fd )，error(-1)。</p></li></ul><h2 id="int-connect-int-sock-fd-struct-sockaddr-serv-addr-int-addrlen"><a href="#int-connect-int-sock-fd-struct-sockaddr-serv-addr-int-addrlen" class="headerlink" title="int connect(int sock_fd, struct sockaddr *serv_addr, int addrlen);"></a>int connect(int sock_fd, struct sockaddr *serv_addr, int addrlen);</h2><ul><li><p>作用：客户端向服务端发起连接请求。</p></li><li><p>参数：</p><p>sock_fd：代表通过socket()函数返回的文件描述符；<br>serv_addr：代表目标服务器的协议族，网络地址以及端口号，是一个sockaddr 类型的指针；<br>addrlen：代表第二个参数内容的大小。</p></li><li><p>返回值：当返回值是0时，代表连接成功；返回值为-1时，代表连接失败。</p></li></ul><h2 id="int-recv-int-sockfd-void-buf-int-len-int-flags"><a href="#int-recv-int-sockfd-void-buf-int-len-int-flags" class="headerlink" title="int recv(int sockfd, void *buf, int len, int flags);"></a>int recv(int sockfd, void *buf, int len, int flags);</h2><ul><li><p>作用：recv函数用于接收对端socket发送过来的数据。不论是客户端还是服务端，应用程序都用recv函数接受来自TCP连接的另一端发送过来的数据。如果socket对端没有发送数据，recv函数就会等待，如果对端发送了数据，函数返回接收到的字符数。</p></li><li><p>参数：</p><p>sockfd：代表接收端的套接字描述符，即通过socket()函数返回的文件描述符；<br>buf：用于接收数据的内存地址，可以是C语言基本数据类型变量的地址，也可以是数组、结构体、字符串；<br>len：指明需要接收数据的字节数。不能超过buf的大小，否则内存溢出；<br>flags：一般设置为0，其他数值意义不大。</p></li><li><p>返回值：失败时，返回值小于0；超时或对端主动关闭，返回值等于0；成功时，返回值是接收数据的长度。</p></li></ul><h2 id="int-send-int-sockfd-const-void-buf-int-len-int-flags"><a href="#int-send-int-sockfd-const-void-buf-int-len-int-flags" class="headerlink" title="int send(int sockfd, const void *buf, int len, int flags);"></a>int send(int sockfd, const void *buf, int len, int flags);</h2><ul><li><p>作用：send函数用于把数据通过socket发送给对端。不论是客户端还是服务端，应用程序都用send函数来向TCP连接的另一端发送数据。</p></li><li><p>参数：</p><p>sockfd：代表发送端的套接字描述符，即通过socket()函数返回的文件描述符；<br>buf：指明需要发送数据的内存地址，可以是C语言基本数据类型变量的地址，也可以是数组、结构体、字符串；<br>len：指明实际发送数据的字节数；<br>flags：一般设置为0，其他数值意义不大。</p></li><li><p>返回值：失败时，返回值小于0；超时或对端主动关闭，返回值等于0；成功时，返回值是发送数据的长度。</p></li></ul><h2 id="int-close-int-sockfd"><a href="#int-close-int-sockfd" class="headerlink" title="int close(int sockfd);"></a>int close(int sockfd);</h2><ul><li><p>作用：关闭套接字，并终止TCP连接。</p></li><li><p>参数：</p><p>sockfd：套接字描述符。</p></li><li><p>返回值：ok(0)，error(-1)。</p></li></ul><h2 id="int-inet-pton-int-af-const-char-src-void-dst"><a href="#int-inet-pton-int-af-const-char-src-void-dst" class="headerlink" title="int inet_pton(int af, const char src, void dst);"></a>int inet_pton(int af, const char <em>src, void </em>dst);</h2><ul><li><p>作用：将文本地址转化为二进制地址。</p></li><li><p>头文件：#include <arpa inet.h></arpa></p></li><li><p>参数：</p><p>af：地址族。AF_INET：IPv4 地址；AF_INET6：IPv6 地址；<br>src：指向“点分式”IPv4 或 IPv6 地址的指针，例如“192.168.1.100”；<br>dst：类型为 struct in_addr <em>或者 struct in6_addr </em>的指针。</p></li><li><p>返回值：成功：1；失败：0 代表地址与地址族不匹配，-1 代表地址不合法。</p></li></ul><h2 id="const-char-inet-ntop-int-af-const-void-src-char-dst-socklen-t-size"><a href="#const-char-inet-ntop-int-af-const-void-src-char-dst-socklen-t-size" class="headerlink" title="const char inet_ntop(int af, const void src, char *dst, socklen_t size);"></a>const char <em>inet_ntop(int af, const void </em>src, char *dst, socklen_t size);</h2><ul><li><p>作用：将二进制地址转化为文本地址。</p></li><li><p>头文件：#include <arpa inet.h></arpa></p></li><li><p>参数：</p><p>af：地址族。AF_INET：IPv4 地址；AF_INET6：IPv6 地址；<br>src：类型为 struct in_addr <em>或者 struct in6_addr </em>的指针；<br>dst：地址缓冲区指针；<br>size：地址缓冲区大小，至少要 INET_ADDRSTRLEN 或者 INET6_ADDRSTRLEN 个字节。</p></li><li><p>返回值：成功：dst； 失败：NULL。</p></li></ul><h2 id="字节序的转换函数"><a href="#字节序的转换函数" class="headerlink" title="字节序的转换函数"></a>字节序的转换函数</h2><ul><li><p>函数原型：</p><p><strong>uint32_t htonl(uint32_t hostlong);</strong><br><strong>uint16_t htons(uint16_t hostshort);</strong><br><strong>uint32_t ntohl(uint32_t netlong);</strong><br><strong>uint16_t ntohs(uint16_t netshort);</strong></p></li><li><p>参数：<br>hostlong：主机字节序的长整型数据；<br>hostshort：主机字节序的短整型数据；<br>netlong： 网络字节序的长整型数据；<br>netshort：网络字节序的短整型数据。</p></li><li><p>返回值：对应的字节序数据。</p></li></ul><h1 id="4-服务端代码"><a href="#4-服务端代码" class="headerlink" title="4. 服务端代码"></a>4. 服务端代码</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;// POSIX系统API访问</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;// 基本系统数据类型</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;// 网络信息转换</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_PORT 9991</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建一个监听socket</span></span><br><span class="line">    <span class="keyword">int</span> listenfd = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (listenfd == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"create listen socket error!"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化服务器地址</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">bindaddr</span>;</span></span><br><span class="line">    bindaddr.sin_family = AF_INET;</span><br><span class="line">    bindaddr.sin_addr.s_addr = htonl(INADDR_ANY);</span><br><span class="line">    bindaddr.sin_port = htons(SERVER_PORT);</span><br><span class="line"><span class="comment">//如果只想在本机上进行访问，bind函数地址可以使用本地回环地址</span></span><br><span class="line"><span class="comment">//如果只想被局域网的内部机器访问，那么bind函数地址可以使用局域网地址</span></span><br><span class="line"><span class="comment">//如果希望被公网访问，那么bind函数地址可以使用INADDR_ANY or 0.0.0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 绑定地址和端口</span></span><br><span class="line">    <span class="keyword">if</span> (bind(listenfd, (struct sockaddr *)&amp;bindaddr, <span class="keyword">sizeof</span>(bindaddr)) == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"bind listen socket error!"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动监听</span></span><br><span class="line">    <span class="keyword">if</span> (listen(listenfd, SOMAXCONN) == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"listen error!"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"start listening..."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">// 创建一个临时的客户端socket</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">clientaddr</span>;</span></span><br><span class="line">        <span class="keyword">socklen_t</span> clientaddrlen = <span class="keyword">sizeof</span>(clientaddr);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 接受客户端连接</span></span><br><span class="line">        <span class="keyword">int</span> clientfd = accept(listenfd, (struct sockaddr *)&amp;clientaddr, &amp;clientaddrlen);</span><br><span class="line">        <span class="keyword">if</span> (clientfd != <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="keyword">char</span> recvBuf[<span class="number">32</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">            <span class="comment">// 从客户端接受数据</span></span><br><span class="line">            <span class="keyword">int</span> ret = recv(clientfd, recvBuf, <span class="keyword">sizeof</span>(recvBuf), <span class="number">0</span>);</span><br><span class="line">            <span class="keyword">if</span> (ret &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; <span class="string">"recv data from cilent successfully, data:"</span> &lt;&lt; recvBuf &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">                <span class="comment">// 将接收到的数据原封不动地发给客户端</span></span><br><span class="line">                ret = send(clientfd, recvBuf, <span class="built_in">strlen</span>(recvBuf), <span class="number">0</span>);</span><br><span class="line">                <span class="keyword">if</span> (ret != <span class="built_in">strlen</span>(recvBuf)) &#123;</span><br><span class="line">                    <span class="built_in">cout</span> &lt;&lt; <span class="string">"send data error!"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="built_in">cout</span> &lt;&lt; <span class="string">"send data to client successfully, data:"</span> &lt;&lt; recvBuf &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; <span class="string">"recv data error!"</span> &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        close(clientfd);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭监听socket</span></span><br><span class="line">    close(listenfd);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="5-客户端代码"><a href="#5-客户端代码" class="headerlink" title="5. 客户端代码"></a>5. 客户端代码</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;// POSIX系统API访问</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;// 基本系统数据类型</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;arpa/inet.h&gt;// 网络信息转换</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_ADDRESS <span class="meta-string">"127.0.0.1"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SERVER_PORT 9991</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span>* message;</span><br><span class="line">    <span class="keyword">if</span>(argc != <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="built_in">fputs</span>(<span class="string">"Usage: ./client message\n"</span>, <span class="built_in">stderr</span>); <span class="comment">// 向指定的文件写入一个字符串</span></span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    message = argv[<span class="number">1</span>];</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"send message: %s\n"</span>, message);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个socket</span></span><br><span class="line">    <span class="keyword">int</span> clientfd = socket(AF_INET, SOCK_STREAM, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (clientfd == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"create client socket error!"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 连接服务器地址</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_in</span> <span class="title">serveraddr</span>;</span></span><br><span class="line">    serveraddr.sin_family = AF_INET;</span><br><span class="line">    serveraddr.sin_addr.s_addr = inet_addr(SERVER_ADDRESS);</span><br><span class="line">    serveraddr.sin_port = htons(SERVER_PORT);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (connect(clientfd, (struct sockaddr *)&amp;serveraddr, <span class="keyword">sizeof</span>(serveraddr)) == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"connect socket error!"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 向服务器发送数据</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> ret = send(clientfd, message, <span class="built_in">strlen</span>(message), <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (ret != <span class="built_in">strlen</span>(message)) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"send data error!"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"send data to server successfully, data:"</span> &lt;&lt; message &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从服务器读取数据</span></span><br><span class="line">    <span class="keyword">char</span> recvBuf[<span class="number">32</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    ret = recv(clientfd, recvBuf, <span class="keyword">sizeof</span>(recvBuf), <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (ret &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"recv data from server successfully, data:"</span> &lt;&lt; recvBuf &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"recv data from server error!"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭socket</span></span><br><span class="line">    close(clientfd);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-服务端创建流程&quot;&gt;&lt;a href=&quot;#1-服务端创建流程&quot; class=&quot;headerlink&quot; title=&quot;1. 服务端创建流程&quot;&gt;&lt;/a&gt;1. 服务端创建流程&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;调用socket函数创建监听socket；&lt;/li&gt;
&lt;li&gt;调用bind函数将socket绑定到某个IP和端口号组成的二元组上；&lt;/li&gt;
&lt;li&gt;调用listen函数开启监听；&lt;/li&gt;
&lt;li&gt;当有客户端连接请求时，调用accept函数接受连接，产生一个新的socket（与客户端通信的socket）；&lt;/li&gt;
&lt;li&gt;基于新产生的socket调用send或recv函数开始与客户端进行数据交流；&lt;/li&gt;
&lt;li&gt;通信结束后，调用close函数关闭socket。&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    <category term="C/C++" scheme="http://yoursite.com/categories/C-C/"/>
    
    
    <category term="C/C++" scheme="http://yoursite.com/tags/C-C/"/>
    
    <category term="网络通信" scheme="http://yoursite.com/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/"/>
    
    <category term="Socket编程" scheme="http://yoursite.com/tags/Socket%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>C++单例模式</title>
    <link href="http://yoursite.com/2023/08/10/C-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"/>
    <id>http://yoursite.com/2023/08/10/C-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/</id>
    <published>2023-08-10T08:37:12.000Z</published>
    <updated>2023-08-10T09:13:17.840Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、什么是单例"><a href="#1、什么是单例" class="headerlink" title="1、什么是单例"></a>1、什么是单例</h1><p>单例 Singleton 是设计模式的一种，其特点是只提供唯一一个类的实例，具有全局变量的特点，在任何位置都可以通过接口获取到那个唯一实例。</p><p>具体应用场景如：</p><ul><li><p>设备管理器。系统中可能有多个设备，但是只有一个设备管理器，用于管理设备驱动；</p></li><li><p>数据池。用来缓存数据的数据结构，需要在一处写，多处读取或者多处写，多处读取。<a id="more"></a></p></li></ul><h1 id="2、一个好的单例应该具备的条件"><a href="#2、一个好的单例应该具备的条件" class="headerlink" title="2、一个好的单例应该具备的条件"></a>2、一个好的单例应该具备的条件</h1><ul><li>全局只有一个实例：static 特性，同时禁止用户自己声明并定义实例（把构造函数设为 private）</li><li>用户通过接口获取实例：使用 static 类成员函数</li><li>禁止赋值和拷贝</li><li>线程安全</li></ul><h1 id="3、懒汉模式与饿汉模式"><a href="#3、懒汉模式与饿汉模式" class="headerlink" title="3、懒汉模式与饿汉模式"></a>3、懒汉模式与饿汉模式</h1><ul><li>懒汉模式：<ul><li>时间换空间；</li><li>故名思义，不到万不得已就不会去实例化对象，在第一次用到类实例的时候才会去实例化对象；</li><li>多线程情况下会存在线程安全问题，需要加互斥锁进行防护。</li></ul></li><li>饿汉模式：<ul><li>空间换时间；</li><li>在单例类定义的时候就进行实例化对象，当需要使用时只要通过接口函数直接获取对象。</li></ul></li></ul><h1 id="4、单例实现：线程安全、内存安全的懒汉式单例（基于C-11的智能指针和互斥锁）"><a href="#4、单例实现：线程安全、内存安全的懒汉式单例（基于C-11的智能指针和互斥锁）" class="headerlink" title="4、单例实现：线程安全、内存安全的懒汉式单例（基于C++11的智能指针和互斥锁）"></a>4、单例实现：线程安全、内存安全的懒汉式单例（基于C++11的智能指针和互斥锁）</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mutex&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ~Singleton() &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"destructor called!"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    Singleton(Singleton&amp;) = <span class="keyword">delete</span>;                  <span class="comment">// copy constructor can't be called</span></span><br><span class="line">    Singleton&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> Singleton&amp;) = <span class="keyword">delete</span>; <span class="comment">// assignment operator can't be called</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Singleton&gt; <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 双重锁检测</span></span><br><span class="line">        <span class="keyword">if</span> (m_pInstance == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="function"><span class="built_in">std</span>::lock_guard&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">lk</span><span class="params">(m_mutex)</span></span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (m_pInstance == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                m_pInstance = <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Singleton&gt;(<span class="keyword">new</span> Singleton);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> m_pInstance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Singleton() &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"constructor called!"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">static</span> <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Singleton&gt; m_pInstance;</span><br><span class="line">    <span class="keyword">static</span> <span class="built_in">std</span>::mutex m_mutex;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// initialization static variables out of class</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Singleton&gt; Singleton::m_pInstance = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="built_in">std</span>::mutex Singleton::m_mutex;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Singleton&gt; instance1 = Singleton::getInstance();</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Singleton&gt; instance2 = Singleton::getInstance();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PS E:\Code\VSCode\Demo\build&gt; .<span class="string">"E:/Code/VSCode/Demo/build/main.exe"</span></span><br><span class="line">constructor called!</span><br><span class="line">destructor called!</span><br></pre></td></tr></table></figure><ul><li>基于 shared_ptr，用了C++比较倡导的 RAII 思想，即用对象管理资源。当 shared_ptr 析构的时候，new 出来的对象也会被 delete 掉，此避免内存泄漏。</li><li>加了锁，使用互斥量来达到线程安全。这里使用了两个 if 判断语句的技术称为双检锁，其好处是，只有判断指针为空的时候才加锁，避免每次调用 get_instance 的方法都加锁，毕竟锁的开销还是有点大的。</li></ul><p>不足之处在于： 使用智能指针会要求用户也得使用智能指针，非必要不应该提出这种约束；使用锁也有开销，并且在某些平台（与编译器和指令集架构有关），双检锁会失效！</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1、什么是单例&quot;&gt;&lt;a href=&quot;#1、什么是单例&quot; class=&quot;headerlink&quot; title=&quot;1、什么是单例&quot;&gt;&lt;/a&gt;1、什么是单例&lt;/h1&gt;&lt;p&gt;单例 Singleton 是设计模式的一种，其特点是只提供唯一一个类的实例，具有全局变量的特点，在任何位置都可以通过接口获取到那个唯一实例。&lt;/p&gt;
&lt;p&gt;具体应用场景如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;设备管理器。系统中可能有多个设备，但是只有一个设备管理器，用于管理设备驱动；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;数据池。用来缓存数据的数据结构，需要在一处写，多处读取或者多处写，多处读取。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary>
    
    
    
    <category term="设计模式" scheme="http://yoursite.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
    
    <category term="C/C++" scheme="http://yoursite.com/tags/C-C/"/>
    
    <category term="设计模式" scheme="http://yoursite.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Redhat7.6配置163yum源</title>
    <link href="http://yoursite.com/2023/06/30/Redhat7-6%E9%85%8D%E7%BD%AE163yum%E6%BA%90/"/>
    <id>http://yoursite.com/2023/06/30/Redhat7-6%E9%85%8D%E7%BD%AE163yum%E6%BA%90/</id>
    <published>2023-06-30T07:24:19.000Z</published>
    <updated>2023-06-30T07:25:54.105Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本文介绍Redhat7.6服务器上配置163yum源的方法。</strong></p><h1 id="1-yum源简介"><a href="#1-yum源简介" class="headerlink" title="1. yum源简介"></a>1. yum源简介</h1><ul><li>yum需要一个yum库，也就是yum源。默认情况下，CentOS就有一个yum源，其配置文件在/etc/yum.repos.d/目录下。</li><li>当使用yum下载安装rpm包时，首先要找一个yum库（源），然后确保本地有一个客户端（yum这个命令就是客户端），由yum程序去连接服务器。连接的方式是由配置文件决定的。通过编辑/etc/yum.repos.d/CentOS-Base.repo文件，可以修改设置。</li></ul><a id="more"></a><h1 id="2-确认版本"><a href="#2-确认版本" class="headerlink" title="2. 确认版本"></a>2. 确认版本</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@promote ~]<span class="comment"># cat /etc/redhat-release</span></span><br><span class="line">Red Hat Enterprise Linux Server release 7.6 (Maipo)</span><br></pre></td></tr></table></figure><h1 id="3-卸载本地yum"><a href="#3-卸载本地yum" class="headerlink" title="3. 卸载本地yum"></a>3. 卸载本地yum</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa|grep yum　　<span class="comment">#查看本地yum</span></span><br><span class="line">rpm -qa|grep yum|xargs rpm -e --nodeps　　<span class="comment">#卸载本地yum，不检查依赖性，卸载yum程序</span></span><br><span class="line">rpm -qa|grep yum    <span class="comment">#再次查看是否正确卸载</span></span><br></pre></td></tr></table></figure><h1 id="4-下载yum相关程序"><a href="#4-下载yum相关程序" class="headerlink" title="4. 下载yum相关程序"></a>4. 下载yum相关程序</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirrors.163.com/centos/7/os/x86_64/Packages/python-urlgrabber-3.10-10.el7.noarch.rpm</span><br><span class="line">wget http://mirrors.163.com/centos/7/os/x86_64/Packages/rpm-4.11.3-45.el7.x86_64.rpm</span><br><span class="line">wget http://mirrors.163.com/centos/7/os/x86_64/Packages/yum-3.4.3-168.el7.centos.noarch.rpm</span><br><span class="line">wget http://mirrors.163.com/centos/7/os/x86_64/Packages/yum-metadata-parser-1.1.4-10.el7.x86_64.rpm</span><br><span class="line">wget http://mirrors.163.com/centos/7/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.31-54.el7_8.noarch.rpm</span><br><span class="line">wget http://mirrors.163.com/centos/7/os/x86_64/Packages/yum-utils-1.1.31-54.el7_8.noarch.rpm</span><br></pre></td></tr></table></figure><h1 id="5-安装yum程序"><a href="#5-安装yum程序" class="headerlink" title="5. 安装yum程序"></a>5. 安装yum程序</h1><p>若提示依赖检测失败，执行以下命令强制安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh * --force --nodeps <span class="comment">#--force 强制安装，--nodeps 不检查依赖</span></span><br></pre></td></tr></table></figure><h1 id="6-修改repo文件"><a href="#6-修改repo文件" class="headerlink" title="6. 修改repo文件"></a>6. 修改repo文件</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/yum.repos.d/　　<span class="comment">#进入到yum配置文件目录</span></span><br><span class="line">mkdir repo_backup   <span class="comment">#创建yum备份目录</span></span><br><span class="line">mv *.repo repo_backup   <span class="comment">#备份</span></span><br><span class="line">wget http://mirrors.163.com/.<span class="built_in">help</span>/CentOS7-Base-163.repo　　<span class="comment">#下载CentOS配置文件</span></span><br></pre></td></tr></table></figure><p>修改CentOS7-Base-163.repo文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi CentOS7-Base-163.repo</span><br><span class="line">:%s/<span class="variable">$releasever</span>/7/ge</span><br></pre></td></tr></table></figure><h1 id="7-清除并更新yum缓存"><a href="#7-清除并更新yum缓存" class="headerlink" title="7. 清除并更新yum缓存"></a>7. 清除并更新yum缓存</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum clean all&amp;yum makecache</span><br><span class="line">yum update</span><br><span class="line">yum repolist <span class="comment">#验证结果</span></span><br></pre></td></tr></table></figure><h1 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h1><p>当发现仍无需要的软件时，CentOS还有一个源叫做EPEL(Extra Packages for Enterprise)，可尝试安装epel yum源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel-release</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;本文介绍Redhat7.6服务器上配置163yum源的方法。&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-yum源简介&quot;&gt;&lt;a href=&quot;#1-yum源简介&quot; class=&quot;headerlink&quot; title=&quot;1. yum源简介&quot;&gt;&lt;/a&gt;1. yum源简介&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;yum需要一个yum库，也就是yum源。默认情况下，CentOS就有一个yum源，其配置文件在/etc/yum.repos.d/目录下。&lt;/li&gt;
&lt;li&gt;当使用yum下载安装rpm包时，首先要找一个yum库（源），然后确保本地有一个客户端（yum这个命令就是客户端），由yum程序去连接服务器。连接的方式是由配置文件决定的。通过编辑/etc/yum.repos.d/CentOS-Base.repo文件，可以修改设置。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
    <category term="Redhat" scheme="http://yoursite.com/tags/Redhat/"/>
    
    <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>算法设计与分析之分治法</title>
    <link href="http://yoursite.com/2023/06/29/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90%E4%B9%8B%E5%88%86%E6%B2%BB%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/29/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90%E4%B9%8B%E5%88%86%E6%B2%BB%E6%B3%95/</id>
    <published>2023-06-29T05:32:15.000Z</published>
    <updated>2023-06-29T05:32:16.987Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-分治法简介"><a href="#1-分治法简介" class="headerlink" title="1. 分治法简介"></a>1. 分治法简介</h1><p>分治法的就是将一个规模为N的问题分解为K个规模较小的子问题，这些子问题相互独立且与原问题性质相同。求出子问题的解，就可得到原问题的解。<a id="more"></a></p><h2 id="1-1-基本思想"><a href="#1-1-基本思想" class="headerlink" title="1.1 基本思想"></a>1.1 基本思想</h2><p>当我们求解某些问题时，由于这些问题要处理的数据相当多，或求解过程相当复杂，使得直接求解法在时间上相当长，或者根本无法直接求出。对于这类问题，我们往往先把它分解成几个子问题，找到求出这几个子问题的解法后，再找到合适的方法，把它们组合成求整个问题的解法。如果这些子问题还较大，难以解决，可以再把它们分成几个更小的子问题，以此类推，直至可以直接求出解为止。这就是分治策略的基本思想。一般情况下，还会用到二分法。</p><blockquote><p>二分法：利用分治策略求解时，所需时间取决于分解后子问题的个数、子问题的规模大小等因素，而二分法，由于其划分的简单和均匀的特点，是经常采用的一种有效的方法，例如二分法检索。</p></blockquote><h2 id="1-2-解决步骤"><a href="#1-2-解决步骤" class="headerlink" title="1.2 解决步骤"></a>1.2 解决步骤</h2><p>分治法解题的一般步骤如下：</p><p>（1）分解，将要解决的问题划分成若干规模较小的同类问题；</p><p>（2）求解，当子问题划分得足够小时，用较简单的方法解决；</p><p>（3）合并，按原问题的要求，将子问题的解逐层合并构成原问题的解。</p><h2 id="1-3-具体算法"><a href="#1-3-具体算法" class="headerlink" title="1.3 具体算法"></a>1.3 具体算法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;//开始</span><br><span class="line"><span class="keyword">if</span>(1、问题不可分)</span><br><span class="line">2、返回问题解；</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  &#123;</span><br><span class="line">   3、从原问题中划分出含一半运算对象的子问题1；</span><br><span class="line"> 4、递归调用分治法过程，求出解1；</span><br><span class="line"> 5、从原问题中划出含另一半运算对象的子问题2；</span><br><span class="line"> 6、递归调用分治法过程，求出解2；</span><br><span class="line">   7、将解1、解2组合成整个问题的解； </span><br><span class="line">  &#125; </span><br><span class="line">&#125;//结束</span><br></pre></td></tr></table></figure><h2 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h2><p> 运用分治策略解决的问题一般来说具有以下特点：</p><p>1、原问题可以分解为多个子问题。这些子问题与原问题相比，只是问题的规模有所降低，其结构和求解方法与原问题相同或相似。</p><p>2、原问题在分解过程中，递归地求解子问题。由于递归都必须有一个终止条件，因此，当分解后的子问题规模足够小时，应能够直接求解。</p><p>3、在求解并得到各个子问题的解后，应能够采用某种方式、方法合并或构造出原问题的解。</p><blockquote><p>不难发现，在分治策略中，由于子问题与原问题在结构和解法上的相似性，用分治方法解决的问题，大都采用了<strong>递归</strong>的形式。在各种排序方法中，如<strong>归并排序、快速排序、堆排序</strong>等，都存在有分治的思想。另外，下列问题也可用分治策略解决：</p><p><strong>Merge Two Sorted Lists（合并两个有序链表）- LeetCode 21</strong><br><strong>Merge k Sorted Lists（合并 K 个升序链表）- LeetCode 23</strong><br><strong>Top K 问题：前 K 个高频元素 - LeetCode 347</strong></p></blockquote><h1 id="2-二分查找"><a href="#2-二分查找" class="headerlink" title="2. 二分查找"></a>2. 二分查找</h1><p><strong>问题描述：</strong>给定 n 个元素，这些元素是有序的（假定为升序），从中查找特定元素 x。</p><p><strong>算法思想：</strong>将有序序列分成规模大致相等的两部分，然后取中间元素与特定查找元素 x 进行比较，如果 x 等于中间元素，则查找成功，算法终止；如果 x 小于中间元素，则在序列的前半部分继续查找，即在序列的前半部分重复分解和治理操作；否则，在序列的后半部分继续查找，即在序列的后半部分重复分解和治理操作。</p><p><strong>算法设计：</strong></p><p>（1）设置查找区间：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> left; <span class="comment">// C++自动初始化为0</span></span><br><span class="line"><span class="keyword">int</span> right = n;</span><br></pre></td></tr></table></figure><p>（2）若查找区间 [left, right] 不存在，则查找失败，返回；否则执行（3）。</p><p>（3）取中间位 mid = (left + right)/2，比较 x 与 a[mid]  ，有三种情况：</p><ul><li><p>若 x &lt; a[mid]，则right = mid - 1；查找在左半区间进行，转（2）；</p></li><li><p>若 x &gt; a[mid]，则left = mid + 1；查找在右半区间进行，转（2）；</p></li><li><p>若 x = a[mid]，则查找成功，返回mid的值。</p></li></ul><p><strong>时间复杂度分析：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（1）顺序查找</span><br><span class="line">- 最好时间复杂度：O(1)</span><br><span class="line">- 最坏时间复杂度：O(n)</span><br><span class="line">（2）二分查找</span><br><span class="line">- 最好时间复杂度：O(1)</span><br><span class="line">- 最坏时间复杂度：O(logn)</span><br></pre></td></tr></table></figure><p><strong>算法实现：</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">binarySearch</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nums, <span class="keyword">int</span> l, <span class="keyword">int</span> r, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (l &lt;= r) &#123;</span><br><span class="line">        <span class="comment">// 计算 mid 时需要防止溢出，left+(right-left)/2和(left+right)/2的结果相同，但是有效防止了left和right太大直接相加导致溢出</span></span><br><span class="line">        <span class="keyword">int</span> mid = l + (r-l)/<span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[mid] == x) &#123;</span><br><span class="line">            <span class="keyword">return</span> mid;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &gt; x) &#123;</span><br><span class="line">            <span class="keyword">return</span> binarySearch(nums, l, mid - <span class="number">1</span>, x);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">            <span class="keyword">return</span> binarySearch(nums, mid + <span class="number">1</span>, r, x);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nums&#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">10</span>, <span class="number">40</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> x = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> ans = binarySearch(nums, <span class="number">0</span>, nums.size() - <span class="number">1</span>, x);</span><br><span class="line">    <span class="keyword">if</span> (ans == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"未找到该数！"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"该数的索引为："</span> &lt;&lt; ans &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-分治法简介&quot;&gt;&lt;a href=&quot;#1-分治法简介&quot; class=&quot;headerlink&quot; title=&quot;1. 分治法简介&quot;&gt;&lt;/a&gt;1. 分治法简介&lt;/h1&gt;&lt;p&gt;分治法的就是将一个规模为N的问题分解为K个规模较小的子问题，这些子问题相互独立且与原问题性质相同。求出子问题的解，就可得到原问题的解。&lt;/p&gt;</summary>
    
    
    
    <category term="算法设计与分析" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    
    
    <category term="算法设计与分析" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>算法设计与分析之贪心算法</title>
    <link href="http://yoursite.com/2023/06/28/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90%E4%B9%8B%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/28/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90%E4%B9%8B%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-28T02:46:37.000Z</published>
    <updated>2023-06-28T05:00:34.442Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-贪心算法简介"><a href="#1-贪心算法简介" class="headerlink" title="1. 贪心算法简介"></a>1. 贪心算法简介</h1><p>贪心算法（Greedy Algorithm）是一种基于贪心思想的算法策略。它通过每一步选择当前状态下最优的解决方案，从而逐步得到全局最优解。贪心算法通常在问题具有<strong>贪心选择性质</strong>和<strong>最优子结构性质</strong>时被应用。<a id="more"></a></p><p>贪心算法的基本思想是，每一步选择当前情况下看起来最好的解决方案，而不考虑之后可能发生的情况。它<strong>不进行回溯，也不考虑全局最优解</strong>，而是<strong>根据局部最优选择</strong>来构建解决方案。</p><p><strong>适合贪心算法具有的特征：</strong></p><p>1、优化问题。</p><p>2、问题的求解可以划分为若干阶段。</p><p>3、能够制定出最优量度标准。</p><p>4、问题具有最优子结构性质。</p><p><strong>贪心算法的步骤通常如下：</strong></p><p>1、确定问题的最优子结构：问题的最优解可以通过子问题的最优解来构建。</p><p>2、定义贪心选择策略：确定每一步的最优选择。</p><p>3、构建解决方案：根据贪心选择策略，逐步构建问题的解决方案。</p><p>4、验证解决方案：检查贪心算法得到的解决方案是否满足问题的要求。</p><h1 id="2-找零钱问题"><a href="#2-找零钱问题" class="headerlink" title="2. 找零钱问题"></a>2. 找零钱问题</h1><p>关于贪心算法的一个经典例子是找零钱问题（Coin Change Problem）。问题描述如下：给定一些不同面额的硬币和一个需要找零的金额，找出最少的硬币数量来凑出该金额。利用贪心算法解决该问题的思路如下：</p><p>（1）输入A和每种货币的数目；</p><p>（2）从币值从高到低进行贪心策略；</p><p>（3）选择所要表示的币值；</p><p>（4）记录所选择的币值所要的数目，现金值实时更改；</p><p>（5）输出结果。</p><p><strong>算法实现</strong></p><pre><code>#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;std::vector&lt;int&gt; greedyCoinChange(std::vector&lt;int&gt;&amp; coins, int amount) {    std::vector&lt;int&gt; result;    // 按面额从大到小排序硬币    std::sort(coins.rbegin(), coins.rend());    for (auto coin : coins) {        while (amount &gt;= coin) {            result.push_back(coin);            amount -= coin;        }    }    if (amount != 0) {        // 找不到合适的硬币组合        result.clear();    }    return result;}int main() {    std::vector&lt;int&gt; coins{1, 5, 10, 25};    int amount = 47;    std::vector&lt;int&gt; result = greedyCoinChange(coins, amount);    if (!result.empty()) {        std::cout &lt;&lt; &quot;找零的硬币数量：&quot; &lt;&lt; result.size() &lt;&lt; std::endl;        std::cout &lt;&lt; &quot;找零的硬币面额：&quot;;        for (auto coin : result) {            std::cout &lt;&lt; coin &lt;&lt; &quot; &quot;;        }        std::cout &lt;&lt; std::endl;    } else {        std::cout &lt;&lt; &quot;无法找零。&quot; &lt;&lt; std::endl;    }    return 0;}</code></pre><p>在上述示例中，我们定义了一个greedyCoinChange函数，它接收一个硬币面额的向量coins和需要找零的金额amount。首先，我们将硬币面额按从大到小排序，以便每次选择最大面额的硬币。然后，我们循环遍历每个硬币，直到无法再选择该硬币为止。最后，如果剩余的金额不为0，则表示无法找零，返回空的结果。在main函数中，我们定义了一组硬币面额和需要找零的金额，并调用greedyCoinChange函数来获取找零的结果。如果结果不为空，我们输出找零的硬币数量和面额；否则，输出无法找零的信息。</p><p>例如，对于硬币面额为{1, 5, 10, 25}，需要找零47的情况下，输出将会是：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">找零的硬币数量：<span class="number">5</span></span><br><span class="line">找零的硬币面额：<span class="number">25</span> <span class="number">10</span> <span class="number">10</span> <span class="number">1</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>这表示我们可以用5个硬币凑出47的金额，其中包括1个25分硬币、2个10分硬币和2个1分硬币。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-贪心算法简介&quot;&gt;&lt;a href=&quot;#1-贪心算法简介&quot; class=&quot;headerlink&quot; title=&quot;1. 贪心算法简介&quot;&gt;&lt;/a&gt;1. 贪心算法简介&lt;/h1&gt;&lt;p&gt;贪心算法（Greedy Algorithm）是一种基于贪心思想的算法策略。它通过每一步选择当前状态下最优的解决方案，从而逐步得到全局最优解。贪心算法通常在问题具有&lt;strong&gt;贪心选择性质&lt;/strong&gt;和&lt;strong&gt;最优子结构性质&lt;/strong&gt;时被应用。&lt;/p&gt;</summary>
    
    
    
    <category term="算法设计与分析" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    
    
    <category term="算法设计与分析" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>算法设计与分析之回溯法</title>
    <link href="http://yoursite.com/2023/06/27/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90%E4%B9%8B%E5%9B%9E%E6%BA%AF%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/27/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90%E4%B9%8B%E5%9B%9E%E6%BA%AF%E6%B3%95/</id>
    <published>2023-06-27T05:30:48.000Z</published>
    <updated>2023-06-28T06:21:51.022Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-回溯法简介"><a href="#1-回溯法简介" class="headerlink" title="1. 回溯法简介"></a>1. 回溯法简介</h1><p>回溯法，又叫试探法，是一种寻找<strong>最优解</strong>的<strong>暴力搜寻法。</strong>由于暴力，回溯法的<strong>时间复杂度较高</strong>，因此在比较一些数字较大的问题，比如<strong>最短路径问题</strong>等时，运行时间一般比较长。在回溯法中，<strong>DFS（深度优先搜索）</strong>是一种很重要的工具。<a id="more"></a></p><h2 id="1-1-DFS的基本思想"><a href="#1-1-DFS的基本思想" class="headerlink" title="1.1 DFS的基本思想"></a>1.1 DFS的基本思想</h2><p>（1）某一种可能情况向前探索，并生成一个子节点；</p><p>（2）过程中，一旦发现原来的选择不符合要求，就<strong>回溯</strong>至父亲结点，然后重新选择另一方向，再次生成子结点，继续向前探索；</p><p>（3）如此反复进行，直至求得最优解。</p><h2 id="1-2-回溯法的基本思想"><a href="#1-2-回溯法的基本思想" class="headerlink" title="1.2 回溯法的基本思想"></a>1.2 回溯法的基本思想</h2><p>（1）针对具体问题，定义问题的<strong>解空间</strong>；</p><p>（2）确定易于搜索的解空间结构（数据结构的选择）；</p><p>（3）一般以<strong>DFS</strong>的方式搜索解空间；</p><p>（4）在搜索过程中，可以使用<strong>剪枝函数</strong>等来优化算法。（剪枝函数：用<strong>约束函数</strong>和<strong>限界函数</strong>剪去得不到最优解的<strong>子树</strong>，统称为剪枝函数。）</p><blockquote><p>解空间：顾名思义，就是一个问题的所有解的集合。（但这离我们要求的<strong>最优解</strong>还差很远。）</p><p>约束条件：有效解的要求，即题目的要求。</p><p>约束函数：减去不满足约束条件的子树的函数。</p><p>限界函数：去掉得不到最优解的结点的函数。</p><p>扩展结点：当前正在产生子结点的结点称为扩展结点。</p></blockquote><p>回溯法处理的解空间类型主要分为以下两种：</p><ul><li><p><strong>子集树：</strong>当所给问题是从集合中找出满足某种性质的<strong>子集</strong>时，相应的解空间树称为子集树。</p></li><li><p><strong>排列树：</strong>当所给问题事从集合中确定满足某种性质的<strong>排列</strong>时，相应的解空间树称为排列树。</p></li></ul><h2 id="1-3-回溯法和DFS的区别"><a href="#1-3-回溯法和DFS的区别" class="headerlink" title="1.3 回溯法和DFS的区别"></a>1.3 回溯法和DFS的区别</h2><p>DFS是一种遍历搜索图、树等数据结构的一种算法，更像一种工具；</p><p>而回溯法则是为了解决问题不断地生成又放弃一些解决方案（解空间在搜索问题的过程中动态产生是回溯法的一个重要特点），直至找到最优解或搜索完毕为止的一种方法，更像一种指导思想，在解空间中利用DFS进行全面的搜索。</p><h2 id="1-4-剪枝"><a href="#1-4-剪枝" class="headerlink" title="1.4 剪枝"></a>1.4 剪枝</h2><p><strong>剪枝</strong>就是在搜索过程中利用<strong>过滤条件</strong>来剪去完全不用考虑（已经判断这条路走下去得不到最优解）的搜索路径，从而<strong>避免了一些不必要的搜索</strong>，优化算法求解速度，当然还必须得保证结果的正确性。</p><p>应用到回溯算法中，我们可以提前<strong>判断当前路径是否能产生结果集</strong>，如果否，就可以提前回溯。而这也叫做<strong>可行性剪枝</strong>。</p><p>另外还有一种叫做<strong>最优性剪枝</strong>，每次记录当前得到的最优值，如果当前结点已经无法产生<strong>比当前最优解更优的解</strong>时，可以提前回溯。</p><p>然而，剪枝的过滤条件不好找，想通过剪枝优化来提高算法高效性，既要保证结果正确性，还要保证剪枝的准确性。</p><h1 id="2-01背包问题：子集树"><a href="#2-01背包问题：子集树" class="headerlink" title="2. 01背包问题：子集树"></a>2. 01背包问题：子集树</h1><h2 id="2-1-问题介绍"><a href="#2-1-问题介绍" class="headerlink" title="2.1 问题介绍"></a>2.1 问题介绍</h2><p>01背包问题就是由子集树解决的一个经典问题。问题如下：</p><p>小明打算去拜访同学，他打算带一背包的巧克力作为礼物。他希望装进的巧克力总价值最高（这样可能比较好吃）。然而小明体力有限，巧克力包不能太重，只能有8kg。可供选择的巧克力如下：</p><div class="table-container"><table><thead><tr><th>序号</th><th>品牌</th><th>重量/kg</th><th>价值</th></tr></thead><tbody><tr><td>1</td><td>费列罗</td><td>4</td><td>45</td></tr><tr><td>2</td><td>好时之点</td><td>5</td><td>57</td></tr><tr><td>3</td><td>德芙</td><td>2</td><td>22</td></tr><tr><td>4</td><td>Cudie（西班牙）</td><td>1</td><td>11</td></tr><tr><td>5</td><td>自制</td><td>6</td><td>67</td></tr></tbody></table></div><h2 id="2-2-解决思路"><a href="#2-2-解决思路" class="headerlink" title="2.2 解决思路"></a>2.2 解决思路</h2><p><img src="/2023/06/27/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90%E4%B9%8B%E5%9B%9E%E6%BA%AF%E6%B3%95/1.jpg" alt="Image"></p><p>因为我们考虑的是找子集，所以每个物品只有选与不选两种状态，因此解空间是一个二叉树。在这个树中，每一层的边表示对一个物品的选择与否。如上图所示，选择第一层点0与左边点1间的边，表示选择1号物品，也就是选择左子树走下去；如果不选择1号物品入包，则进入右子树，选择右边点1。那么，一共有n件物品，就有n层的边，n+1层点。最后一层的每一个叶结点分别表示一种选择法，一共有2<sup>n</sup>个叶结点，即解空间中共有2<sup>n</sup>种解，我们要在这些叶结点中选择最佳结点。</p><p>我们先给出利用回溯法搜索子树集的伪代码框架：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">void search(层数)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span>(搜索到最底层)</span><br><span class="line">打印出结果解;</span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line"><span class="keyword">for</span>(遍历当前层解)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span>(合适解)</span><br><span class="line">继续搜索;</span><br><span class="line">撤消当前状态的影响; //回溯</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>回溯法讲究“暴力”。从暴力的角度思考，想把所有的尽量装满背包的搭配都找出来，需要标记每一种装法（每一个解）最大value，从而找到最优解。我们从第一种巧克力开始装，然后找下一个，判断能否装入，再递归，到达边界，比较，记录较优解，回溯，继续往下找……循环。从子集树的角度将，我们优先选择走左子树，也就是入包；当走到叶结点或不符合约束的重量条件时，回溯到父结点，进入右结点，最后遍历全树。</p><blockquote><p>判断能否装入后可以用一个book数组来标记是否选择入包。</p></blockquote><h2 id="2-3-算法实现"><a href="#2-3-算法实现" class="headerlink" title="2.3 算法实现"></a>2.3 算法实现</h2><p>由以上思路写出01背包问题的算法如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//01背包问题-回溯法-子集树 </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> n, bag_v, bag_w;</span><br><span class="line"><span class="keyword">int</span> bag[<span class="number">100</span>], x[<span class="number">100</span>], w[<span class="number">100</span>], val[<span class="number">100</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">//search递归函数，当前节点背包的价值为cur_v（current value），重量为cur_w（current weight）</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">search</span><span class="params">(<span class="keyword">int</span> cur, <span class="keyword">int</span> cur_v, <span class="keyword">int</span> cur_w)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(cur &gt; n) <span class="comment">//判断子集树的边界   </span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(cur_v &gt; bag_v) <span class="comment">//子集树对应的背包价值 是否超过了 最大价值</span></span><br><span class="line">        &#123;</span><br><span class="line">            bag_v = cur_v; <span class="comment">//得到最大价值</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)      </span><br><span class="line">                bag[i] = x[i]; <span class="comment">//x表示当前子集树各物品是否被选中，将选中的物品存入bag中 </span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= <span class="number">1</span>; j++) <span class="comment">//遍历当前解层：j 代表是否选择该物品</span></span><br><span class="line">        &#123;</span><br><span class="line">            x[cur] = j;      </span><br><span class="line">            <span class="keyword">if</span>(cur_w + x[cur]*w[cur] &lt;= bag_w) <span class="comment">//满足重量约束,继续向前寻找配对 </span></span><br><span class="line">            &#123;</span><br><span class="line">                cur_w += w[cur]*x[cur];</span><br><span class="line">                cur_v += val[cur]*x[cur];</span><br><span class="line">                search(cur + <span class="number">1</span>, cur_v, cur_w); <span class="comment">//递归，下一层物品 </span></span><br><span class="line">                <span class="comment">//清除痕迹，回溯上一层 </span></span><br><span class="line">                cur_w -= w[cur]*x[cur];   </span><br><span class="line">                cur_v -= val[cur]*x[cur];</span><br><span class="line">                x[cur] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    bag_v = <span class="number">0</span>; <span class="comment">//初始化背包最大价值</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//输入数据 </span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入背包最大容量:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; bag_w;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入物品个数:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"请依次输入物品的重量:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n; i++) </span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; w[i];</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"请依次输入物品的价值:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n; i++) </span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; val[i]; </span><br><span class="line">    search(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"最大价值为:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; bag_v &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"物品的编号依次为:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        <span class="keyword">if</span>(bag[i] == <span class="number">1</span>) </span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">PS E:\Code\VSCode\Learning\build&gt; .\main.exe</span><br><span class="line">请输入背包最大容量:</span><br><span class="line"><span class="number">8</span></span><br><span class="line">请输入物品个数:</span><br><span class="line"><span class="number">5</span></span><br><span class="line">请依次输入物品的重量:</span><br><span class="line"><span class="number">4</span> <span class="number">5</span> <span class="number">2</span> <span class="number">1</span> <span class="number">6</span></span><br><span class="line">请依次输入物品的价值:</span><br><span class="line"><span class="number">45</span> <span class="number">57</span> <span class="number">22</span> <span class="number">11</span> <span class="number">67</span></span><br><span class="line">最大价值为:</span><br><span class="line"><span class="number">90</span></span><br><span class="line">物品的编号依次为:</span><br><span class="line"><span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br></pre></td></tr></table></figure><h2 id="2-4-如何优化"><a href="#2-4-如何优化" class="headerlink" title="2.4 如何优化"></a>2.4 如何优化</h2><p>我们可以用一个上界函数bound()：当前价值+剩余容量可容纳的最大价值，去和目前的背包最大价值（也就是最优解）比较，如果bound()更小，那就没有继续搜索的意义了，剪去左子树，即不选择当前物品，进入右子树。</p><p>因为物品只有选与不选2个决策，而总共有n个物品，所以时间复杂度为O(2<sup>n</sup>)。因为递归栈最多达到n层，而且存储所有物品的信息也只需要常数个一维数组，所以最终的空间复杂度为O(n)。</p><p>那么，我们如何计算这个“剩余容量可容纳的最大价值”呢？首先，我们先将物品按照其单位重量价值从大到小排序，此后就按照顺序考虑各个物品。代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(cur_w+w[cur]&lt;=bag_w) <span class="comment">//将物品cur放入背包,搜索左子树,即选择当前物品 </span></span><br><span class="line">&#123;</span><br><span class="line">cur_w+=w[cur]; <span class="comment">//同步更新当前背包的重量</span></span><br><span class="line">cur_v+=val[cur]; <span class="comment">//同步更新当前背包的总价值</span></span><br><span class="line">put[cur]=<span class="number">1</span>;</span><br><span class="line">search(cur+<span class="number">1</span>,cur_v,cur_w); <span class="comment">//深度搜索进入下一层</span></span><br><span class="line">cur_w-=w[cur]; <span class="comment">//回溯复原</span></span><br><span class="line">cur_v-=val[cur]; <span class="comment">//回溯复原</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(bound(cur+<span class="number">1</span>,cur_v,cur_w)&gt;bag_v) <span class="comment">//如若符合条件则搜索右子树,即不选择当前物品 </span></span><br><span class="line">&#123;</span><br><span class="line">put[cur]=<span class="number">0</span>;</span><br><span class="line">search(cur+<span class="number">1</span>,cur_v,cur_w);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>当i&lt;=n，重量超过限制时，<strong>leftw为负</strong>，我们得到的是一个<strong>达不到的理想最大价值</strong>，因为此时最后放入的物品单位价值较高，但无法完全塞进书包，我们就去掉多余的部分，只取一部分该物体入包。当然，这是做不到的。因此计算出的值是一个达不到的理想值。</p></li><li><p>当i&gt;n，重量未超过限制时，则是可达到的最大价值。</p></li></ul><p>这样就解释了这个上界函数的优化。可以看出，这是一个最优性剪枝优化，判断当前结点是否有机会产生更优解。</p><p><strong>优化后的算法如下：</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> n, bag_v, bag_w;</span><br><span class="line"><span class="keyword">int</span> bag[<span class="number">100</span>], put[<span class="number">100</span>], w[<span class="number">100</span>], val[<span class="number">100</span>], order[<span class="number">100</span>];</span><br><span class="line"><span class="keyword">double</span> perp[<span class="number">100</span>]; </span><br><span class="line"></span><br><span class="line"><span class="comment">//按照单位重量价值排序，这里用冒泡 </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bubblesort</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i,j;</span><br><span class="line">    <span class="keyword">int</span> temporder = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">double</span> temp = <span class="number">0.0</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>;i &lt;= n; i++)</span><br><span class="line">        perp[i] = val[i] / w[i]; <span class="comment">//计算单位价值（单位重量的物品价值）</span></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n - <span class="number">1</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(j = i + <span class="number">1</span>; j &lt;= n; j++)</span><br><span class="line">            <span class="keyword">if</span>(perp[i] &lt; perp[j]) <span class="comment">//冒泡排序perp[], order[], sortv[], sortw[]</span></span><br><span class="line">        &#123;</span><br><span class="line">            temp = perp[i];  <span class="comment">//冒泡对perp[]排序交换 </span></span><br><span class="line">            perp[i] = perp[i];</span><br><span class="line">            perp[j] = temp;</span><br><span class="line"> </span><br><span class="line">            temporder = order[i]; <span class="comment">//冒泡对order[]交换 </span></span><br><span class="line">            order[i] = order[j];</span><br><span class="line">            order[j] = temporder;</span><br><span class="line"> </span><br><span class="line">            temp = val[i]; <span class="comment">//冒泡对val[]交换 </span></span><br><span class="line">            val[i] = val[j];</span><br><span class="line">            val[j] = temp;</span><br><span class="line"> </span><br><span class="line">            temp = w[i]; <span class="comment">//冒泡对w[]交换 </span></span><br><span class="line">            w[i] = w[j];</span><br><span class="line">            w[j] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//计算上界函数，功能为剪枝</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">bound</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> cur_v, <span class="keyword">int</span> cur_w)</span></span></span><br><span class="line"><span class="function"></span>&#123;   <span class="comment">//判断当前背包的总价值cur_v ＋ 剩余容量可容纳的最大价值 &lt;= 当前最优价值</span></span><br><span class="line">    <span class="keyword">double</span> leftw = bag_w - cur_w; <span class="comment">//剩余背包容量</span></span><br><span class="line">    <span class="keyword">double</span> b = cur_v; <span class="comment">//记录当前背包的总价值cur_v，最后求上界</span></span><br><span class="line">    <span class="comment">//以物品单位重量价值递减次序装入物品</span></span><br><span class="line">    <span class="keyword">while</span>(i &lt;= n &amp;&amp; w[i] &lt;= leftw)</span><br><span class="line">    &#123;</span><br><span class="line">        leftw -= w[i];</span><br><span class="line">        b += val[i];</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//装满背包</span></span><br><span class="line">    <span class="keyword">if</span>(i &lt;= n)</span><br><span class="line">        b += val[i] / w[i] * leftw;</span><br><span class="line">    <span class="keyword">return</span> b; <span class="comment">//返回计算出的上界</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">search</span><span class="params">(<span class="keyword">int</span> cur, <span class="keyword">int</span> cur_v, <span class="keyword">int</span> cur_w)</span></span></span><br><span class="line"><span class="function"></span>&#123;   <span class="comment">//search递归函数，当前current节点的价值为current value，重量为current weight </span></span><br><span class="line">    <span class="keyword">if</span>(cur &gt; n) <span class="comment">//判断边界   </span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(cur_v &gt; bag_v) <span class="comment">//是否超过了最大价值</span></span><br><span class="line">        &#123;</span><br><span class="line">            bag_v = cur_v; <span class="comment">//得到最大价值</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)      </span><br><span class="line">                bag[order[i]] = put[i]; <span class="comment">//put表示当前是否被选中，将选中的物品存入bag中 </span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//如若左子节点可行，则直接搜索左子树</span></span><br><span class="line">    <span class="comment">//对于右子树，先计算上界函数，以判断是否将其减去</span></span><br><span class="line">    <span class="keyword">if</span>(cur_w + w[cur] &lt;= bag_w) <span class="comment">//将物品cur放入背包,搜索左子树,即选择当前物品 </span></span><br><span class="line">    &#123;</span><br><span class="line">        cur_w += w[cur]; <span class="comment">//同步更新当前背包的重量</span></span><br><span class="line">        cur_v += val[cur]; <span class="comment">//同步更新当前背包的总价值</span></span><br><span class="line">        put[cur] = <span class="number">1</span>;</span><br><span class="line">        search(cur + <span class="number">1</span>, cur_v, cur_w); <span class="comment">//深度搜索进入下一层</span></span><br><span class="line">        cur_w -= w[cur]; <span class="comment">//回溯复原</span></span><br><span class="line">        cur_v -= val[cur]; <span class="comment">//回溯复原</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(bound(cur + <span class="number">1</span>, cur_v, cur_w) &gt; bag_v) <span class="comment">//如若符合条件则搜索右子树，即不选择当前物品 </span></span><br><span class="line">    &#123;</span><br><span class="line">        put[cur] = <span class="number">0</span>;</span><br><span class="line">        search(cur + <span class="number">1</span>, cur_v, cur_w);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    bag_v = <span class="number">0</span>; <span class="comment">//初始化背包最大价值</span></span><br><span class="line">    <span class="comment">//输入数据 </span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入背包最大容量:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; bag_w;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"请输入物品个数:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"请依次输入物品的重量:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n; i++) </span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; w[i];</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"请依次输入物品的价值:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n; i++) </span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; val[i];</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n; i++) <span class="comment">//新增的order数组，存储初始编号 </span></span><br><span class="line">        order[i] = i;</span><br><span class="line">    search(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"最大价值为:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; bag_v &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"物品的编号依次为:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        <span class="keyword">if</span>(bag[i] == <span class="number">1</span>) </span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="3-旅行商问题TSP：排序树"><a href="#3-旅行商问题TSP：排序树" class="headerlink" title="3. 旅行商问题TSP：排序树"></a>3. 旅行商问题TSP：排序树</h1><h2 id="3-1-问题介绍"><a href="#3-1-问题介绍" class="headerlink" title="3.1 问题介绍"></a>3.1 问题介绍</h2><p>小明在去同学那前想了一想，准备顺便拜访各高校的高中同学。他打算从本校出发，途径高中同学所在的一些高校，最终回到自己学校。小明很懒，希望只走最短的路，同时不想在一个学校玩第二次，因为他们不是主要目标。如何制定一个旅行方案？</p><p>乍一看这个题目是不是和最短路径问题很像？但很可惜的是，最短路径不要求通过每一个点，还是有所不同。</p><h2 id="3-2-解决思路"><a href="#3-2-解决思路" class="headerlink" title="3.2 解决思路"></a>3.2 解决思路</h2><p>排列树与子集树最大的区别在于，<strong>子集树</strong>的解是<strong>无序</strong>的子集，而<strong>排列树</strong>的解则包含整个集合的所有元素，我们从暴力的原则出发，将元素进行<strong>全排列</strong>。</p><p>​                                       <img src="/2023/06/27/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90%E4%B9%8B%E5%9B%9E%E6%BA%AF%E6%B3%95/2.jpg" alt="Image"></p><blockquote><p>{ } 外的数表示已经排好序，{ } 内的数表示尚未排序。</p></blockquote><p>在排序树中，<strong>每一层选择一个数字排到队尾</strong>，因此对一个n元素的集合，树的第一层将有n个子结点，表示可选n个数放在队伍的第一个位置，一次分叉比前一次减少一个（因为已经确定了一个位置的元素）；树共有<strong>n+1层</strong>（图中省略了最后一层），表示选择<strong>n次</strong>；叶结点共有<strong>n!</strong>个，表示组合数A，全排列共有n!种情形（因此时间复杂度也是n!）。</p><p>在这个问题中，我们的解空间就是所有城市的全排列，即走过每一个城市的顺序，因此可以用排序树来考虑这个问题。</p><h2 id="3-3-算法框架"><a href="#3-3-算法框架" class="headerlink" title="3.3 算法框架"></a>3.3 算法框架</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backtrack</span><span class="params">(<span class="keyword">int</span> t)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(t &gt; n)</span><br><span class="line">        output(x);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = t; i &lt;= n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            swap(x[t], x[i]);</span><br><span class="line">            <span class="keyword">if</span>(constraint(t) &amp;&amp; bound(t))</span><br><span class="line">                backtrack(t+<span class="number">1</span>);</span><br><span class="line">            swap(x[i],x[t]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里的swap是一个<strong>交换函数</strong>，对于一个排列，只要交换任意两数后就是一个新排列。constraint()和bound())分别是<strong>约束条件</strong>和<strong>限定函数</strong>（用于剪枝优化）。</p><p><strong>为什么要用swap来交换，而不是把数据放入新数组啦等等什么别的操作呢？</strong>这是因为，当我们在原先存储数据的数组x内进行交换时，我们<strong>把排好序的元素放到了数组的前面，留下的数据则是未排序的</strong>。这样在我们进行for循环的时候就能从t开始，同时避免了重复遇到排过序的数，也不需要book记录等多余的代码。</p><h2 id="3-4-算法实现"><a href="#3-4-算法实现" class="headerlink" title="3.4 算法实现"></a>3.4 算法实现</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//旅行商问题-回溯法-排序树 </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">int</span> n, t;</span><br><span class="line"><span class="keyword">int</span> dis[<span class="number">100</span>][<span class="number">100</span>], x[<span class="number">100</span>], bestroad[<span class="number">100</span>]; </span><br><span class="line"><span class="keyword">int</span> cur_dis, bestdis;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> INF=<span class="number">99999</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>&amp; a, <span class="keyword">int</span>&amp; b)</span>  <span class="comment">//swap函数，交换 </span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> temp;</span><br><span class="line">temp = a;</span><br><span class="line">a = b;</span><br><span class="line">b = temp;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backtrack</span><span class="params">(<span class="keyword">int</span> t)</span>   </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (t == n)</span><br><span class="line">&#123; <span class="comment">//判断边界。很长的判断，不能到自己或到不了，要比当前最优解短 </span></span><br><span class="line"><span class="keyword">if</span> (dis[x[n - <span class="number">1</span>]][x[n]] != <span class="number">0</span> &amp;&amp; dis[x[n]][<span class="number">1</span>] != <span class="number">0</span> &amp;&amp;(cur_dis + dis[x[n - <span class="number">1</span>]][x[n]] + dis[x[n]][<span class="number">1</span>] &lt; bestdis || bestdis == <span class="number">0</span>)) </span><br><span class="line">&#123;  <span class="comment">//记录最优路径，最优距离 </span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= n; j++)</span><br><span class="line">bestroad[j] = x[j];</span><br><span class="line">bestdis = cur_dis + dis[x[n<span class="number">-1</span>]][x[n]] + dis[x[n]][<span class="number">1</span>];</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j=t;j&lt;= n; j++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span>(dis[x[t]][x[j]]!=<span class="number">0</span>&amp;&amp; (cur_dis + dis[x[t - <span class="number">1</span>]][x[t]] + dis[x[t]][<span class="number">1</span>] &lt; bestdis || bestdis == <span class="number">0</span>))</span><br><span class="line">&#123;</span><br><span class="line">swap(x[t], x[j]);</span><br><span class="line">cur_dis += dis[x[t]][x[t<span class="number">-1</span>]];</span><br><span class="line">backtrack(t+<span class="number">1</span>);</span><br><span class="line"><span class="comment">//回溯 </span></span><br><span class="line">cur_dis -= dis[x[t]][x[t<span class="number">-1</span>]];</span><br><span class="line">swap(x[t], x[j]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i, j, m, a, b, c;</span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"输入城市数:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; n; </span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"输入路径数:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; </span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; m;</span><br><span class="line"><span class="comment">//初始化邻接矩阵</span></span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line"><span class="keyword">for</span>(j = <span class="number">1</span>; j &lt;= n; j++)</span><br><span class="line">dis[i][j] = <span class="number">0</span>;  </span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"输入路径与距离:"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//读入城市之间的距离</span></span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">&#123; </span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b &gt;&gt; c;</span><br><span class="line">dis[a][b] = dis[b][a] = c; <span class="comment">//无向图，两边都记录 </span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">x[i] = i;</span><br><span class="line"></span><br><span class="line">backtrack(<span class="number">2</span>);      </span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"最佳路径为:"</span>;</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; bestroad[i] &lt;&lt; <span class="string">" --&gt; "</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"1"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"最短距离为:"</span> &lt;&lt; bestdis;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">PS E:\Code\VSCode\Learning\build&gt; .<span class="string">"E:/Code/VSCode/Learning/build/main.exe"</span></span><br><span class="line">输入城市数:</span><br><span class="line"><span class="number">4</span></span><br><span class="line">输入路径数:</span><br><span class="line"><span class="number">6</span></span><br><span class="line">输入路径与距离:</span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">30</span></span><br><span class="line"><span class="number">1</span> <span class="number">3</span> <span class="number">6</span></span><br><span class="line"><span class="number">1</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line"><span class="number">2</span> <span class="number">3</span> <span class="number">5</span></span><br><span class="line"><span class="number">2</span> <span class="number">4</span> <span class="number">10</span></span><br><span class="line"><span class="number">3</span> <span class="number">4</span> <span class="number">20</span></span><br><span class="line">最佳路径为:<span class="number">1</span> --&gt; <span class="number">4</span> --&gt; <span class="number">2</span> --&gt; <span class="number">3</span> --&gt; <span class="number">1</span></span><br><span class="line">最短距离为:<span class="number">25</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：</p><ul><li><p>不同于最短路径，这里我们把<strong>INF（即无路径连通）与0（即自身）</strong>放在一起处理，因为他们都不需要swap。</p></li><li><p>我们用t==n，而不是t&gt;=n，是为了<strong>防止数组下表越界</strong>。</p></li></ul></blockquote><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><ul><li><p>回溯法作为一种极暴力的搜索法，其时间复杂度是极高的，子集树大概是2<sup>n</sup>，排序树大概是n！，所以处理大的问题不太给力。但作为回报，它能给出真正的最优解。</p></li><li><p>回溯法的子集树和排序树，可以处理两类问题，求子集最优和排序最优。</p></li><li><p>想要利用剪枝函数优化是非常困难的。</p></li></ul><p>参考文章：<a href="https://mp.weixin.qq.com/s?__biz=MzI3NTkyODIzNg==&amp;mid=2247484972&amp;idx=1&amp;sn=3549d22ce88e7a8326e8670a06e10ff6&amp;chksm=eb7c03efdc0b8af9d5558a07d7b509b56438f45ba22487f80d4aa4aa7ca7595152c29b196d2e&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">程序猿声：【算法学习】再谈回溯法</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-回溯法简介&quot;&gt;&lt;a href=&quot;#1-回溯法简介&quot; class=&quot;headerlink&quot; title=&quot;1. 回溯法简介&quot;&gt;&lt;/a&gt;1. 回溯法简介&lt;/h1&gt;&lt;p&gt;回溯法，又叫试探法，是一种寻找&lt;strong&gt;最优解&lt;/strong&gt;的&lt;strong&gt;暴力搜寻法。&lt;/strong&gt;由于暴力，回溯法的&lt;strong&gt;时间复杂度较高&lt;/strong&gt;，因此在比较一些数字较大的问题，比如&lt;strong&gt;最短路径问题&lt;/strong&gt;等时，运行时间一般比较长。在回溯法中，&lt;strong&gt;DFS（深度优先搜索）&lt;/strong&gt;是一种很重要的工具。&lt;/p&gt;</summary>
    
    
    
    <category term="算法设计与分析" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    
    
    <category term="算法设计与分析" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>动态规划算法</title>
    <link href="http://yoursite.com/2023/06/26/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/26/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-26T03:57:06.000Z</published>
    <updated>2023-06-26T05:51:00.666Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-动态规划简介"><a href="#1-动态规划简介" class="headerlink" title="1. 动态规划简介"></a>1. 动态规划简介</h1><h2 id="1-1-什么是动态规划"><a href="#1-1-什么是动态规划" class="headerlink" title="1.1 什么是动态规划"></a>1.1 什么是动态规划</h2><p>动态规划（Dynamic programming），是一种在数学、计算机科学和经济学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。动态规划算法是通过拆分问题，定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。</p><p>动态规划对于子问题重叠的情况特别有效，因为它将子问题的解保存在表格中，当需要某个子问题的解时，直接取值即可，从而避免重复计算。</p><p>动态规划是一种灵活的方法，不存在一种万能的动态规划算法可以解决各类最优化问题（每种算法都有它的缺陷）。所以除了要对基本概念和方法正确理解外，必须具体问题具体分析处理，用灵活的方法建立数学模型，用创造性的技巧去求解。<a id="more"></a></p><h2 id="1-2-基本策略"><a href="#1-2-基本策略" class="headerlink" title="1.2 基本策略"></a>1.2 基本策略</h2><p>动态规划中的子问题往往不是相互独立的（即子问题重叠）。在求解的过程中，许多子问题的解被反复地使用。为了避免重复计算，动态规划算法采用了填表来保存子问题解的方法。</p><p>动态规划与分治法类似，都是把大问题拆分成小问题，通过寻找大问题与小问题的递推关系，解决一个个小问题，最终达到解决原问题的效果。但不同的是，分治法在子问题和子子问题等上被重复计算了很多次，而动态规划则具有记忆性，通过填写表把所有已经解决的子问题答案纪录下来，在新问题里需要用到的子问题可以直接提取，避免了重复计算，从而节约了时间，所以在问题满足最优性原理之后，用动态规划解决问题的核心就在于填表，表填写完毕，最优解也就找到。</p><p>最优性原理是动态规划的基础，最优性原理是指“多阶段决策过程的最优决策序列具有这样的性质：不论初始状态和初始决策如何，对于前面决策所造成的某一状态而言，其后各阶段的决策序列必须构成最优策略”。</p><h2 id="1-3-适用问题及求解步骤"><a href="#1-3-适用问题及求解步骤" class="headerlink" title="1.3 适用问题及求解步骤"></a>1.3 适用问题及求解步骤</h2><p>那么什么样的问题适合用动态规划的方法来解决呢？适合用动态规划来解决的问题，都具有下面三个特点：最优化原理、无后效性、有重叠子问题。</p><p>（1）最优化原理：如果问题的最优解所包含的子问题的解也是最优的，就称该问题具有最优子结构，即满足最优化原理。</p><p>（2）无后效性：即某阶段状态一旦确定，就不受这个状态以后决策的影响。也就是说，某状态以后的过程不会影响以前的状态，只与当前状态有关。</p><p>（3）有重叠子问题：即子问题之间是不独立的，一个子问题在下一阶段决策中可能被多次使用到。（该性质并不是动态规划适用的必要条件，但是如果没有这条性质，动态规划算法同其他算法相比就不具备优势。</p><p>这类问题的求解步骤通常是：初始状态→│决策1│→│决策2│→…→│决策n│→结束状态。具体包括以下几步：</p><p>（1）划分：按照问题的特征，把问题分为若干阶段。注意：划分后的阶段一定是有序的或者可排序的。</p><p>（2）确定状态和状态变量：将问题发展到各个阶段时所处的各种不同的客观情况表现出来。状态的选择要满足无后续性。</p><p>（3）确定决策并写出状态转移方程：状态转移就是根据上一阶段的决策和状态来导出本阶段的状态。根据相邻两个阶段状态之间的联系来确定决策方法和状态转移方程。</p><p>（4）边界条件：状态转移方程是一个递推式，因此需要找到递推终止的条件。</p><h2 id="1-4-算法实现"><a href="#1-4-算法实现" class="headerlink" title="1.4 算法实现"></a>1.4 算法实现</h2><p>动态规划三要素：</p><p>（1）问题的阶段</p><p>（2）每个阶段的状态</p><p>（3）相邻两个阶段之间的递推关系</p><p>整个求解过程可以用一张最优决策表来描述，最优决策表是一张二维表（行：决策阶段，列：问题的状态）。表格需要填写的数据一般对应此问题在某个阶段某个状态下的最优值（如最短路径，最长公共子序列，最大价值等），填表的过程就是根据递推关系，最后根据整个表格的数据通过简单的取舍或者运算求得问题的最优解。</p><h1 id="2-01背包问题"><a href="#2-01背包问题" class="headerlink" title="2. 01背包问题"></a>2. 01背包问题</h1><h2 id="2-1-问题描述"><a href="#2-1-问题描述" class="headerlink" title="2.1 问题描述"></a>2.1 问题描述</h2><p>有n个物品，它们有各自的体积和价值，现有给定容量的背包，如何让背包里装入的物品具有最大的价值总和？如下表所示：</p><div class="table-container"><table><thead><tr><th style="text-align:center">i（物品编号）</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th></tr></thead><tbody><tr><td style="text-align:center">w（体积）</td><td style="text-align:center">2</td><td style="text-align:center">3</td><td style="text-align:center">4</td><td style="text-align:center">5</td></tr><tr><td style="text-align:center">v（价值）</td><td style="text-align:center">3</td><td style="text-align:center">4</td><td style="text-align:center">5</td><td style="text-align:center">6</td></tr></tbody></table></div><h2 id="2-2-总体思路"><a href="#2-2-总体思路" class="headerlink" title="2.2 总体思路"></a>2.2 总体思路</h2><p>根据动态规划解题步骤（问题抽象化、建立模型、寻找约束条件、判断是否满足最优性原理、找大问题与小问题的递推关系式、填表、寻找解组成）找出01背包问题的最优解以及解组成，然后编写代码实现。</p><h2 id="2-3-解决过程"><a href="#2-3-解决过程" class="headerlink" title="2.3 解决过程"></a>2.3 解决过程</h2><p>0、在解决问题之前，为描述方便，首先定义一些变量：capacity表示背包总容量，Vi表示第 i 个物品的价值，Wi表示第 i 个物品的体积，定义V(i,j)：当前背包容量 j，前 i 个物品最佳组合对应的价值。同时，将背包问题抽象化（X1，X2，…，Xn，其中 Xi 取0或1，表示第 i 个物品选或不选）。</p><p>1、建立模型，即求max(V1X1+V2X2+…+VnXn)；</p><p>2、寻找约束条件，W1X1+W2X2+…+WnXn&lt;capacity；</p><p>3、寻找递推关系式，面对当前商品有两种可能性：</p><ul><li>包的容量比该商品体积小，装不下，此时的价值与前i-1个的价值是一样的，即V(i,j)=V(i-1,j)；</li><li>还有足够的容量可以装该商品，但装了也不一定达到当前最优价值，所以在装与不装之间选择最优的一个，即V(i,j)=max｛V(i-1,j)，V(i-1,j-w(i))+v(i)｝。<br>其中V(i-1,j)表示不装，V(i-1,j-w(i))+v(i) 表示装了第i个商品，背包容量减少w(i)，但价值增加了v(i)。</li></ul><p>由此可以得出递推关系式：</p><script type="math/tex; mode=display">\begin{cases}j<w(i) & V(i,j)=V(i-1,j) \\j>=w(i) & V(i,j)=max｛V(i-1,j)，V(i-1,j-w(i))+v(i)｝\end{cases}</script><blockquote><p>这里需要解释一下，为什么能装的情况下，需要这样求解（这是本问题的关键所在）。可以这么理解，如果要到达 V(i,j) 这一个状态有几种方式？</p><p>肯定是两种，第一种是第i件商品没有装进去，第二种是第i件商品装进去了。没有装进去很好理解，就是V(i-1,j)；装进去了怎么理解呢？如果装进去第i件商品，那么装入之前是什么状态，肯定是V(i-1,j-w(i))。由于最优性原理（上文讲到），V(i-1,j-w(i))就是前面决策造成的一种状态，后面的决策就要构成最优策略。两种情况进行比较，得出最优。</p></blockquote><p>4、填表，首先初始化边界条件，V(0,j)=V(i,0)=0；</p><p><img src="/2023/06/26/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/1.jpg" alt></p><p>然后一行一行的填表：</p><p>如，i=1，j=1，w(1)=2，v(1)=3，有j<w(1)，故V(1,1)=V(1-1,1)=0； 又如，i="1，j=2，w(1)=2，v(1)=3，有j=w(1)，故V(1,2)=max｛" v(1-1,2)，v(1-1,2-w(1))+v(1) ｝="max｛0，0+3｝=3；" 如此下去，填到最后一个，i="4，j=8，w(4)=5，v(4)=6，有j">w(4)，故V(4,8)=max｛ V(4-1,8)，V(4-1,8-w(4))+v(4) ｝=max｛9，4+6｝=10。</w(1)，故V(1,1)=V(1-1,1)=0；></p><p>所以填完表如下图：</p><p><img src alt><img src="/2023/06/26/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/2.jpg" alt="2"></p><p>5、表格填完，最优解即是V(number,capacity)=V(4,8)=10。</p><h2 id="2-4-代码实现"><a href="#2-4-代码实现" class="headerlink" title="2.4 代码实现"></a>2.4 代码实现</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> w[<span class="number">5</span>] = &#123; <span class="number">0</span> , <span class="number">2</span> , <span class="number">3</span> , <span class="number">4</span> , <span class="number">5</span> &#125;; <span class="comment">//商品的体积2、3、4、5</span></span><br><span class="line"><span class="keyword">int</span> v[<span class="number">5</span>] = &#123; <span class="number">0</span> , <span class="number">3</span> , <span class="number">4</span> , <span class="number">5</span> , <span class="number">6</span> &#125;; <span class="comment">//商品的价值3、4、5、6</span></span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">5</span>][<span class="number">9</span>] = &#123; <span class="number">0</span> &#125;; <span class="comment">//动态规划表</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">4</span>; i++) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= <span class="number">8</span>; j++) &#123;</span><br><span class="line"><span class="keyword">if</span> (j &lt; w[i])</span><br><span class="line">dp[i][j] = dp[i - <span class="number">1</span>][j];</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">dp[i][j] = <span class="built_in">std</span>::max(dp[i - <span class="number">1</span>][j], dp[i - <span class="number">1</span>][j - w[i]] + v[i]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">//动态规划表的输出</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">9</span>; j++) &#123;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; dp[i][j] &lt;&lt; <span class="string">' '</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-5-最优解回溯"><a href="#2-5-最优解回溯" class="headerlink" title="2.5 最优解回溯"></a>2.5 最优解回溯</h2><p>通过上面的方法可以求出背包问题的最优解，但还不知道这个最优解由哪些商品组成，故要根据最优解回溯找出解的组成，根据填表的原理可以有如下的寻解方式：</p><ul><li>V(i,j)=V(i-1,j)时，说明没有选择第i 个商品，则回到V(i-1,j)；</li><li>V(i,j)=V(i-1,j-w(i))+v(i)时，说明装了第i个商品，该商品是最优解组成的一部分，随后我们得回到装该商品之前，即回到V(i-1,j-w(i))，一直遍历到i＝0结束为止，所有解的组成都会找到。</li></ul><p>具体如下：</p><ul><li>最优解为V(4,8)=10，而V(4,8)!=V(3,8)，V(4,8)=V(3,8-w(4))+v(4)=V(3,3)+6=4+6=10，所以第4件商品被选中，并且回到V(3,8-w(4))=V(3,3)；</li><li>有V(3,3)=V(2,3)=4，所以第3件商品没被选择，回到V(2,3)；</li><li>而V(2,3)!=V(1,3)，V(2,3)=V(1,3-w(2))+v(2)=V(1,0)+4=0+4=4，所以第2件商品被选中，并且回到V(1,3-w(2))=V(1,0)；</li><li>有V(1,0)=V(0,0)=0，所以第1件商品没被选择。</li></ul><p>代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> w[<span class="number">5</span>] = &#123; <span class="number">0</span> , <span class="number">2</span> , <span class="number">3</span> , <span class="number">4</span> , <span class="number">5</span> &#125;; <span class="comment">//商品的体积2、3、4、5</span></span><br><span class="line"><span class="keyword">int</span> v[<span class="number">5</span>] = &#123; <span class="number">0</span> , <span class="number">3</span> , <span class="number">4</span> , <span class="number">5</span> , <span class="number">6</span> &#125;; <span class="comment">//商品的价值3、4、5、6</span></span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">5</span>][<span class="number">9</span>] = &#123; <span class="number">0</span> &#125;; <span class="comment">//动态规划表</span></span><br><span class="line"><span class="keyword">int</span> item[<span class="number">5</span>]; <span class="comment">//最优解情况</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//寻找最优解</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">findWhat</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (i &gt;= <span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">if</span> (dp[i][j] == dp[i - <span class="number">1</span>][j]) &#123;</span><br><span class="line">item[i] = <span class="number">0</span>;</span><br><span class="line">findWhat(i - <span class="number">1</span>, j);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (j - w[i] &gt;= <span class="number">0</span> &amp;&amp; dp[i][j] == dp[i - <span class="number">1</span>][j - w[i]] + v[i]) &#123;</span><br><span class="line">item[i] = <span class="number">1</span>;</span><br><span class="line">findWhat(i - <span class="number">1</span>, j - w[i]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//动态规划算法</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">4</span>; i++) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= <span class="number">8</span>; j++) &#123;</span><br><span class="line"><span class="keyword">if</span> (j &lt; w[i])</span><br><span class="line">dp[i][j] = dp[i - <span class="number">1</span>][j];</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">dp[i][j] = <span class="built_in">std</span>::max(dp[i - <span class="number">1</span>][j], dp[i - <span class="number">1</span>][j - w[i]] + v[i]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//动态规划表的输出</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">9</span>; j++) &#123;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; dp[i][j] &lt;&lt; <span class="string">' '</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//寻找最优解</span></span><br><span class="line">findWhat(<span class="number">4</span>, <span class="number">8</span>);</span><br><span class="line"><span class="comment">//输出最优解</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++)</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; item[i] &lt;&lt; <span class="string">' '</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-动态规划简介&quot;&gt;&lt;a href=&quot;#1-动态规划简介&quot; class=&quot;headerlink&quot; title=&quot;1. 动态规划简介&quot;&gt;&lt;/a&gt;1. 动态规划简介&lt;/h1&gt;&lt;h2 id=&quot;1-1-什么是动态规划&quot;&gt;&lt;a href=&quot;#1-1-什么是动态规划&quot; class=&quot;headerlink&quot; title=&quot;1.1 什么是动态规划&quot;&gt;&lt;/a&gt;1.1 什么是动态规划&lt;/h2&gt;&lt;p&gt;动态规划（Dynamic programming），是一种在数学、计算机科学和经济学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。动态规划算法是通过拆分问题，定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。&lt;/p&gt;
&lt;p&gt;动态规划对于子问题重叠的情况特别有效，因为它将子问题的解保存在表格中，当需要某个子问题的解时，直接取值即可，从而避免重复计算。&lt;/p&gt;
&lt;p&gt;动态规划是一种灵活的方法，不存在一种万能的动态规划算法可以解决各类最优化问题（每种算法都有它的缺陷）。所以除了要对基本概念和方法正确理解外，必须具体问题具体分析处理，用灵活的方法建立数学模型，用创造性的技巧去求解。&lt;/p&gt;</summary>
    
    
    
    <category term="算法设计与分析" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    
    
    <category term="算法设计与分析" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络CNN</title>
    <link href="http://yoursite.com/2023/06/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/"/>
    <id>http://yoursite.com/2023/06/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/</id>
    <published>2023-06-24T02:01:26.000Z</published>
    <updated>2023-06-24T02:13:10.688Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-卷积神经网络简介"><a href="#1-卷积神经网络简介" class="headerlink" title="1. 卷积神经网络简介"></a>1. 卷积神经网络简介</h1><p>卷积神经网络(convolutional neural network, CNN)，是一类包含卷积计算且具有深度结构的前馈神经网络。卷积神经网络是受生物学上感受野（Receptive Field）的机制而提出的。卷积神经网络专门用来处理具有类似网格结构的数据的神经网络。例如，时间序列数据（可以认为是在时间轴上有规律地采样形成的一维网格）和图像数据（可以看作是二维的像素网格）。<a id="more"></a></p><h1 id="2-卷积神经网络原理"><a href="#2-卷积神经网络原理" class="headerlink" title="2. 卷积神经网络原理"></a>2. 卷积神经网络原理</h1><p>典型的CNN由卷积层、池化层、全连接层3个部分构成：</p><ul><li>卷积层：负责提取图像中的局部特征；</li><li>池化层：大幅降低参数量级（降维）；</li><li>全连接层：类似传统神经网络的部分，用来输出想要的结果。</li></ul><h2 id="2-1-卷积层（convolutional-layer）"><a href="#2-1-卷积层（convolutional-layer）" class="headerlink" title="2.1 卷积层（convolutional layer）"></a>2.1 卷积层（convolutional layer）</h2><p>卷积可以理解为使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。</p><p>在具体应用中，往往有多个卷积核，可以认为「每个卷积核代表了一种图像模式」，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了 6 个卷积核，可以理解：我们认为这个图像上有 6 种底层纹理模式，也就是我们用 6 种基础模式就能描绘出一副图像。</p><p>卷积层通过卷积核的过滤提取出图片中局部的特征，与人类视觉的特征提取类似。</p><h2 id="2-2-池化层（pooling-layer）"><a href="#2-2-池化层（pooling-layer）" class="headerlink" title="2.2 池化层（pooling layer）"></a>2.2 池化层（pooling layer）</h2><p>池化层简单说就是下采样，他可以大大降低数据的维度。需要池化层的原因：即使做完了卷积，图像仍然很大（因为卷积核通常比较小），所以为了降低数据维度，就进行下采样。池化层函数实际上是一个统计函数，例如最大池化、平均池化、累加池化等。</p><p>那么池化层函数会不会对图像数据产生副作用呢？答案是：一般不会。</p><p>关于池化层，有一个局部线性变换的不变性（invariant）理论：如果输入数据的局部进行了线性变换操作（如平移或旋转等），那么经过池化操作后，输出的结果并不会发生变化。局部平移“不变性”特别有用，尤其是我们关心某个特征是否出现，而不关心它出现的位置时（例如，在模式识别场景中，当我们检测人脸时，我们只关心图像中是否具备人脸的特征，而并不关心人脸是在图像的左上角和右下角）。</p><p>为什么池化层可以降低过拟合的概率呢？因为池化函数使得模型更关注偏全局的特征（而非局部），所以可以尽量避免让模型专注于图像的一些特化细节（例如让模型更关注一张人脸，而不是他眼睛的大小）。</p><p>总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</p><h2 id="2-3-全连接层（fully-connected-layer）"><a href="#2-3-全连接层（fully-connected-layer）" class="headerlink" title="2.3 全连接层（fully-connected layer）"></a>2.3 全连接层（fully-connected layer）</h2><p>经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。“全连接”意味着，前层网络中的所有神经元都与下一层的所有神经元连接。</p><p>全连接层设计目的在于：它将前面各个层学习到的“分布式特征表示”映射到“样本标记空间”，然后利用损失函数来调控学习过程，最后给出对象的分类预测。</p><p>虽然池化层看似是整个网络结构中最不起眼的一步，但是由于其对所有的参数进行“连接”，其会造成大量的冗余参数，不良的设计会导致在全连接层极易出现「过拟合」的现象，对此，可以使用 Dropout 方法来缓解；同时其极高的参数量会导致性能的降低，对此，颜水成博士团队曾发表论文 Network in Network（NIN），提出使用全局均值池化策略（Global Average Pooling，GAP）取代全连接层。</p><h1 id="3-卷积神经网络特点"><a href="#3-卷积神经网络特点" class="headerlink" title="3. 卷积神经网络特点"></a>3. 卷积神经网络特点</h1><p><strong><em>优点</em></strong><br>（1）共享卷积核，对高维数据处理无压力<br>（2）无需手动选取特征，训练好权重，即得特征分类效果好</p><p><strong><em>缺点</em></strong><br>（1）需要调参，需要大样本量，训练最好要GPU<br>（2）物理含义不明确（也就说，我们并不知道没个卷积层到底提取到的是什么特征，而且神经网络本身就是一种难以解释的“黑箱模型”）</p><h1 id="4-卷积神经网络的常见网络结构及框架"><a href="#4-卷积神经网络的常见网络结构及框架" class="headerlink" title="4. 卷积神经网络的常见网络结构及框架"></a>4. 卷积神经网络的常见网络结构及框架</h1><h2 id="4-1-网络结构"><a href="#4-1-网络结构" class="headerlink" title="4.1 网络结构"></a>4.1 网络结构</h2><ul><li>LeNet，这是最早用于数字识别的CNN。</li><li>AlexNet，2012 ILSVRC比赛远超第2名的CNN，比LeNet更深，用多层小卷积层叠加替换单大卷积层。</li><li>ZF Net，2013 ILSVRC比赛冠军。</li><li>GoogLeNet，2014 ILSVRC比赛冠军。</li><li>VGGNet，2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好。</li></ul><h2 id="4-2-框架"><a href="#4-2-框架" class="headerlink" title="4.2 框架"></a>4.2 框架</h2><p>（1）Caffe</p><ul><li><p>源于Berkeley的主流CV工具包，支持C++,python,matlab</p></li><li><p>Model Zoo中有大量预训练好的模型供使用</p></li></ul><p>（2）PyTorch</p><ul><li><p>Facebook用的卷积神经网络工具包</p></li><li><p>通过时域卷积的本地接口，使用非常直观</p></li><li><p>定义新网络层简单</p></li></ul><p>（3）TensorFlow</p><ul><li><p>Google的深度学习框架</p></li><li><p>TensorBoard可视化很方便</p></li><li><p>数据和模型并行化好，速度快</p></li></ul><h1 id="5-卷积神经网络的Python应用"><a href="#5-卷积神经网络的Python应用" class="headerlink" title="5. 卷积神经网络的Python应用"></a>5. 卷积神经网络的Python应用</h1><p>下面以TensorFlow框架为例，说明Python中CNN的应用。TensorFlow中CNN的相关函数有卷积函数和池化函数。</p><h2 id="5-1-卷积函数"><a href="#5-1-卷积函数" class="headerlink" title="5.1 卷积函数"></a>5.1 卷积函数</h2><p>卷积函数定义在tensorflow/python/ops下的nn_impl.py和nn_ops.py文件中。</p><p>它包括了很多类型的卷积函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br><span class="line">tf.nn.depthwise_conv2d(input, filter, strides, padding, name=<span class="literal">None</span>)</span><br><span class="line">tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, name=<span class="literal">None</span>)</span><br><span class="line">……</span><br></pre></td></tr></table></figure><p>在这里，我们只对平时用的比较多的二维卷积进行介绍。其他函数的使用方法跟二维卷积是一样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><blockquote><p><em>参数说明：</em></p><p>input：需要做卷积的输入数据。注意：这是一个4维的张量（[batch, in_height, in_width, in_channels]）。对于图像数据来说，batch是这一批样本的个数，in_height和in_width是图像的尺寸，in_channels是图像的通道数，而且要求图像的类型为float32或float64。因此，我们在对图像进行处理的时候，首先要把图像转换成这种特定的类型。</p><p>filter：卷积核。这也是一个4维的张量（[filter_height, filter_width, in_channels, out_channels]）。filter_height,和filter_width是图像的尺寸，in_channels,是输入的通道数，out_channels是输出的通道数。</p><p>strides：图像每一维的步长。是一个一维向量，长度为4。</p><p>padding：定义元素边框与元素内容之间的空间。这里只能选择”SAME”或”VALID”，这个值决定了不同的卷积方式。当它为”SAME”时，表示边缘填充，适用于全尺寸操作；当它为”VALID”时，表示边缘不填充。</p><p>use_cudnn_on_gpu：bool类型，是否使用cudnn加速。</p><p>name：该操作的名称。</p><p>返回值：返回一个张量（tensor），即特征图（feature map）。</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#卷积函数</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)</span></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">4</span>), dtype=np.float32)</span><br><span class="line">filter_data =  tf.Variable(np.random.rand(<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>), dtype=np.float32)</span><br><span class="line"><span class="comment"># y = tf.nn.conv2d(input_data, filter_data, strides=[1,1,1,1], padding = 'SAME')</span></span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">print(input_data)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><h2 id="5-2-池化函数"><a href="#5-2-池化函数" class="headerlink" title="5.2 池化函数"></a>5.2 池化函数</h2><p>池化函数定义在tensorflow/python/ops下的nn.py和gen_nn_ops.py文件中。</p><p>我们用的比较多的是下面这两个池化函数：</p><ul><li><p>最大池化：tf.nn.max_pool(value, ksize, strides, padding, name=None)</p></li><li><p>平均池化：tf.nn.avg_pool(value, ksize, strides, padding, name=None)</p></li></ul><p>这里所需要指定的输入参数，跟我们之前介绍的二维卷积函数是一样的：</p><blockquote><p>value：需要池化的输入。一般池化层接在卷积层后面，所以输入通常是conv2d所输出的feature map，依然是4维的张量（[batch, height, width, channels]）。</p><p>ksize：池化窗口的大小。由于一般不在batch和channel上做池化，所以ksize一般是[1,height, width,1]。</p><p>strides：图像每一维的步长。是一个一维向量，长度为4。</p><p>padding：和卷积函数中padding含义一样。</p><p>name：该操作的名称。</p><p>返回值：返回一个张量（tensor）。</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#池化函数</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_data = tf.Variable(np.random.rand(<span class="number">10</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">4</span>), dtype=np.float32)</span><br><span class="line">filter_data =  tf.Variable(np.random.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>), dtype=np.float32)</span><br><span class="line">y = tf.nn.conv2d(input_data, filter_data, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大池化</span></span><br><span class="line"><span class="comment"># tf.nn.max_pool(value, ksize, strides, padding, name=None) </span></span><br><span class="line">output = tf.nn.max_pool(value=y,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 平均池化</span></span><br><span class="line"><span class="comment"># tf.nn.avg_pool(value, ksize, strides, padding, name=None)</span></span><br><span class="line"><span class="comment"># output = tf.nn.avg_pool(value=y,ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv:'</span>,y)</span><br><span class="line">print(<span class="string">'pool_padding_valid:'</span>,output)</span><br></pre></td></tr></table></figure><h1 id="6-源码仓库地址"><a href="#6-源码仓库地址" class="headerlink" title="6. 源码仓库地址"></a>6. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-卷积神经网络简介&quot;&gt;&lt;a href=&quot;#1-卷积神经网络简介&quot; class=&quot;headerlink&quot; title=&quot;1. 卷积神经网络简介&quot;&gt;&lt;/a&gt;1. 卷积神经网络简介&lt;/h1&gt;&lt;p&gt;卷积神经网络(convolutional neural network, CNN)，是一类包含卷积计算且具有深度结构的前馈神经网络。卷积神经网络是受生物学上感受野（Receptive Field）的机制而提出的。卷积神经网络专门用来处理具有类似网格结构的数据的神经网络。例如，时间序列数据（可以认为是在时间轴上有规律地采样形成的一维网格）和图像数据（可以看作是二维的像素网格）。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>人工神经网络ANN</title>
    <link href="http://yoursite.com/2023/06/23/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CANN/"/>
    <id>http://yoursite.com/2023/06/23/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CANN/</id>
    <published>2023-06-23T01:23:25.000Z</published>
    <updated>2023-06-23T01:32:59.831Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-人工神经网络简介"><a href="#1-人工神经网络简介" class="headerlink" title="1. 人工神经网络简介"></a>1. 人工神经网络简介</h1><h2 id="1-1-生物神经网络"><a href="#1-1-生物神经网络" class="headerlink" title="1.1 生物神经网络"></a>1.1 生物神经网络</h2><p>人工神经网络的灵感来自其生物学对应物。生物神经网络使大脑能够以复杂的方式处理大量信息。大脑的生物神经网络由大约1000亿个神经元组成，这是大脑的基本处理单元。神经元通过彼此之间巨大的连接（称为突触）来执行其功能。<a id="more"></a></p><p>人体神经元模型如下图所示：</p><p><img src="/2023/06/23/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CANN/1.png" alt="img"> </p><ul><li>接收区（receptive zone）：树突接收到输入信息。</li><li>触发区（trigger zone）：位于轴突和细胞体交接的地方，决定是否产生神经冲动。</li><li>传导区（conducting zone）：由轴突进行神经冲动的传递。</li><li>输出区（output zone）：神经冲动的目的就是要让神经末梢，突触的神经递质或电力释出，才能影响下一个接受的细胞（神经元、肌肉细胞或是腺体细胞），此称为突触传递。</li></ul><h2 id="1-2-人工神经网络"><a href="#1-2-人工神经网络" class="headerlink" title="1.2 人工神经网络"></a>1.2 人工神经网络</h2><p>人工神经网络定义为：人工神经网络是一种由具有自适应性的简单单元构成的广泛并行互联的网络，它的组织结构能够模拟生物神经系统对真实世界所做出的交互反应。</p><p>人工神经网络的结构如下图所示：</p><p><img src="/2023/06/23/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CANN/2.png" alt="img"> </p><ul><li>输入层：输入层接收特征向量 x 。</li><li>输出层：输出层产出最终的预测 h 。</li><li>隐含层：隐含层介于输入层与输出层之间，之所以称之为隐含层，是因为当中产生的值并不像输入层使用的样本矩阵 X 或者输出层用到的标签矩阵 y 那样直接可见。</li></ul><p>人工神经网络由一个输入层和一个输出层组成，其中输入层从外部源（数据文件，图像，硬件传感器等）接收数据，一个或多个隐藏层处理数据，输出层提供一个或多个数据点基于网络的功能。根据不同的问题，可以加入多个隐藏层，由图中这种全连接改为部 分连接，甚至是环形连接等。</p><h1 id="2-人工神经网络原理"><a href="#2-人工神经网络原理" class="headerlink" title="2. 人工神经网络原理"></a>2. 人工神经网络原理</h1><p>人工神经网络的强大之处在于，它拥有很强的学习能力。在得到一个训练集之后，它能通过学习提取所观察事物的各个部分的特征，将特征之间用不同网络节点连接，通过训练连接的网络权重，改变每一个连接的强度，直到顶层的输出得到正确的答案。</p><p>神经网络算法大致过程如下：<br>第一步，我们要预先设定一种网络结构和激活函数，这一步其实很困难，因为网络结构可以无限拓展，要知道什么样的结构才符合我们的问题需要做大量的试验。<br>第二步，初始化模型中的权重。模型中的每一个连接都会有一个权重，在初始化的时候可以都随机给予一个值。<br>第三步，就是根据输入数据和权重来预测结果。由于最开始的参数都是随机设置的，所以获得的结果肯定与真实的结果差距比较大，所以在这里要计算一个误差，误差反映了预测结果和真实结果的差距有多大。<br>最后一步，模型要调节权重。这里我们可以参与的就是需要设置一个“学习率”，这个学习率是针对误差的，每次获得误差后，连接上的权重都会按照误差的这个比率来进行调整，从而期望在下次计算时获得一个较小的误差。经过若干次循环这个过程，我们可以选择达到一个比较低的损失值的时候停止并输出模型，也可以选择一个确定的循环轮次来结束。</p><h2 id="2-1-ANN的基本构造"><a href="#2-1-ANN的基本构造" class="headerlink" title="2.1 ANN的基本构造"></a>2.1 ANN的基本构造</h2><p>人工神经网络是许多神经元经联接而成的网络结构。因此，ANN的构造有两层含义：1. 神经元的结构；2. 网络互联结构(拓扑结构)。</p><h3 id="2-1-1-神经元的结构模型"><a href="#2-1-1-神经元的结构模型" class="headerlink" title="2.1.1 神经元的结构模型"></a>2.1.1 神经元的结构模型</h3><p>根据前面对生物神经元的分析，应具有以下特点：<br>（a） 神经元是一个多输入、单输出的元件。<br>（b） 神经元是一个具有非线性输入/输出特性的元件。表现在只有当来自各神经突触的活动电位达到一定强度后，该神经才能被激活，释放出神经传递化学物质，发出本身的活动电位脉冲。<br>（c） 神经元的连接具有可塑性，表现在其活动电位脉冲的传递强度依靠神经传递化学物质的释放量及突触间隙的变化量，可以进行调节。<br>（d） 神经元的输出响应，是各个输入的综合结果。</p><h3 id="2-1-2-网络拓扑结构"><a href="#2-1-2-网络拓扑结构" class="headerlink" title="2.1.2 网络拓扑结构"></a>2.1.2 网络拓扑结构</h3><p>网络拓扑结构即神经元的联接形式，从大的方面来看，ANN网络拓扑结构可分为层次结构、模块结构和层次模块结构等几种。<br>1.层次结构：神经元的联接按层次排列。<br>2.模块结构：主要特点是将整个网络按功能划分为不同的模块，每个模块内部的神经元紧密互联，并完成各自特定的功能，模块之间再互联以完成整体功能。<br>3.层次模块结构：将模块结构和层次结构结合起来，使之更接近人脑神经系统的结构，这也是目前为人们广泛注意的一种新型网络互联模式。</p><p>根据网络中神经元的层数不同，可将神经网络分为单层网络和多层网络；</p><p>根据同层网络神经元之间有无相互联接以及后层神经元与前层神经元有无反馈作用的不同，可将神经网络分为以下多种。<br>（a）前向网络: 网络中的神经元分层排列，每个神经元只与前一层神经元相连，层间神经元无连接。最上一层为输出层，最下一层为输入层，中间层称为隐层。<br>（b）从输出到输入有反馈的前向网络: 从输出到输入有反馈环节的前向网络。<br>（c）层内互连前向网络: 通过层内神经元的相互连接，可以实现同一层神经元间的相互制约，从而可以将层内神经元分为几组，让每组作为一个整体来动作。<br>（d）互连网络:分为局部互连和全互连两种。全互连网络中每个神经元的输出都与其他神经元相连；而局部互连网络中，有些神经元间没有连接关系。</p><h2 id="2-2-学习规则"><a href="#2-2-学习规则" class="headerlink" title="2.2 学习规则"></a>2.2 学习规则</h2><p>神经网络要能工作必须首先进行学习，学习规则多种多样，一般可以归结为以下两类：<br>（1）有指导学习：不但需要学习用的输入事例（训练样本，通常为一矢量），同时还要求与之对应的表示所需期望输出的目标矢量。进行学习时，首先计算一个输入矢量的网络输出，然后同相应的目标输出比较，比较结果的误差用来按规定的算法改变加权。<br>（2）无指导学习：不要求有目标矢量，网络通过自身的“经历”来学会某种功能，在学习时，关键不在于网络实际输出是否与外部的期望输出相一致，而在于调整权重以反映学习样本的分布，因此整个训练过程实质是抽取训练样本集的统计特性。特别适用于对未知事物的研究。</p><p>工程实践中，有指导学习和无指导学习并不是相互冲突的，目前已经出现了一些融合有指导学习和无指导学习的训练算法。如在应用有指导学习训练一个网络后，再利用一些后期的无指导学习来使得网络自适应于环境的变化。</p><h2 id="2-3-学习算法"><a href="#2-3-学习算法" class="headerlink" title="2.3 学习算法"></a>2.3 学习算法</h2><p>学习算法是人工神经网络研究的主要内容和中心环节，许多性能各异的神经网络的差异也主要体现在学习算法的不同上，同时，对于神经网络学习算法也是至今人们研究得最多的一个方面。</p><p>截止目前，人们已先后提出了误差反向传播算法(BP算法)、Hopfield算法、自适应共振理论算法(ART算法)、自组织特征映射算法(Kohonen算法)等。</p><p>误差反向传播算法简称BP算法，它是Werbos等人提出的一个有监督训练的多层神经网络算法。在网络学习阶段，其每一个训练范例在网络中经过两个方向的传递计算。一遍向前传播计算，从输入层开始传递至各层，经过处理后产生一个输出，由此可得到一个该实际输出与其理想输出之差的误差矢量；此后，再进行反向传播计算，即从输出层开始至输入层结束，根据误差矢量并以一定的速度对各权值依次进行修正。</p><p>BP算法有很强的数学基础，扩展了神经网络的应用范围，已有许多成功的应用实例，对神经网络研究的再次兴起起过很大作用。</p><h1 id="3-人工神经网络特点"><a href="#3-人工神经网络特点" class="headerlink" title="3. 人工神经网络特点"></a>3. 人工神经网络特点</h1><p><strong><em>优点：</em></strong><br>（1）具有自学习功能 。<br>（2）具有联想存储功能。<br>（3）具有高速寻找优化解的能力。</p><p><strong><em>缺点：</em></strong><br>（1）神经网络需要大量数据，非常消耗资源，开销也非常大的，而且训练时间长，还需要耗费很大的人力物力。<br>（2）神经网络在概括方面很不好。<br>（3）神经网络是不透明的。</p><h1 id="4-人工神经网络的Python应用"><a href="#4-人工神经网络的Python应用" class="headerlink" title="4. 人工神经网络的Python应用"></a>4. 人工神经网络的Python应用</h1><p>在 Scikit 中神经网络被称为多层感知器（Multi-layer Perceptron），它可以用于分类或回归的非线性函数。用于分类的模块是 MLPClassifier，而用于回归的模块则是 MLPRegressor。</p><p>MLPClassifier 主要用来做分类，下面用 MLPClassifier 在鸢尾花数据上做测试。其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf = MLPClassifier(solver=’sgd’,activation=’relu’,alpha=<span class="number">1e-4</span>,hidden_layer_sizes=(<span class="number">50</span>,<span class="number">50</span>),random_state=<span class="number">1</span>,max_iter=<span class="number">10</span>,verbose=<span class="number">10</span>,learning_rate_init=<span class="number">.1</span>)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：</p><p>hidden_layer_sizes：例如hidden_layer_sizes=(50, 50)，表示有两层隐藏层，第一层隐藏层有50个神经元，第二层也有50个神经元。</p><p>Activation：{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}，默认‘relu’。</p><p>solver： {‘lbfgs’, ‘sgd’, ‘adam’}, 默认adam，用来优化权重。</p><p>alpha：float,可选的，默认0.0001，正则化项参数。</p><p>learning_rate：学习率,用于权重更新,只有当solver为’sgd’时使用，{‘constant’，’invscaling’, ‘adaptive’}，默认constant。</p><p>random_state：int 或RandomState，可选，默认None，随机数生成器的状态或种子。</p><p>max_iter：int，可选，默认200，最大迭代次数。</p><p>verbose：bool, 可选，默认False，是否将过程打印到stdout。</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">iris = load_iris()  <span class="comment"># 导入数据集</span></span><br><span class="line">X = iris[<span class="string">'data'</span>]  <span class="comment"># 获取自变量数据</span></span><br><span class="line">y = iris[<span class="string">'target'</span>]  <span class="comment"># 获取因变量数据</span></span><br><span class="line">X_train, X_test, y_train, y_test = \</span><br><span class="line">    train_test_split(X, y, test_size=<span class="number">0.2</span>)  <span class="comment"># 分割训练集和测试集</span></span><br><span class="line">clf = MLPClassifier(solver=<span class="string">'adam'</span>, alpha=<span class="number">1e-5</span>, \</span><br><span class="line">                    hidden_layer_sizes=(<span class="number">3</span>,<span class="number">3</span>), random_state=<span class="number">1</span>,max_iter=<span class="number">100000</span>,) <span class="comment"># 创建神经网络分类器对象</span></span><br><span class="line">clf.fit(X, y) <span class="comment"># 训练模型</span></span><br><span class="line">clf.score(X_test,y_test) <span class="comment"># 模型评分</span></span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">score= <span class="number">0.9666666666666667</span></span><br></pre></td></tr></table></figure><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-人工神经网络简介&quot;&gt;&lt;a href=&quot;#1-人工神经网络简介&quot; class=&quot;headerlink&quot; title=&quot;1. 人工神经网络简介&quot;&gt;&lt;/a&gt;1. 人工神经网络简介&lt;/h1&gt;&lt;h2 id=&quot;1-1-生物神经网络&quot;&gt;&lt;a href=&quot;#1-1-生物神经网络&quot; class=&quot;headerlink&quot; title=&quot;1.1 生物神经网络&quot;&gt;&lt;/a&gt;1.1 生物神经网络&lt;/h2&gt;&lt;p&gt;人工神经网络的灵感来自其生物学对应物。生物神经网络使大脑能够以复杂的方式处理大量信息。大脑的生物神经网络由大约1000亿个神经元组成，这是大脑的基本处理单元。神经元通过彼此之间巨大的连接（称为突触）来执行其功能。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>随机森林</title>
    <link href="http://yoursite.com/2023/06/22/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://yoursite.com/2023/06/22/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</id>
    <published>2023-06-22T02:34:45.000Z</published>
    <updated>2023-06-22T02:49:43.748Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-随机森林原理"><a href="#1-随机森林原理" class="headerlink" title="1. 随机森林原理"></a>1. 随机森林原理</h1><h2 id="1-1-集成学习"><a href="#1-1-集成学习" class="headerlink" title="1.1 集成学习"></a>1.1 集成学习</h2><p>集成学习通过训练学习出多个估计器，当需要预测时通过结合器将多个估计器的结果整合起来当作最后的结果输出。<a id="more"></a></p><p>下图展示了集成学习的基本流程：</p><p><img src="/2023/06/22/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/1.png" alt="img"> </p><p>  集成学习分两种：<br>（1）Bagging装袋：又称自主聚集(bootstrap aggregating)，是一种根据均匀概率分布从数据集中重复抽样（有放回）的技术。每个新数据集和原始数据集大小相等。代表算法：随机森林。<br>（2）Boosting提升：是一个迭代的过程，用来自适应地改变训练样本的分布，使得弱学习器聚焦到那些很难分类的样本上。它的做法是给每一个训练样本赋予一个权重，在每一轮训练结束时自动地调整权重。代表算法：Adaboost、GBDT、XGBoost。</p><p>集成学习的优势是提升了单个估计器的通用性与鲁棒性，比单个估计器拥有更好的预测性能。集成学习的另一个特点是能方便的进行并行化操作。</p><h2 id="1-2-Bagging算法"><a href="#1-2-Bagging算法" class="headerlink" title="1.2 Bagging算法"></a>1.2 Bagging算法</h2><p>Bagging 算法是一种集成学习算法，其全称为自助聚集算法（Bootstrap aggregating），顾名思义算法由 Bootstrap 与 Aggregating 两部分组成。</p><p>下图展示了Bagging算法使用自助取样（Bootstrapping）生成多个子数据的示例：</p><p><img src="/2023/06/22/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/2.png" alt="img"> </p><p>算法的具体步骤为：假设有一个大小为 N 的训练数据集，每次从该数据集中有放回的取选出大小为 M 的子数据集，一共选 K 次，根据这 K 个子数据集，训练学习出 K 个模型。当要预测的时候，使用这 K 个模型进行预测，再通过取平均值或者多数分类的方式，得到最后的预测结果。</p><h2 id="1-3-随机森林算法"><a href="#1-3-随机森林算法" class="headerlink" title="1.3 随机森林算法"></a>1.3 随机森林算法</h2><p>将多个决策树结合在一起，每次数据集是随机有放回的选出，同时随机选出部分特征作为输入，所以该算法被称为随机森林算法。可以看到随机森林算法是以决策树为估计器的Bagging算法。</p><p><img src="/2023/06/22/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/3.png" alt="img"> </p><p>上图展示了随机森林算法的具体流程，其中结合器在分类问题中，选择多数分类结果作为最后的结果，在回归问题中，对多个回归结果取平均值作为最后的结果。</p><p>使用Bagging算法能降低过拟合的情况，从而带来了更好的性能。单个决策树对训练集的噪声非常敏感，但通过Bagging算法降低了训练出的多颗决策树之间关联性，有效缓解了上述问题。</p><h1 id="2-随机森林算法步骤"><a href="#2-随机森林算法步骤" class="headerlink" title="2. 随机森林算法步骤"></a>2. 随机森林算法步骤</h1><p>假设训练集 T 的大小为 N ，特征数目为 M ，随机森林的树的数量为 K ，随机森林算法的具体步骤如下：<br>重复下面步骤 K 次，即生成K棵决策树，形成随机森林：<br>第一步：从训练集 T 中有放回抽样的方式，取样N 次形成一个新子训练集 D；<br>第二步：随机选择 m 个特征，其中 m &lt; M；<br>第三步：使用新的训练集 D 和 m 个特征，学习出一个完整的决策树（一般是CART）。</p><h1 id="3-随机森林特点"><a href="#3-随机森林特点" class="headerlink" title="3. 随机森林特点"></a>3. 随机森林特点</h1><p><strong><em>优点：</em></strong><br>（1）实现简单，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的；<br>（2）相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；<br>（3）能直接特征很多的高维数据，因为在训练过程中依旧会从这些特征中随机选取部分特征用来训练；<br>（4）相比SVM，不是很怕特征缺失，因为待选特征也是随机选取；<br>（5）训练完成后可以给出特征重要性。当然，这个优点主要来源于决策树。因为决策树在训练过程中会计算熵或者是基尼系数，越往树的根部，特征越重要。</p><p><strong><em>缺点：</em></strong><br>（1）在噪声过大的分类和处理回归问题时还是容易过拟合；<br>（2）相比于单一决策树，它的随机性让我们难以对模型进行解释。</p><h1 id="4-随机森林的Python应用"><a href="#4-随机森林的Python应用" class="headerlink" title="4. 随机森林的Python应用"></a>4. 随机森林的Python应用</h1><p>Sklearn的ensemble库中提供了随机森林分类和回归的方法，分别是ensemble.RandomForestClassifier([…])随机森林分类和ensemble.RandomForestRegressor([…])随机森林回归。</p><h2 id="4-1-RandomForestClassifier随机森林分类"><a href="#4-1-RandomForestClassifier随机森林分类" class="headerlink" title="4.1 RandomForestClassifier随机森林分类"></a>4.1 RandomForestClassifier随机森林分类</h2><p>RandomForestClassifier函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RandomForestClassifier(n_estimators=<span class="number">100</span>, *, criterion=<span class="string">'gini'</span>, max_depth=<span class="literal">None</span>,min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="string">'auto'</span>, max_leaf_nodes=<span class="literal">None</span>, min_impurity_decrease=<span class="number">0.0</span>,min_impurity_split=<span class="literal">None</span>, bootstrap=<span class="literal">True</span>, oob_score=<span class="literal">False</span>, n_jobs=<span class="literal">None</span>, random_state=<span class="literal">None</span>, verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>, class_weight=<span class="literal">None</span>,ccp_alpha=<span class="number">0.0</span>, max_samples=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：</p><p>n_estimators：森林中决策树的数量。默认100。</p><p>criterion：分裂节点所用的标准，可选“gini”, “entropy”，默认“gini”。</p><p>max_depth：树的最大深度。如果为None，则将节点展开，直到所有叶子都是纯净的(只有一个类)，或者直到所有叶子都包含少于min_samples_split个样本。默认是None。</p><p>min_samples_split：拆分内部节点所需的最少样本数：如果为int，则将min_samples_split视为最小值。如果为float，则min_samples_split是一个分数，而ceil（min_samples_split * n_samples）是每个拆分的最小样本数。默认是2。</p><p>min_samples_leaf：在叶节点处需要的最小样本数。仅在任何深度的分割点在左分支和右分支中的每个分支上至少留下min_samples_leaf个训练样本时，才考虑。这可能具有平滑模型的效果，尤其是在回归中。如果为int，则将min_samples_leaf视为最小值。如果为float，则min_samples_leaf是分数，而ceil（min_samples_leaf * n_samples）是每个节点的最小样本数。默认是1。</p><p>min_weight_fraction_leaf：在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。</p><p>max_features：寻找最佳分割时要考虑的特征数量：如果为int，则在每个拆分中考虑max_features个特征。如果为float，则max_features是一个分数，并在每次拆分时考虑int（max_features * n_features）个特征。如果为“auto”，则max_features = sqrt（n_features）。如果为“ sqrt”，则max_features = sqrt（n_features）。如果为“ log2”，则max_features = log2（n_features）。如果为None，则max_features = n_features。注意：在找到至少一个有效的节点样本分区之前，分割的搜索不会停止，即使它需要有效检查多个max_features功能也是如此。</p><p>max_leaf_nodes：最大叶子节点数，整数，默认为None</p><p>min_impurity_decrease：如果分裂指标的减少量大于该值，则进行分裂。</p><p>min_impurity_split：决策树生长的最小纯净度。默认是0。自版本0.19起不推荐使用：不推荐使用min_impurity_split，而建议使用0.19中的min_impurity_decrease。min_impurity_split的默认值在0.23中已从1e-7更改为0，并将在0.25中删除。</p><p>bootstrap：是否进行bootstrap操作，bool。默认True。如果bootstrap==True，将每次有放回地随机选取样本，只有在extra-trees中，bootstrap=False</p><p>oob_score：是否使用袋外样本来估计泛化精度。默认False。</p><p>n_jobs：并行计算数。默认是None。</p><p>random_state：控制bootstrap的随机性以及选择样本的随机性。</p><p>verbose：在拟合和预测时控制详细程度。默认是0。</p><p>warm_start：不常用</p><p>class_weight：每个类的权重，可以用字典的形式传入{class_label: weight}。如果选择了“balanced”，则输入的权重为n_samples / (n_classes * np.bincount(y))。</p><p>ccp_alpha：将选择成本复杂度最大且小于ccp_alpha的子树。默认情况下，不执行修剪。</p><p>max_samples：如果bootstrap为True，则从X抽取以训练每个基本分类器的样本数。如果为None（默认），则抽取X.shape [0]样本。如果为int，则抽取max_samples样本。如果为float，则抽取max_samples * X.shape [0]个样本。因此，max_samples应该在（0，1）中。是0.22版中的新功能。</p></blockquote><p>以datasets里的红酒数据为例，测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment">#切分训练集和测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="comment">#导入随机森林模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree, datasets</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入红酒数据</span></span><br><span class="line">wine = datasets.load_wine()</span><br><span class="line"></span><br><span class="line"><span class="comment">#只选取前两个特征</span></span><br><span class="line">X = wine.data[:, :<span class="number">2</span>]</span><br><span class="line">y = wine.target</span><br><span class="line"></span><br><span class="line"><span class="comment">#拆分训练集和数据集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设定随机森林中有6颗树</span></span><br><span class="line">forest = RandomForestClassifier(n_estimators=<span class="number">6</span>, random_state=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#拟合数据</span></span><br><span class="line">forest.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制图形</span></span><br><span class="line"><span class="comment">#定义图像中分区的颜色和散点的颜色</span></span><br><span class="line">cmap_light= ListedColormap([<span class="string">'#FFAAAA'</span>, <span class="string">'#AAFFAA'</span>, <span class="string">'#AAAAFF'</span>])</span><br><span class="line">cmap_bold = ListedColormap([<span class="string">'#FF0000'</span>, <span class="string">'#00FF00'</span>, <span class="string">'#0000FF'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#分别用样本的两个特征值创建图像和横轴和纵轴</span></span><br><span class="line">x_min, x_max = X[:,<span class="number">0</span>].min()<span class="number">-1</span>, X[:,<span class="number">0</span>].max()+<span class="number">1</span></span><br><span class="line">y_min, y_max = X[:,<span class="number">1</span>].min()<span class="number">-1</span>, X[:,<span class="number">1</span>].max()+<span class="number">1</span></span><br><span class="line"><span class="comment">#用不同的背景色表示不同的类</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="number">.02</span>),</span><br><span class="line">                     np.arange(y_min, y_max, <span class="number">.02</span>))</span><br><span class="line">z = forest.predict(np.c_[(xx.ravel(), yy.ravel())]).reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.pcolormesh(xx, yy, z, cmap=cmap_light)</span><br><span class="line"></span><br><span class="line"><span class="comment">#用散点把样本标出来</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=cmap_bold, edgecolors=<span class="string">'k'</span>, s=<span class="number">20</span>)</span><br><span class="line">plt.xlim(xx.min(), xx.max())</span><br><span class="line">plt.ylim(yy.min(), yy.max())</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Classifier: RandomForestClassifier'</span>) <span class="comment">#依照参数值修改标题</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/22/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/4.png" alt="img"> </p><h2 id="4-2-RandomForestRegressor随机森林回归"><a href="#4-2-RandomForestRegressor随机森林回归" class="headerlink" title="4.2 RandomForestRegressor随机森林回归"></a>4.2 RandomForestRegressor随机森林回归</h2><p>ensemble.RandomForestRegressor函数原型如下：</p><p><img src="/2023/06/22/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/5.png" alt="img"> </p><p><strong><em>其中关于决策树的参数：</em></strong></p><blockquote><p>criterion: “mse”来选择最合适的节点。</p><p>splitter: ”best” or “random”(default=”best”)随机选择属性还是选择不纯度最大的属性，建议用默认。</p><p>max_features: 选择最适属性时划分的特征不能超过此值。</p><p>当为整数时，即最大特征数；当为小数时，训练集特征数*小数；</p><p>if “auto”, then max_features=sqrt(n_features).</p><p>If “sqrt”, thenmax_features=sqrt(n_features).</p><p>If “log2”, thenmax_features=log2(n_features).</p><p>If None, then max_features=n_features.</p><p>max_depth: (default=None)设置树的最大深度，默认为None，这样建树时，会使每一个叶节点只有一个类别，或是达到min_samples_split。</p><p>min_samples_split: 根据属性划分节点时，每个划分最少的样本数。</p><p>min_samples_leaf: 叶子节点最少的样本数。</p><p>max_leaf_nodes: (default=None)叶子树的最大样本数。</p><p>min_weight_fraction_leaf: (default=0) 叶子节点所需要的最小权值</p><p>verbose: (default=0) 是否显示任务进程</p></blockquote><p><strong><em>关于随机森林特有的参数：</em></strong></p><blockquote><p>n_estimators=10：决策树的个数，越多越好，但是性能就会越差，至少100左右（具体数字忘记从哪里来的了）可以达到可接受的性能和误差率。</p><p>bootstrap=True：是否有放回的采样。</p><p>oob_score=False：oob（out of band，带外）数据，即：在某次决策树训练中没有被bootstrap选中的数据。多单个模型的参数训练，我们知道可以用  cross validation（cv）来进行，但是特别消耗时间，而且对于随机森林这种情况也没有大的必要，所以就用这个数据对决策树模型进行验证，算是一个简单的交叉验证。性能消耗小，但是效果不错。</p><p>n_jobs=1：并行job个数。这个在ensemble算法中非常重要，尤其是bagging（而非boosting，因为boosting的每次迭代之间有影响，所以很难进行并行化），因为可以并行从而提高性能。1=不并行；n：n个并行；-1：CPU有多少core，就启动多少job。</p><p>warm_start=False：热启动，决定是否使用上次调用该类的结果然后增加新的。</p><p>class_weight=None：各个label的权重。</p></blockquote><p><strong><em>进行预测可以有几种形式：</em></strong></p><blockquote><p>predict_proba(x)：给出带有概率值的结果。每个点在所有label的概率和为1.</p><p>predict(x)：直接给出预测结果。内部还是调用的predict_proba()，根据概率的结果看哪个类型的预测值最高就是哪个类型。</p><p>predict_log_proba(x)：和predict_proba基本上一样，只是把结果给做了log()处理。</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机森林</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris=load_iris()</span><br><span class="line"><span class="comment">#print iris#iris的４个属性是：萼片宽度　萼片长度　花瓣宽度　花瓣长度　标签是花的种类：setosa versicolour virginica</span></span><br><span class="line">print(iris[<span class="string">'target'</span>].shape)</span><br><span class="line">rf=RandomForestRegressor()<span class="comment">#这里使用了默认的参数设置</span></span><br><span class="line">rf.fit(iris.data[:<span class="number">150</span>],iris.target[:<span class="number">150</span>])<span class="comment">#进行模型的训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#随机挑选两个预测不相同的样本</span></span><br><span class="line">instance=iris.data[[<span class="number">100</span>,<span class="number">109</span>]]</span><br><span class="line">print(instance)</span><br><span class="line">rf.predict(instance[[<span class="number">0</span>]])</span><br><span class="line">print(<span class="string">'instance 0 prediction；'</span>,rf.predict(instance[[<span class="number">0</span>]]))</span><br><span class="line">print( <span class="string">'instance 1 prediction；'</span>,rf.predict(instance[[<span class="number">1</span>]]))</span><br><span class="line">print(iris.target[<span class="number">100</span>],iris.target[<span class="number">109</span>])</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="/2023/06/22/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/6.png" alt="img"> </p><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-随机森林原理&quot;&gt;&lt;a href=&quot;#1-随机森林原理&quot; class=&quot;headerlink&quot; title=&quot;1. 随机森林原理&quot;&gt;&lt;/a&gt;1. 随机森林原理&lt;/h1&gt;&lt;h2 id=&quot;1-1-集成学习&quot;&gt;&lt;a href=&quot;#1-1-集成学习&quot; class=&quot;headerlink&quot; title=&quot;1.1 集成学习&quot;&gt;&lt;/a&gt;1.1 集成学习&lt;/h2&gt;&lt;p&gt;集成学习通过训练学习出多个估计器，当需要预测时通过结合器将多个估计器的结果整合起来当作最后的结果输出。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机</title>
    <link href="http://yoursite.com/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://yoursite.com/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</id>
    <published>2023-06-21T11:55:55.000Z</published>
    <updated>2023-06-21T12:42:51.592Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-支持向量机简介"><a href="#1-支持向量机简介" class="headerlink" title="1. 支持向量机简介"></a>1. 支持向量机简介</h1><p>支持向量机（Support Vector Machine, SVM）是一类按监督学习（supervised learning）方式对数据进行二元分类的广义线性分类器（generalized linear classifier），其决策边界是对学习样本求解的最大边距超平面。对于线性可分两类数据，支持向量机就是条直线(对于高维数据点就是一个超平面), 两类数据点中的的分割线有无数条，SVM就是这无数条中最完美的一条，怎么样才算最完美呢？就是这条线距离两类数据点越远，则当有新的数据点的时候我们使用这条线将其分类的结果也就越可信。<a id="more"></a></p><p>我们需要先找到数据点中距离分割超平面距离最近的点（即找最小），然后尽量使得距离超平面最近的点的距离的绝对值尽量的大（即求最大）。</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/1.png" alt="img" style="zoom:80%;"> </p><p>这里的数据点到超平面的距离就是间隔（margin），当间隔越大，我们这条线（分类器）也就越健壮。那些距离分割平面最近的点就是支持向量（Support Vectors）。</p><h1 id="2-支持向量机原理"><a href="#2-支持向量机原理" class="headerlink" title="2. 支持向量机原理"></a>2. 支持向量机原理</h1><p>支持向量机是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，核技巧使它成为实质上的非线性分类器。支持向量机一般可分为3类：<strong>线性可分支持向量机、线性支持向量机和非线性支持向量机</strong>。</p><h2 id="2-1-线性可分支持向量机"><a href="#2-1-线性可分支持向量机" class="headerlink" title="2.1 线性可分支持向量机"></a>2.1 线性可分支持向量机</h2><p>线性可分支持向量机得到一个分离超平面wx+b=0可以将实例分成不同的类，它由法向量w和截距b决定。</p><p>对于线性可分的数据，可能有无数个平面能降正负类分开，线性可分支持向量机利用硬间隔最大化求最优分离超平面，这时候解是唯一的。</p><p>假设超平面为wx+b=0，对于一个点，wx+b的符号与类标记y的符号是否一致能够表示分类是否正确，点距离超平面的远近∣wx+b∣可以表示分类预测的确信程度，所以，可用y(wx+b)表示分类的正确性与确信度，这也就是函数间隔。</p><h3 id="2-1-1-函数间隔"><a href="#2-1-1-函数间隔" class="headerlink" title="2.1.1 函数间隔"></a>2.1.1 函数间隔</h3><p>对于给定的训练数据集T和超平面(w,b)，定义超平面(w,b)关于样本点的函数间隔为：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/2.png" alt="img"> </p><p>定义超平面(w,b)关于数据集T的函数间隔为：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/3.png" alt="img"> </p><p>然而，仅仅有函数间隔还不够，只要成比例的改变w和b，虽然超平面没有改变，函数间隔却变为原来的两倍，因此要对超平面法向量加一些约束，使间隔是确定的。</p><h3 id="2-1-2-几何间隔"><a href="#2-1-2-几何间隔" class="headerlink" title="2.1.2 几何间隔"></a>2.1.2 几何间隔</h3><p>对于给定的训练数据集T和超平面(w,b)，定义超平面(w,b)关于样本点的几何间隔为：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/4.png" alt="img"> </p><p>定义超平面(w,b)关于数据集T的几何间隔为：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/5.png" alt="img"> </p><p><strong>注意：</strong></p><p>（1）几何间隔与函数间隔的关系：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/6.png" alt="img"> </p><p>（2）二者都是有符号的，以几何间隔为例，类标记一般取+1和-1，若样本在法向量正的一侧，距离为<img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/7.png" alt="img">，若样本在法向量负的一侧，距离为<img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/8.png" alt="img">。</p><p>因此，找最大间隔的超平面，直观上也就是以最大的确信度进行分类。可以将这个问题表示为下面的约束优化问题：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/9.png" alt="img"> </p><p>接下来就是一步步等价变形，首先目标函数中的γ根据几何间隔和函数间隔的关系可以改成<img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/10.png" alt="img">，约束条件的两边同时乘上||w||，从而变成：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/11.png" alt="img"> </p><p>因为<img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/12.png" alt>的取值不影响最优化问题的解，可以取<img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/12.png" alt="img">=1，同时最大化<img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/13.png" alt="img">和最小化<img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/14.png" alt="img">是等价的，从而变成：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/15.png" alt="img"> </p><p>这是一个凸二次规划问题。</p><h3 id="2-1-3-算法：最大间隔法-对上面的整个过程进行一个总结）"><a href="#2-1-3-算法：最大间隔法-对上面的整个过程进行一个总结）" class="headerlink" title="2.1.3 算法：最大间隔法(对上面的整个过程进行一个总结）"></a>2.1.3 算法：最大间隔法(对上面的整个过程进行一个总结）</h3><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/16.png" alt="img"> </p><p>（1）构造并求解约束最优化问题</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/17.png" alt="img"> </p><p>（2）由此得到</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/18.png" alt="img"> </p><h3 id="2-1-4-支持向量"><a href="#2-1-4-支持向量" class="headerlink" title="2.1.4 支持向量"></a>2.1.4 支持向量</h3><p>训练集中与分离超平面距离最近的样本点的实例称为支持向量，即下图中虚线上的点，也就是使约束条件式等号成立的点，即yi(wxi+b)-1=0。</p><p>H1和H2叫间隔边界，二者之间的距离成为间隔，间隔依赖于法向量w，等于<img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/19.png" alt="img">。</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/20.png" alt="img" style="zoom:67%;"> </p><h2 id="2-2-线性支持向量机"><a href="#2-2-线性支持向量机" class="headerlink" title="2.2 线性支持向量机"></a>2.2 线性支持向量机</h2><p>上面【线性可分支持向量机】所设定的【数据集线性可分】的条件太过理想，实际上，大部分的数据达不到的。那么，怎么将其扩展到非线性可分的问题呢？——把原来的硬性条件放宽，改硬为软。</p><p>线性不可分意味着某些样本点(xi,yi)不能满足函数间隔大于等于1的约束条件，所以对每个样本点引入一个松弛变量ξi≥0，使函数间隔加上松弛变量大于等于1。约束条件变为:</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/21.png" alt="img"> </p><p>同时，对每个松弛变量ξi要支付一个代价，目标函数变为：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/22.png" alt="img"> </p><p>此处，C是惩罚参数，C值大时对误分类的惩罚增大；C值小时对误分类的惩罚减小。目标函数有两层含义：<br>（1）<img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/23.png" alt="img">尽量小，即间隔尽量大<br>（2）误分类点的个数尽量小，C是调和二者之间的系数</p><p> 综上，线性不可分的线性支持向量机的学习问题：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/24.png" alt="img"> </p><h2 id="2-3-非线性支持向量机与核函数"><a href="#2-3-非线性支持向量机与核函数" class="headerlink" title="2.3 非线性支持向量机与核函数"></a>2.3 非线性支持向量机与核函数</h2><p>核技巧：通过一个非线性变换将输入空间对应于一个特征空间，使得在输入空间中的超曲面模型对应于特征空间中的超平面模型。</p><p>常用核函数有：线性核函数、多项式核函数、高斯核函数等。<br>（1）线性核函数，最基本的核函数，平面上的直线，三维的面，多维的超平面，可以使用对偶svm利用二次规划库直接计算。这是我们第一个需要考虑的，简单有效，但是如果是线性不可分的训练集则无法使用。<br>（2）多项式核函数，优点是阶数Q可以灵活设置；缺点是当Q很大时，K的数值范围波动很大，而且参数个数较多，难以选择合适的值，超平面是多项式的曲面<br>（3）高斯核函数，优点是边界更加复杂多样，能最准确地区分数据样本，数值计算K值波动较小，而且只有一个参数，容易选择；缺点是由于特征转换到无限维度中，w没有求解出来，而且可能会发生过拟合。</p><p>实际上核函数代表的是两个样本x和x’，特征变换后的相似性即内积。</p><h1 id="3-支持向量机特点"><a href="#3-支持向量机特点" class="headerlink" title="3. 支持向量机特点"></a>3. 支持向量机特点</h1><p><strong><em>优点：</em></strong><br>（1）支持向量机算法可以解决小样本情况下的机器学习问题，简化了通常的分类和回归等问题。<br>（2）由于采用核函数方法克服了维数灾难和非线性可分的问题，所以向高维空间映射时没有增加计算的复杂性。换句话说，由于支持向量计算法的最终决策函数只由少数的支持向量所确定，所以计算的复杂性取决于支持向量的数目，而不是样本空间的维数。<br>（3）支持向量机算法利用松弛变量可以允许一些点到分类平面的距离不满足原先要求，从而避免这些点对模型学习的影响。 </p><p><strong><em>缺点：</em></strong><br>（1）支持向量机算法对大规模训练样本难以实施。这是因为支持向量机算法借助二次规划求解支持向量，这其中会涉及m阶矩阵的计算，所以矩阵阶数很大时将耗费大量的机器内存和运算时间。<br>（2）经典的支持向量机算法只给出了二分类的算法，而在数据挖掘的实际应用中，一般要解决多分类问题，但支持向量机对于多分类问题解决效果并不理想。<br>（3）SVM算法效果与核函数的选择关系很大，往往需要尝试多种核函数，即使选择了效果比较好的高斯核函数，也要调参选择恰当的γ参数。另一方面就是现在常用的SVM理论都是使用固定惩罚系数C，但正负样本的两种错误造成的损失是不一样的。</p><h1 id="4-支持向量机的Python应用"><a href="#4-支持向量机的Python应用" class="headerlink" title="4. 支持向量机的Python应用"></a>4. 支持向量机的Python应用</h1><p>sklearn中对于支持向量机提供了很多模型：LinearSVC, LinearSVR, NuSVC, NuSVR, SVC, SVR。</p><p>下面以核函数为linear的SVM支持向量机为例，测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入numpy</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#导入画图工具</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#导入支持向量机svm</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="comment">#导入数据集生成工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"> </span><br><span class="line"><span class="comment">#先创建50个数据点,让他们分为两类</span></span><br><span class="line">X,y = make_blobs(n_samples=<span class="number">50</span>,centers=<span class="number">2</span>,random_state=<span class="number">6</span>)</span><br><span class="line"><span class="comment">#创建一个线性内核的支持向量机模型</span></span><br><span class="line">clf = svm.SVC(kernel = <span class="string">'linear'</span>,C=<span class="number">1000</span>)</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"><span class="comment">#把数据点画出来</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>],X[:, <span class="number">1</span>],c=y,s=<span class="number">30</span>,cmap=plt.cm.Paired)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#建立图像坐标</span></span><br><span class="line">ax = plt.gca()</span><br><span class="line">xlim = ax.get_xlim()</span><br><span class="line">ylim = ax.get_ylim()</span><br><span class="line"><span class="comment">#生成两个等差数列</span></span><br><span class="line">xx = np.linspace(xlim[<span class="number">0</span>],xlim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">yy = np.linspace(ylim[<span class="number">0</span>],ylim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">YY,XX = np.meshgrid(yy,xx)</span><br><span class="line">xy = np.vstack([XX.ravel(),YY.ravel()]).T</span><br><span class="line">Z = clf.decision_function(xy).reshape(XX.shape)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#把分类的决定边界画出来</span></span><br><span class="line">ax.contour(XX,YY,Z,colors=<span class="string">'k'</span>,levels=[<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>],alpha=<span class="number">0.5</span>,linestyles=[<span class="string">'--'</span>,<span class="string">'-'</span>,<span class="string">'--'</span>])</span><br><span class="line">ax.scatter(clf.support_vectors_[:, <span class="number">0</span>],clf.support_vectors_[:, <span class="number">1</span>],s=<span class="number">100</span>,linewidth=<span class="number">1</span>,facecolors=<span class="string">'none'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/21/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/25.png" alt="img"> </p><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-支持向量机简介&quot;&gt;&lt;a href=&quot;#1-支持向量机简介&quot; class=&quot;headerlink&quot; title=&quot;1. 支持向量机简介&quot;&gt;&lt;/a&gt;1. 支持向量机简介&lt;/h1&gt;&lt;p&gt;支持向量机（Support Vector Machine, SVM）是一类按监督学习（supervised learning）方式对数据进行二元分类的广义线性分类器（generalized linear classifier），其决策边界是对学习样本求解的最大边距超平面。对于线性可分两类数据，支持向量机就是条直线(对于高维数据点就是一个超平面), 两类数据点中的的分割线有无数条，SVM就是这无数条中最完美的一条，怎么样才算最完美呢？就是这条线距离两类数据点越远，则当有新的数据点的时候我们使用这条线将其分类的结果也就越可信。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="http://yoursite.com/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://yoursite.com/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2023-06-20T05:57:53.000Z</published>
    <updated>2023-06-20T06:18:37.972Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-决策树简介"><a href="#1-决策树简介" class="headerlink" title="1. 决策树简介"></a>1. 决策树简介</h1><p>决策树，顾名思义，就是帮我们做出决策的树。现实生活中我们往往会遇到各种各样的抉择，把我们的决策过程整理一下，就可以发现，该过程实际上就是一个树的模型。<a id="more"></a></p><p>决策树分为分类树和回归树两种，分类树对离散变量做决策树，回归树对连续变量做决策树，这里我们只讨论分类树。</p><p> 比如选择好瓜的时候：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/1.png" alt="img" style="zoom:80%;"> </p><p>我们可以认为色泽、根蒂、敲声是一个西瓜的三个特征，每次我们做出抉择都是基于这三个特征来把一个节点分成好几个新的节点。</p><p>在上面的例子中，色泽、根蒂、声音特征选取完成后，开始进行决策，在我们的问题中，决策的内容实际上是将结果分成两类，即是（1）否（0）好瓜。这一类智能决策问题称为分类问题，决策树是一种简单的处理分类问题的算法。</p><h1 id="2-决策树原理"><a href="#2-决策树原理" class="headerlink" title="2. 决策树原理"></a>2. 决策树原理</h1><h2 id="2-1-引例"><a href="#2-1-引例" class="headerlink" title="2.1 引例"></a>2.1 引例</h2><p>一颗完整的决策树包含以下三个部分：<br>（1）根节点：就是树最顶端的节点，比如上面图中的“色泽”。<br>（2）叶子节点：树最底部的那些节点，也就是决策结果，好瓜还是坏瓜。<br>（3）内部节点，除了叶子结点，都是内部节点。</p><p>树中每个内部节点表示在一个属性特征上的测试，每个分支代表一个测试输出，每个叶节点表示一种类别。</p><p>给定一个决策树的实例：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/2.png" alt="img" style="zoom:80%;"> </p><p>构造决策树如下：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/3.png" alt="img" style="zoom:80%;"> </p><p><strong><em>第一层</em></strong></p><p>根节点：被分成17份，8是/9否，总体的信息熵为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/4.png" alt="img"> </p><p><strong><em>第二层</em></strong></p><p>清晰：被分成9份，7是/2否，它的信息熵为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/5.png" alt="img"> </p><p>稍糊：被分成5份，1是/4否，它的信息熵为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/6.png" alt="img"> </p><p>模糊：被分成3份，0是/3否，它的信息熵为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/7.png" alt="img"> </p><p>我们规定，假设我们选取纹理为分类依据，把它作为根节点，那么第二层的加权信息熵可以定义为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/8.png" alt="img"> </p><p>我们规定，H’&lt; H<sub>0</sub>，也就是随着决策的进行，其不确定度要减小才行，决策肯定是一个由不确定到确定状态的转变。</p><p>因此，决策树采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为0，此时每个叶子节点中的实例都属于同一类。</p><h2 id="2-2-生成算法"><a href="#2-2-生成算法" class="headerlink" title="2.2 生成算法"></a>2.2 生成算法</h2><p>构建决策树时，首先要选择一个根节点，而究竟选择谁来当根节点的准则，有以下三种。</p><h3 id="2-2-1-ID3（信息增益）"><a href="#2-2-1-ID3（信息增益）" class="headerlink" title="2.2.1 ID3（信息增益）"></a>2.2.1 ID3（信息增益）</h3><p>从信息论的知识中我们知道：信息熵越大，样本的纯度越低。ID3 算法的核心思想就是以信息增益来度量特征选择，选择信息增益最大的特征进行分裂。</p><p>信息增益 = 信息熵 - 条件熵：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/9.png" alt="img"> </p><p>也可以表示为H0 - H1，比如上面实例中我选择纹理作为根节点，将根节点一分为三，则：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/10.png" alt="img"> </p><p>意思是，没有选择纹理特征前，是否是好瓜的信息熵为0.998，在我选择了纹理这一特征之后，信息熵下降为0.764，信息熵下降了0.234，也就是信息增益为0.234。</p><h3 id="2-2-2-C4-5（信息增益率）"><a href="#2-2-2-C4-5（信息增益率）" class="headerlink" title="2.2.2 C4.5（信息增益率）"></a>2.2.2 C4.5（信息增益率）</h3><p>C4.5算法最大的特点是克服了ID3对特征数目的偏重这一缺点，引入信息增益率来作为分类标准。</p><p>信息增益率=信息增益/特征本身的熵：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/11.png" alt="img"> </p><p>信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此C4.5并不是直接用增益率最大的特征进行划分，而是使用一个启发式方法：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。</p><p>例如上述的例子，我们考虑纹理本身的熵，也就是是否是好瓜的熵。</p><p>纹理本身有三种可能，每种概率都已知，则纹理的熵为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/12.png" alt="img"> </p><p>那么选择纹理作为分类依据时，信息增益率为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/13.png" alt="img"> </p><h3 id="2-2-3-CART（基尼指数）"><a href="#2-2-3-CART（基尼指数）" class="headerlink" title="2.2.3 CART（基尼指数）"></a>2.2.3 CART（基尼指数）</h3><p>基尼指数（基尼不纯度）：表示在样本集合中一个随机选中的样本被分错的概率。</p><p>基尼系数越小，不纯度越低，特征越好。这和信息增益（率）正好相反。基尼指数可以用来度量任何不均匀分布，是介于0-1之间的数，0是完全相等，1是完全不相等。</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/14.png" alt="img"> </p><h2 id="2-3-三种算法的对比"><a href="#2-3-三种算法的对比" class="headerlink" title="2.3 三种算法的对比"></a>2.3 三种算法的对比</h2><p><strong><em>适用范围：</em></strong></p><p>ID3算法只能处理离散特征的分类问题，C4.5能够处理离散特征和连续特征的分类问题，CART算法可以处理离散和连续特征的分类与回归问题。</p><p><strong><em>假设空间：</em></strong></p><p>ID3和C4.5算法使用的决策树可以是多分叉的，而CART算法的决策树必须是二叉树。</p><p><strong><em>优化算法：</em></strong></p><p>ID3算法没有剪枝策略，当叶子节点上的样本都属于同一个类别或者所有特征都使用过了的情况下决策树停止生长。</p><p>C4.5算法使用预剪枝策略，当分裂后的增益小于给定阈值或者叶子上的样本数量小于某个阈值或者叶子节点数量达到限定值或者树的深度达到限定值，决策树停止生长。</p><p>CART决策树主要使用后剪枝策略。</p><h2 id="2-4-剪枝处理"><a href="#2-4-剪枝处理" class="headerlink" title="2.4 剪枝处理"></a>2.4 剪枝处理</h2><p>决策树算法很容易过拟合，剪枝算法就是用来防止决策树过拟合，提高泛华性能的方法。剪枝分为<strong>预剪枝</strong>与<strong>后剪枝</strong>。</p><h3 id="2-4-1-预剪枝"><a href="#2-4-1-预剪枝" class="headerlink" title="2.4.1 预剪枝"></a>2.4.1 预剪枝</h3><p>预剪枝是指在决策树的生成过程中，对每个节点在划分前先进行评估，若当前的划分不能带来泛化性能的提升，则停止划分，并将当前节点标记为叶节点。</p><p>预剪枝方法有：<br>（1）当叶节点的实例个数小于某个阈值时停止生长；<br>（2）当决策树达到预定高度时停止生长；<br>（3）当每次拓展对系统性能的增益小于某个阈值时停止生长；</p><p>预剪枝不足就是剪枝后决策树可能会不满足需求就被过早停止决策树的生长。</p><h3 id="2-4-2-后剪枝"><a href="#2-4-2-后剪枝" class="headerlink" title="2.4.2 后剪枝"></a>2.4.2 后剪枝</h3><p>后剪枝是指先从训练集生成一颗完整的决策树，然后自底向上对非叶节点进行考察，若将该节点对应的子树替换为叶节点，能带来泛化性能的提升，则将该子树替换为叶节点。</p><p>后剪枝决策树通常比预剪枝决策树保留了更多的分枝，一般情形下，后剪枝决策树的欠拟合风险很小，泛化能力往往优于预剪枝决策树。但后剪枝决策树是在生产完全决策树之后进行的，并且要自底向上地对所有非叶子节点进行逐一考察，因此其训练时间开销比未剪枝的决策树和预剪枝的决策树都要大很多。</p><h1 id="3-决策树特点"><a href="#3-决策树特点" class="headerlink" title="3. 决策树特点"></a>3. 决策树特点</h1><p><strong><em>优点：</em></strong><br>容易理解，可解释性较好<br>可以用于小数据集<br>时间复杂度较小<br>可以处理多输入问题，可以处理不相关特征数据<br>对缺失值不敏感 </p><p><strong><em>缺点：</em></strong><br>在处理特征关联性比较强的数据时，表现得不太好<br>当样本中各类别不均匀时，信息增益会偏向于那些具有更多数值的特征<br>对连续性的字段比较难预测<br>容易出现过拟合<br>当类别太多时，错误可能会增加得比较快</p><h1 id="4-决策树的Python应用"><a href="#4-决策树的Python应用" class="headerlink" title="4. 决策树的Python应用"></a>4. 决策树的Python应用</h1><p>在sklearn库中提供了DecisionTreeClassifier函数来实现决策树算法，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>,max_deepth=<span class="literal">None</span>,random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：<br>在python中决策数中默认的是gini系数，也可以选择信息增益的熵’entropy’<br>max_depth：树的深度大小<br>random_state：随机数种子</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">  </span><br><span class="line">iris = load_iris()<span class="comment"># 数据集导入</span></span><br><span class="line">features = iris.data<span class="comment"># 属性特征</span></span><br><span class="line">labels = iris.target<span class="comment"># 分类标签</span></span><br><span class="line">train_features, test_features, train_labels, test_labels = \</span><br><span class="line">    train_test_split(features, labels, test_size=<span class="number">0.3</span>, random_state=<span class="number">1</span>)<span class="comment"># 训练集，测试集分类</span></span><br><span class="line">clf = tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>,max_depth=<span class="number">3</span>)</span><br><span class="line">clf = clf.fit(train_features, train_labels)<span class="comment">#X,Y分别是属性特征和分类label</span></span><br><span class="line">test_labels_predict = clf.predict(test_features)<span class="comment"># 预测测试集的标签</span></span><br><span class="line">score = accuracy_score(test_labels, test_labels_predict)<span class="comment"># 将预测后的结果与实际结果进行对比</span></span><br><span class="line">print(<span class="string">"CART分类树的准确率 %.4lf"</span> % score)<span class="comment"># 输出结果</span></span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=<span class="string">'iris_tree.dot'</span>)<span class="comment"># 生成决策树可视化的dot文件</span></span><br></pre></td></tr></table></figure><p> 输出结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CART分类树的准确率 <span class="number">0.9556</span></span><br></pre></td></tr></table></figure><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-决策树简介&quot;&gt;&lt;a href=&quot;#1-决策树简介&quot; class=&quot;headerlink&quot; title=&quot;1. 决策树简介&quot;&gt;&lt;/a&gt;1. 决策树简介&lt;/h1&gt;&lt;p&gt;决策树，顾名思义，就是帮我们做出决策的树。现实生活中我们往往会遇到各种各样的抉择，把我们的决策过程整理一下，就可以发现，该过程实际上就是一个树的模型。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯算法</title>
    <link href="http://yoursite.com/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-19T02:36:12.000Z</published>
    <updated>2023-06-19T04:08:18.300Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-贝叶斯定理"><a href="#1-贝叶斯定理" class="headerlink" title="1. 贝叶斯定理"></a>1. 贝叶斯定理</h1><p>先验概率：即基于统计的概率，是基于以往历史经验和分析得到的结果，不需要依赖当前发生的条件。</p><p>后验概率：则是从条件概率而来，由因推果，是基于当下发生了事件之后计算的概率，依赖于当前发生的条件。<a id="more"></a></p><p>条件概率：记事件A发生的概率为P(A)，事件B发生的概率为P(B)，则在B事件发生的前提下，A事件发生的概率即为条件概率，记为P(A|B)。<!-- more --></p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/1.png" alt="img"> </p><p>贝叶斯公式：贝叶斯公式便是基于条件概率，通过P(B|A)来求P(A|B)，如下：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/2.png" alt="img"> </p><p>将A看成“规律”，B看成“现象”，那么贝叶斯公式可以看成：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/3.png" alt="img"> </p><p>全概率公式：表示若事件A1,A2,…,An构成一个完备事件组且都有正概率，则对任意一个事件B都有公式成立：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/4.png" alt="img"> </p><p>将全概率公式带入贝叶斯公式中，得到：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/5.png" alt="img"> </p><h1 id="2-朴素贝叶斯"><a href="#2-朴素贝叶斯" class="headerlink" title="2. 朴素贝叶斯"></a>2. 朴素贝叶斯</h1><h2 id="2-1-朴素贝叶斯原理"><a href="#2-1-朴素贝叶斯原理" class="headerlink" title="2.1 朴素贝叶斯原理"></a>2.1 朴素贝叶斯原理</h2><p>特征条件假设：假设每个特征之间没有联系，给定训练数据集，其中每个样本x都包括n维特征，即x = (x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>)，类标记集合含有k种类别，即y = (y<sub>1</sub>,y<sub>2</sub>,…,y<sub>k</sub>)。</p><p>对于给定的新样本x，判断其属于哪个标记的类别，根据贝叶斯定理，可以得到x属于y<sub>k</sub>类别的概率P(y<sub>k</sub>|x)：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/6.png" alt="img"> </p><p>最大的类别记为预测类别，即：<img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/7.png" alt="img"></p><p>朴素贝叶斯算法对条件概率分布作出了独立性的假设，通俗地讲就是说假设各个维度的特征x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>互相独立，在这个假设的前提上，条件概率可以转化为：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/8.png" alt="img"> </p><p>代入上面贝叶斯公式中，得到：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/9.png" alt="img"> </p><p>于是，朴素贝叶斯分类器可表示为：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/10.png" alt="img"> </p><p>因为对所有的y<sub>k</sub>，上式中的分母的值都是一样的，所以可以忽略分母部分，朴素贝叶斯分类器最终表示为：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/11.png" alt="img"> </p><h2 id="2-2-朴素贝叶斯适用范围"><a href="#2-2-朴素贝叶斯适用范围" class="headerlink" title="2.2 朴素贝叶斯适用范围"></a>2.2 朴素贝叶斯适用范围</h2><p>朴素贝叶斯只适用于特征之间是条件独立的情况下，否则分类效果不好，这里的朴素指的就是条件独立。</p><p>朴素贝叶斯主要被广泛地使用在文档分类中。</p><h2 id="2-3-朴素贝叶斯常用模型"><a href="#2-3-朴素贝叶斯常用模型" class="headerlink" title="2.3 朴素贝叶斯常用模型"></a>2.3 朴素贝叶斯常用模型</h2><p>朴素贝叶斯常用的模型如下：<br>（1）高斯模型：处理特征是连续型变量的情况。<br>（2）多项式模型：最常见，要求特征是离散数据。<br>（3）伯努利模型：要求特征是离散的，且为布尔类型，即true和false，或者1和0。</p><h1 id="3-朴素贝叶斯算法的特点"><a href="#3-朴素贝叶斯算法的特点" class="headerlink" title="3. 朴素贝叶斯算法的特点"></a>3. 朴素贝叶斯算法的特点</h1><p><strong><em>优点：</em></strong><br>1、朴素贝叶斯模型有稳定的分类效率。<br>2、对小规模的数据表现很好，能处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练。<br>3、对缺失数据不太敏感，算法也比较简单，常用于文本分类。</p><p><strong><em>缺点：</em></strong><br>1、需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。<br>2、对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。</p><h1 id="4-朴素贝叶斯的Python应用"><a href="#4-朴素贝叶斯的Python应用" class="headerlink" title="4. 朴素贝叶斯的Python应用"></a>4. 朴素贝叶斯的Python应用</h1><p>在sklearn库中提供了GaussianNB、MultinomialNB和BernoulliNB 3种朴素贝叶斯模型，下面以GaussianNB为例。</p><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB  <span class="comment"># 高斯分布，假定特征服从正态分布的</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment"># 数据集划分</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分数据集,random_state:随机数种子</span></span><br><span class="line">train_x,test_x,train_y,test_y = train_test_split(iris.data,iris.target,random_state=<span class="number">12</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 建模</span></span><br><span class="line">gnb_clf = GaussianNB()</span><br><span class="line">gnb_clf.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行预测</span></span><br><span class="line"><span class="comment"># predict()：直接给出预测的类别</span></span><br><span class="line"><span class="comment"># predict_proba()：输出的是每个样本属于某种类别的概率</span></span><br><span class="line">predict_class = gnb_clf.predict(test_x)</span><br><span class="line"><span class="comment"># predict_class_proba = gnb_clf.predict_proba(test_x)</span></span><br><span class="line">print(<span class="string">"测试集准确率为："</span>,accuracy_score(test_y,predict_class))</span><br></pre></td></tr></table></figure><p> 输出结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">测试集准确率为： <span class="number">0.9736842105263158</span>。</span><br></pre></td></tr></table></figure><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-贝叶斯定理&quot;&gt;&lt;a href=&quot;#1-贝叶斯定理&quot; class=&quot;headerlink&quot; title=&quot;1. 贝叶斯定理&quot;&gt;&lt;/a&gt;1. 贝叶斯定理&lt;/h1&gt;&lt;p&gt;先验概率：即基于统计的概率，是基于以往历史经验和分析得到的结果，不需要依赖当前发生的条件。&lt;/p&gt;
&lt;p&gt;后验概率：则是从条件概率而来，由因推果，是基于当下发生了事件之后计算的概率，依赖于当前发生的条件。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归</title>
    <link href="http://yoursite.com/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</id>
    <published>2023-06-18T02:37:42.000Z</published>
    <updated>2023-06-18T03:13:44.729Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-逻辑回归简介"><a href="#1-逻辑回归简介" class="headerlink" title="1. 逻辑回归简介"></a>1. 逻辑回归简介</h1><p>逻辑回归（Logistic Regression）虽然被称为回归，但其实际上是分类模型，并常用于二分类。逻辑回归与线性回归本质上是类似的，相较线性回归只是多了一个Logistic函数（或称为Sigmoid函数）。<a id="more"></a></p><h2 id="1-1-分类和回归"><a href="#1-1-分类和回归" class="headerlink" title="1.1 分类和回归"></a>1.1 分类和回归</h2><p>分类和回归是机器学习可以解决的两大主要问题，从预测值的类型上看，连续变量预测的定量输出称为回归；离散变量预测的定性输出称为分类。例如：预测明天多少度，是一个回归任务；预测明天阴还是晴，就是一个分类任务。</p><h2 id="1-2-逻辑回归"><a href="#1-2-逻辑回归" class="headerlink" title="1.2 逻辑回归"></a>1.2 逻辑回归</h2><p>Logistic Regression原理与Linear Regression回归类似，其主要流程如下：<br>（1）构建预测函数。一般来说在构建之前，需要根据数据来确定函数模型，是线性还是非线性。<br>（2）构建Cost函数(损失函数)。该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。<br>（3）采用梯度下降算法，minJ(θ)。在数据量很大时，梯度下降算法执行起来会较慢。因此出现了随机梯度下降等算法进行优化。</p><h1 id="2-逻辑回归原理"><a href="#2-逻辑回归原理" class="headerlink" title="2. 逻辑回归原理"></a>2. 逻辑回归原理</h1><h2 id="2-1-构造预测函数"><a href="#2-1-构造预测函数" class="headerlink" title="2.1 构造预测函数"></a>2.1 构造预测函数</h2><p>在线性回归中模型中预测函数为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/1.png" alt="img"> </p><p>逻辑回归作为分类问题，结果h = {1 or 0}即可。若逻辑回归采用线性回归预测函数则会产生远大于1或远小于0得值，不便于预测。因此线性回归预测函数需要做一定改进。设想如果有一个函数h(x)能够把预测结果值压缩到0-1这个区间，那么我们就可以设定一个阈值s，若h(x) &gt;= s,则认定为预测结果为1，否之为0。</p><p>实际中也存在这样的函数：Logistic函数（或称为Sigmoid函数），函数表达式为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/2.png" alt="img"> </p><p>其图像如下图所示：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/3.png" alt="img"> </p><p>对于线性边界的情况，边界形式如下：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/4.png" alt="img"> </p><p>构造预测函数为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/5.png" alt="img"> </p><p>函数h<sub>θ</sub>(x)的值有特殊的含义，它表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/6.png" alt="img">    公式（1）</p><h2 id="2-2-构造损失函数-J"><a href="#2-2-构造损失函数-J" class="headerlink" title="2.2 构造损失函数 J"></a>2.2 构造损失函数 J</h2><p>Cost函数和J函数如下，它们是基于最大似然估计推导得到的：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/7.png" alt="img"> </p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/8.png" alt="img"> </p><p>下面详细说明推导的过程：</p><p>2.1节公式(1) 综合起来可以写成：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/9.png" alt="img"> </p><p>取似然函数为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/10.png" alt="img"> </p><p>对数似然函数为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/11.png" alt="img"> </p><p>最大似然估计就是求使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将取为下式，即：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/12.png" alt="img"> </p><h2 id="2-3-梯度下降法求J-θ-的最小值"><a href="#2-3-梯度下降法求J-θ-的最小值" class="headerlink" title="2.3 梯度下降法求J(θ)的最小值"></a>2.3 梯度下降法求J(θ)的最小值</h2><p>θ更新过程如下：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/13.png" alt="img"> </p><p>经化简后θ的最终更新过程可以写成：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/14.png" alt="img"> </p><p>另外，可以通过向量化，即使用矩阵计算来代替for循环，以简化计算过程，提高效率；可以通过正则化解决过拟合问题（过拟合即过分拟合了训练数据，使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力））。</p><h1 id="3-逻辑回归特点"><a href="#3-逻辑回归特点" class="headerlink" title="3. 逻辑回归特点"></a>3. 逻辑回归特点</h1><p><strong><em>优点：</em></strong><br>（1）简单实现，模型的可解释性很好。<br>（2）训练速度快。计算的量只和特征的数目有关，不需要缩放输入特征等等。<br>（3）资源占用小。因为只需要存储各个维度的特征值。</p><p><strong><em>缺点：</em></strong><br>（1）准确率不是很高，就是因为模型（非常类似线性模型），很难拟合数据的真实分布，容易导致过拟合。<br>（2）难处理数据不平衡的问题。<br>（3）逻辑回归本身无法解决非线性问题，因为它的决策边界是线性的。<br>（4）高度依赖正确的数据表示。</p><h1 id="4-逻辑回归的应用场景"><a href="#4-逻辑回归的应用场景" class="headerlink" title="4. 逻辑回归的应用场景"></a>4. 逻辑回归的应用场景</h1><p>当Y变量只有两个值时，当面临分类问题时，可以考虑使用逻辑回归。</p><p>逻辑回归也用于多分类别分类。有很多种多分类算法，如随机森林分类器或者朴素贝叶斯分类器等等。逻辑回归也可以用于多分类任务。可以通过一些技巧，分两种策略：</p><p>一对多策略：基本思想是将第i种类型的所有样本作为正例，将剩下的所有样本作为负例。进行训练得出一个分类器。这样，我们就得到N个分类器。预测的时候，将样本给N个分类器，获得N个结果，选择其中概率值最大的那个作为结果。</p><p>一对一策略：这种策略，假设有N个类别，不同的类别之间，训练一个分类器，训练得到的结果有</p><script type="math/tex; mode=display">C_N^2</script><p>种不同的分类器。预测的时候，将样本给所有的分类器，会有N(N-1)个结果，最终结果通过“投票”产生。</p><h1 id="5-逻辑回归的Python应用"><a href="#5-逻辑回归的Python应用" class="headerlink" title="5. 逻辑回归的Python应用"></a>5. 逻辑回归的Python应用</h1><h2 id="5-1-自定义函数实现逻辑回归"><a href="#5-1-自定义函数实现逻辑回归" class="headerlink" title="5.1 自定义函数实现逻辑回归"></a>5.1 自定义函数实现逻辑回归</h2><p>可以自定义sigmod函数、损失函数、梯度下降等实现逻辑回归，如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># sigmod函数，即得分函数,计算数据的概率是0还是1；得到y大于等于0.5是1，y小于等于0.5为0。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmod</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="comment"># hx是概率估计值，是sigmod(x)得来的值，y是样本真值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(hx, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -y * np.log(hx) - (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - hx)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(current_para, x, y, learning_rate)</span>:</span></span><br><span class="line">    m = len(y)</span><br><span class="line">    matrix_gradient = np.zeros(len(x[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        current_x = x[i]</span><br><span class="line">        current_y = y[i]</span><br><span class="line">        current_x = np.asarray(current_x)</span><br><span class="line">        matrix_gradient += (sigmod(np.dot(current_para, current_x)) - current_y) * current_x</span><br><span class="line">    new_para = current_para - learning_rate * matrix_gradient</span><br><span class="line">    <span class="keyword">return</span> new_para</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 误差计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error</span><span class="params">(para, x, y)</span>:</span></span><br><span class="line">    total = len(y)</span><br><span class="line">    error_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total):</span><br><span class="line">        current_x = x[i]</span><br><span class="line">        current_y = y[i]</span><br><span class="line">        hx = sigmod(np.dot(para, current_x))  <span class="comment"># LR算法</span></span><br><span class="line">        <span class="keyword">if</span> cost(hx, current_y) &gt; <span class="number">0.5</span>:  <span class="comment"># 进一步计算损失</span></span><br><span class="line">            error_num += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> error_num / total</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(initial_para, x, y, learning_rate, num_iter)</span>:</span></span><br><span class="line">    dataMat = np.asarray(x)</span><br><span class="line">    labelMat = np.asarray(y)</span><br><span class="line">    para = initial_para</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iter + <span class="number">1</span>):</span><br><span class="line">        para = gradient(para, dataMat, labelMat, learning_rate)  <span class="comment"># 梯度下降法</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            err = error(para, dataMat, labelMat)</span><br><span class="line">            print(<span class="string">"iter:"</span> + str(i) + <span class="string">" ; error:"</span> + str(err))</span><br><span class="line">    <span class="keyword">return</span> para</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 数据集加载</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"logistic_regression_binary.csv"</span>, <span class="string">"r+"</span>) <span class="keyword">as</span> file_object:</span><br><span class="line">        lines = file_object.readlines()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            line_array = line.strip().split()</span><br><span class="line">            dataMat.append([<span class="number">1.0</span>, float(line_array[<span class="number">0</span>]), float(line_array[<span class="number">1</span>])])  <span class="comment"># 数据</span></span><br><span class="line">            labelMat.append(int(line_array[<span class="number">2</span>]))  <span class="comment"># 标签</span></span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 绘制图形</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span><span class="params">(wei, data, label)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> type(wei).__name__ == <span class="string">'ndarray'</span>:</span><br><span class="line">        weights = wei</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        weights = wei.getA()</span><br><span class="line">    fig = plt.figure(<span class="number">0</span>)</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    xxx = np.arange(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line">    yyy = - weights[<span class="number">0</span>] / weights[<span class="number">2</span>] - weights[<span class="number">1</span>] / weights[<span class="number">2</span>] * xxx</span><br><span class="line">    ax.plot(xxx, yyy)</span><br><span class="line">    cord1 = []</span><br><span class="line">    cord0 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(label)):</span><br><span class="line">        <span class="keyword">if</span> label[i] == <span class="number">1</span>:</span><br><span class="line">            cord1.append(data[i][<span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cord0.append(data[i][<span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">    cord1 = np.array(cord1)</span><br><span class="line">    cord0 = np.array(cord0)</span><br><span class="line">    ax.scatter(cord1[:, <span class="number">0</span>], cord1[:, <span class="number">1</span>], c=<span class="string">'g'</span>)</span><br><span class="line">    ax.scatter(cord0[:, <span class="number">0</span>], cord0[:, <span class="number">1</span>], c=<span class="string">'r'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span><span class="params">()</span>:</span></span><br><span class="line">    x, y = load_dataset()</span><br><span class="line">    n = len(x[<span class="number">0</span>])</span><br><span class="line">    initial_para = np.ones(n)</span><br><span class="line">    learning_rate = <span class="number">0.001</span></span><br><span class="line">    num_iter = <span class="number">1000</span></span><br><span class="line">    print(<span class="string">"初始参数："</span>, initial_para)</span><br><span class="line">    para = train(initial_para, x, y, learning_rate, num_iter)</span><br><span class="line">    print(<span class="string">"训练所得参数："</span>, para)</span><br><span class="line">    plotBestFit(para, x, y)</span><br><span class="line"> </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    logistic_regression()</span><br></pre></td></tr></table></figure><h2 id="5-2-sklearn库LogisticRegression函数的应用"><a href="#5-2-sklearn库LogisticRegression函数的应用" class="headerlink" title="5.2 sklearn库LogisticRegression函数的应用"></a>5.2 sklearn库LogisticRegression函数的应用</h2><p>另外，可以通过sklearn库的逻辑回归函数来训练模型并预测，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LogisticRegression(C=<span class="number">1.0</span>,class_weight=<span class="literal">None</span>,dual=<span class="literal">False</span>,fit_intercept=<span class="literal">True</span>,intercept_scaling=<span class="number">1</span>,max_iter=<span class="number">100</span>,multi_class=<span class="string">'ovr'</span>,n_jobs=<span class="number">1</span>,penalty=<span class="string">'l2'</span>,random_state=<span class="literal">None</span>,solver=<span class="string">'liblinear'</span>,tol=<span class="number">0.0001</span>,verbose=<span class="number">0</span>,warm_start=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明:</p><p>C：正则化系数λ的倒数，默认为1.0。</p><p>class_weight：用于标示分类模型中各种类型的权重，不考虑权重，即为None。</p><p>dual：对偶或原始方法，bool类型，默认为False。</p><p>fit_intercept：是否存在截距或偏差，bool类型，默认为True。</p><p>intercept_scaling：仅在正则化项为”liblinear”，且fit_intercept设置为True时有用，float类型，默认为1。</p><p>max_iter：算法收敛最大迭代次数，int类型，默认为10，仅在正则化优化算法为newton-cg, sag和lbfgs才有用。</p><p>multi_class：分类方式选择参数，str类型，可选参数为ovr和multinomial，默认为ovr。</p><p>n_jobs：并行数，int类型，默认为1。</p><p>penalty：用于指定惩罚项中使用的规范，str类型，可选参数为l1和l2，默认为l2。</p><p>random_state：随机数种子，int类型，可选参数，默认为无。</p><p>solver：优化算法选择参数，决定了我们对逻辑回归损失函数的优化方法。有五个可选参数，即newtoncg,lbfgs,liblinear,sag,saga。对于小型数据集来说，‘liblinear’是一个不错的选择，而‘sag’和‘saga’对于大型数据集则更快。</p><p>tol：停止求解的标准，float类型，默认为1e-4。</p><p>verbose：日志冗长度，int类型，默认为0。</p><p>warm_start：热启动参数，bool类型，默认为False。</p></blockquote><p>下面以通过泰坦尼克号数据集预测乘客生还情况为例，说明上述函数的使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 建立模型用的训练数据集和验证数据集</span></span><br><span class="line">train_X, test_X, train_y, test_y = train_test_split(source_X , source_y, train_size=<span class="number">.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment"># 创建模型：逻辑回归（logisic regression）</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit( train_X , train_y )</span><br><span class="line">LogisticRegression(C=<span class="number">1.0</span>, class_weight=<span class="literal">None</span>, dual=<span class="literal">False</span>, fit_intercept=<span class="literal">True</span>,</span><br><span class="line">          intercept_scaling=<span class="number">1</span>, max_iter=<span class="number">100</span>, multi_class=<span class="string">'ovr'</span>, n_jobs=<span class="number">1</span>,</span><br><span class="line">          penalty=<span class="string">'l2'</span>, random_state=<span class="literal">None</span>, solver=<span class="string">'liblinear'</span>, tol=<span class="number">0.0001</span>,</span><br><span class="line">          verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="comment"># 分类问题，score得到的是模型的正确率</span></span><br><span class="line">model.score(test_X , test_y )</span><br><span class="line"><span class="comment"># k折交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="comment"># 将训练集分成5份，4份用来训练模型，1份用来预测，这样就可以用不同的训练集在一个模型中训练</span></span><br><span class="line">print(model_selection.cross_val_score(model, source_X, source_y, cv=<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果预测</span></span><br><span class="line">pred_Y = model.predict(pred_X)</span><br><span class="line"><span class="comment"># 生成的预测值是浮点数（0.0,1,0）,所以要对数据类型进行转换</span></span><br><span class="line">pred_Y=pred_Y.astype(int)</span><br><span class="line"><span class="comment"># 乘客id</span></span><br><span class="line">passenger_id = full.loc[sourceRow:,<span class="string">'PassengerId'</span>]</span><br><span class="line"><span class="comment"># 数据框：乘客id，预测生存情况的值</span></span><br><span class="line">predDf = pd.DataFrame(</span><br><span class="line">    &#123; <span class="string">'PassengerId'</span>: passenger_id ,</span><br><span class="line">     <span class="string">'Survived'</span>: pred_Y &#125; )</span><br><span class="line">predDf.shape</span><br><span class="line">print(predDf.head())</span><br><span class="line"><span class="comment"># 保存结果</span></span><br><span class="line">predDf.to_csv(<span class="string">'titanic_pred.csv'</span>, index = <span class="literal">False</span> )</span><br></pre></td></tr></table></figure><h1 id="6-源码仓库地址"><a href="#6-源码仓库地址" class="headerlink" title="6. 源码仓库地址"></a>6. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-逻辑回归简介&quot;&gt;&lt;a href=&quot;#1-逻辑回归简介&quot; class=&quot;headerlink&quot; title=&quot;1. 逻辑回归简介&quot;&gt;&lt;/a&gt;1. 逻辑回归简介&lt;/h1&gt;&lt;p&gt;逻辑回归（Logistic Regression）虽然被称为回归，但其实际上是分类模型，并常用于二分类。逻辑回归与线性回归本质上是类似的，相较线性回归只是多了一个Logistic函数（或称为Sigmoid函数）。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>K-means算法</title>
    <link href="http://yoursite.com/2023/06/17/K-means%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/17/K-means%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-17T12:20:15.000Z</published>
    <updated>2023-06-17T12:39:29.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-K-means算法简介"><a href="#1-K-means算法简介" class="headerlink" title="1. K-means算法简介"></a>1. K-means算法简介</h1><p>K-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。<a id="more"></a></p><h1 id="2-K-means算法原理"><a href="#2-K-means算法原理" class="headerlink" title="2. K-means算法原理"></a>2. K-means算法原理</h1><p>K-means算法解决的问题是，在事先不知道如何分类的情况下（即无监督），让程序根据距离的远近，把N个对象（局部）最优的划分为k个类。它是无监督算法中比较常见的一种算法，原理比较简单易懂。本质是通过循环，不断迭代类中心点，计算各个对象到新的类中心点的距离并根据距离最近的原则重新归类，当类内距离最小、类间距离最大时，即可停止迭代（使用中，常常会限定迭代次数，防止陷入死循环。当达到预先设定得循环次数或类中心点不再发生变化时，最后一次迭代得到的结果，即为最终聚类结果）。</p><h2 id="2-1-算法具体步骤"><a href="#2-1-算法具体步骤" class="headerlink" title="2.1 算法具体步骤"></a>2.1 算法具体步骤</h2><p>算法具体步骤如下：<br>第一步：指定聚类类数k（此处涉及k的选择方法）；<br>第二步：选定初始化聚类中心。随机或指定k个对象，作为初始化聚类中心（此处随机选的方法可以升级，以达到更好的聚类效果，比如kmeans++聚类算法）；<br>第三步：得到初始化聚类结果。计算每个对象到k个聚类中心的距离，把每个对象分配给离它最近的聚类中心所代表的类别中，全部分配完毕即得到初始化聚类结果，聚类中心连同分配给它的对象作为一类；<br>第四步：重新计算聚类中心。得到初始化聚类结果后，重新计算每类的类中心点（计算均值），得到新的聚类中心；<br>第五步：迭代循环，得到最终聚类结果。重复第三步和第四步，直到满足迭代终止条件。</p><h2 id="2-2-k取值方法"><a href="#2-2-k取值方法" class="headerlink" title="2.2 k取值方法"></a>2.2 k取值方法</h2><p>结合书本中理论和在实际业务中遇到的情况，总结出以下几点k值的选择方法供参考：<br>1、如果实际业务中，数据维度不超过三维，可先通过画散点图的方法大致确定聚类数目。<br>2、工作中，结合业务方需求背景或经验，可敲定聚类数目。如，做用户的RFM模型（k=3）、判断是否为作弊用户（k=2）等。<br>3、实际业务中做探索性分析时，没有经验等做参考，可使用肘方法（elbow method）确定分类数。此处结合第一部分提到的误差平方和最小来一起理解。要使误差平方和变小，一种方法就是增加类数，这样有助于降低每个类的类内误差平方和，从而降低整体的误差平方和。但若类数太多，一是归类后解释困难，另一个降低类内误差平方和的边际效应可能下降（即增加类数k，误差平方和降低的不显著）。此时，k取误差平方和关于k的曲线的拐点。<br>4、交叉验证方法。将N个对象分为m个部分，用m-1个部分建立聚类模型，并用剩下的一部分检验聚类质量。<br>5、轮廓系数法。计算k取不同值时的轮廓系数，选择轮廓系数较接近1的分类数。</p><p>下面具体介绍<strong>手肘法</strong>和<strong>轮廓系数法</strong>。</p><h3 id="2-2-1-手肘法"><a href="#2-2-1-手肘法" class="headerlink" title="2.2.1 手肘法"></a>2.2.1 手肘法</h3><p>核心公式：SSE(sum of the squared errors，误差平方和)</p><p><img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/1.png" alt="img"> </p><p>其中，C<sub>i</sub>是第i个簇；x是C<sub>i</sub>中的样本点；m<sub>i</sub>是C<sub>i</sub>的质心（C<sub>i</sub>中所有样本的均值）；SSE是所有样本的聚类误差，代表了聚类效果的好坏。</p><p>随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。</p><p><img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/2.png" alt="img" style="zoom: 67%;"> </p><p>显然，肘部对于的k值为4，梯度最大，下降最快，故对于这个数据集的聚类而言，最佳聚类数应该选4。</p><h3 id="2-2-2-轮廓系数法"><a href="#2-2-2-轮廓系数法" class="headerlink" title="2.2.2 轮廓系数法"></a>2.2.2 轮廓系数法</h3><p>具体方法如下：<br>1）计算样本i到同簇其他样本的平均距离a<sub>i</sub>。a<sub>i</sub>越小，说明样本i越应该被聚类到该簇。将a<sub>i</sub>称为样本i的簇内不相似度。簇C中所有样本的均值称为簇C的簇不相似度。<br>2）计算样本i到其他某簇C<sub>j</sub>的所有样本的平均距离b<sub>i</sub><sup>j</sup>，称为样本i与簇C<sub>j</sub>的不相似度。定义为样本i的簇间不相似度：<img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/3.png" alt="img">，bi越大，说明样本i越不属于其他簇。<br>3）根据样本i的簇内不相似度ai和簇间不相似度bi，定义样本i的轮廓系数。</p><p><img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/4.png" alt="img"> </p><p>轮廓系数范围在[-1,1]之间。该值越大，越合理。s<sub>i</sub>接近1，则说明样本i聚类合理；接近-1，则说明样本i更应该分类到另外的簇；若s<sub>i</sub>近似为0，则说明样本i在两个簇的边界上。</p><p>所有样本的s<sub>i</sub>的均值称为聚类结果的轮廓系数，是该聚类是否合理、有效的度量。使用轮廓系数(silhouette coefficient)来确定，选择使系数较大所对应的k值。</p><h2 id="2-3-K-means"><a href="#2-3-K-means" class="headerlink" title="2.3 K-means++"></a>2.3 K-means++</h2><p>我们知道初始值的选取对结果的影响很大，对初始值选择的改进是很重要的一部分。在所有的改进算法中，K-means++最有名。K-means++算法步骤如下所示：（1）随机选取一个中心点a<sub>1</sub>；<br>（2）计算数据到之前n个聚类中心最远的距离D(x)，并以一定概率<img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/5.png" alt="img">选择新中心点a<sub>i</sub>；<br>（3）重复第二步。</p><p>简单的来说，K-means++就是选择离已选中心点最远的点。这也比较符合常理，聚类中心当然是互相离得越远越好。但是这个算法的缺点在于，难以并行化。所以k-meansII改变取样策略，并非按照k-means++那样每次遍历只取样一个样本，而是每次遍历取样k个，重复该取样过程log(n)次，则得到klog(n)个样本点组成的集合，然后从这些点中选取k个。当然一般也不需要log(n)次取样，5次即可。</p><h2 id="2-4-算法终止条件"><a href="#2-4-算法终止条件" class="headerlink" title="2.4 算法终止条件"></a>2.4 算法终止条件</h2><p>终止条件一般为以下几类：<br>a、达到预先设定的迭代次数，如20次。<br>b、类中心点不再发生变化或没有对象被分配给新的类。<br>c、误差平方和最小，误差平方和公式：<img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/6.png" alt="img">，其中Xi代表被分到第i类的对象集合，μ<sub>c(i)</sub>代表第i个聚类的均值（即类中心），c可以理解为迭代这个步骤，因为c随着迭代而发生变化，所以也是个变量。</p><p>在实际编程实现算法时，a常配合着c（误差平方和最小化）一起使用，由于使误差平方和最小有时会陷入死循环或迭代多步类中心变化不大，因此常会限制迭代次数。</p><h1 id="3-K-means算法特点"><a href="#3-K-means算法特点" class="headerlink" title="3. K-means算法特点"></a>3. K-means算法特点</h1><p><strong><em>优点：</em></strong><br>1）容易理解，聚类效果不错，虽然是局部最优，但往往局部最优就够了。<br>2）处理大数据集的时候，该算法可以保证较好的伸缩性。<br>3）当簇近似高斯分布的时候，效果非常不错。<br>4）算法复杂度低。<br>5）主要需要调参的参数仅仅是簇数k。</p><p><strong><em>缺点：</em></strong><br>1）K值的选取不好把握。<br>2）结果的好坏依赖于初始类中心的选择。<br>3）算法常陷入局部最优，更换初始聚类中心后，新的聚类结果可能效果更优。<br>4）对孤立点敏感，如数据集存在异常突出点，会影响聚类效果。<br>5）不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。</p><h1 id="4-K-means算法应用场景"><a href="#4-K-means算法应用场景" class="headerlink" title="4. K-means算法应用场景"></a>4. K-means算法应用场景</h1><p>1、隐含类别的数据较为平衡的情况，如隐含类别的数据量差别较大，则聚类的效果就较差。<br>2、数据最好是凸数据，即隐含类别间的差异越大，则聚类效果越好，因为中心点不再变化所需要的迭代次数较少，比较容易收敛。<br>3、一般作为数据预处理，或者用于辅助分类贴标签使用，因为在已经经过分类的数据上再进行聚类，准确度会非常高。</p><h1 id="5-K-means算法的Python应用"><a href="#5-K-means算法的Python应用" class="headerlink" title="5. K-means算法的Python应用"></a>5. K-means算法的Python应用</h1><h2 id="5-1-K-means算法的Python实现"><a href="#5-1-K-means算法的Python实现" class="headerlink" title="5.1 K-means算法的Python实现"></a>5.1 K-means算法的Python实现</h2><p>以一系列二维点作为原始数据，通过K-means算法来预测新的点属于哪一类。测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">K_Means</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># k是分组数；tolerance'中心点误差'；max_iter是迭代次数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k=<span class="number">2</span>, tolerance=<span class="number">0.0001</span>, max_iter=<span class="number">300</span>)</span>:</span></span><br><span class="line">        self.k_ = k</span><br><span class="line">        self.tolerance_ = tolerance</span><br><span class="line">        self.max_iter_ = max_iter</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.centers_ = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.k_):</span><br><span class="line">            self.centers_[i] = data[random.randint(<span class="number">0</span>,len(data))]</span><br><span class="line">        <span class="comment"># print('center', self.centers_)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.max_iter_):</span><br><span class="line">            self.clf_ = &#123;&#125; <span class="comment">#用于装归属到每个类中的点[k,len(data)]</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.k_):</span><br><span class="line">                self.clf_[i] = []</span><br><span class="line">            <span class="comment"># print("质点:",self.centers_)</span></span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> data:</span><br><span class="line">                distances = [] <span class="comment">#装中心点到每个点的距离[k]</span></span><br><span class="line">                <span class="keyword">for</span> center <span class="keyword">in</span> self.centers_:</span><br><span class="line">                    <span class="comment"># 欧拉距离</span></span><br><span class="line">                    distances.append(np.linalg.norm(feature - self.centers_[center]))</span><br><span class="line">                classification = distances.index(min(distances))</span><br><span class="line">                self.clf_[classification].append(feature)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># print("分组情况:",self.clf_)</span></span><br><span class="line">            prev_centers = dict(self.centers_)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> self.clf_:</span><br><span class="line">                self.centers_[c] = np.average(self.clf_[c], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># '中心点'是否在误差范围</span></span><br><span class="line">            optimized = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">for</span> center <span class="keyword">in</span> self.centers_:</span><br><span class="line">                org_centers = prev_centers[center]</span><br><span class="line">                cur_centers = self.centers_[center]</span><br><span class="line">                <span class="keyword">if</span> np.sum((cur_centers - org_centers) / org_centers * <span class="number">100.0</span>) &gt; self.tolerance_:</span><br><span class="line">                    optimized = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> optimized:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, p_data)</span>:</span></span><br><span class="line">        distances = [np.linalg.norm(p_data - self.centers_[center]) <span class="keyword">for</span> center <span class="keyword">in</span> self.centers_]</span><br><span class="line">        index = distances.index(min(distances))</span><br><span class="line">        <span class="keyword">return</span> index</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1.5</span>, <span class="number">1.8</span>], [<span class="number">5</span>, <span class="number">8</span>], [<span class="number">8</span>, <span class="number">8</span>], [<span class="number">1</span>, <span class="number">0.6</span>], [<span class="number">9</span>, <span class="number">11</span>]])</span><br><span class="line">    k_means = K_Means(k=<span class="number">2</span>)</span><br><span class="line">    k_means.fit(x)</span><br><span class="line">    <span class="keyword">for</span> center <span class="keyword">in</span> k_means.centers_:</span><br><span class="line">        pyplot.scatter(k_means.centers_[center][<span class="number">0</span>], k_means.centers_[center][<span class="number">1</span>], marker=<span class="string">'*'</span>, s=<span class="number">150</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> cat <span class="keyword">in</span> k_means.clf_:</span><br><span class="line">        <span class="keyword">for</span> point <span class="keyword">in</span> k_means.clf_[cat]:</span><br><span class="line">            pyplot.scatter(point[<span class="number">0</span>], point[<span class="number">1</span>], c=(<span class="string">'r'</span> <span class="keyword">if</span> cat == <span class="number">0</span> <span class="keyword">else</span> <span class="string">'b'</span>))</span><br><span class="line"></span><br><span class="line">    predict = [[<span class="number">2</span>, <span class="number">1</span>], [<span class="number">6</span>, <span class="number">9</span>]]</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> predict:</span><br><span class="line">        cat = k_means.predict(feature)</span><br><span class="line">        pyplot.scatter(feature[<span class="number">0</span>], feature[<span class="number">1</span>], c=(<span class="string">'r'</span> <span class="keyword">if</span> cat == <span class="number">0</span> <span class="keyword">else</span> <span class="string">'b'</span>), marker=<span class="string">'x'</span>)</span><br><span class="line"></span><br><span class="line">    pyplot.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/7.png" alt="img"> </p><p>上图可见，所有点被分成了两类（圆点）：红色和蓝色，预测的点（×）被正确分到了所属类别。</p><h2 id="5-2-sklearn-cluster-Kmeans函数的应用"><a href="#5-2-sklearn-cluster-Kmeans函数的应用" class="headerlink" title="5.2 sklearn.cluster.Kmeans函数的应用"></a>5.2 sklearn.cluster.Kmeans函数的应用</h2><p>sklearn库使用sklearn.cluster.KMeans函数来实现K-Means算法，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.cluster.KMeans(n_clusters=<span class="number">8</span>,*,init=<span class="string">'k-means++'</span>, n_init=<span class="number">10</span>,max_iter=<span class="number">300</span>,tol=<span class="number">0.0001</span>,precompute_distances=<span class="string">'deprecated'</span>,verbose=<span class="number">0</span>,random_state=<span class="literal">None</span>,copy_x=<span class="literal">True</span>,n_jobs=<span class="string">'deprecated'</span>,algorithm=<span class="string">'auto'</span>)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：</p><p>n_clusters：int, default=8，簇的个数，即你想聚成几类。</p><p>init：{‘k-means++’, ‘random’, ndarray, callable}, default=’k-means++’，初始簇中心的获取方法，’k-means ++’：以一种聪明的方式为k-mean聚类选择初始聚类中心，以加快收敛速度。有关更多详细信息，请参见k_init中的注释部分。’random’：n_clusters从初始质心的数据中随机选择观察（行）。如果传递了ndarray，则其形状应为（n_clusters，n_features），并给出初始中心。如果传递了callable，则应使用参数X，n_clusters和随机状态并返回初始化。</p><p>n_init：int, default=10，获取初始簇中心的更迭次数，k均值算法将在不同质心种子下运行的次数。</p><p>max_iter：int, default=300，最大迭代次数（因为kmeans算法的实现需要迭代），单次运行的k均值算法的最大迭代次数。</p><p>tol：float, default=1e-4，容忍度，即kmeans运行准则收敛的条件，关于Frobenius范数的相对容差，该范数表示两个连续迭代的聚类中心的差异，以声明收敛。</p><p>precompute_distances：{‘auto’, True, False}, default=’auto’，是否需要提前计算距离，这个参数会在空间和时间之间做权衡，如果是True 会把整个距离矩阵都放到内存中，auto 会默认在数据样本大于featurs*samples 的数量大于12e6 的时候False,False 时核心实现的方法是利用Cpython 来实现的。</p><p>verbose：int, default=0，冗长模式（不太懂是啥意思，反正一般不去改默认值）。</p><p>random_state：int, RandomState instance, default=None，确定质心初始化的随机数生成。使用整数使随机性具有确定性。</p><p>copy_x：bool, default=True， 对是否修改数据的一个标记，如果True，即复制了就不会修改数据。bool 在scikit-learn 很多接口中都会有这个参数的，就是是否对输入数据继续copy 操作，以便不修改用户的输入数据。这个要理解Python 的内存机制才会比较清楚。</p><p>n_job：sint, default=None，并行设置。</p><p>algorithm：{‘auto’, ‘full’, ‘elkan’}, default=’auto’，kmeans的实现算法，经典的EM风格算法是’full’的。通过使用三角形不等式，’elkan’变异对于定义良好的聚类的数据更有效。但是，由于分配了额外的形状数组（n_samples，n_clusters），因此需要更多的内存。目前，’auto’（保持向后兼容性）选择’elkan’，但为了更好的启发式，将来可能会更改。</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入可视化工具包</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">X=load_iris().data</span><br><span class="line">clf = KMeans(n_clusters=<span class="number">3</span>,random_state=<span class="number">0</span>)</span><br><span class="line">clf.fit(X)</span><br><span class="line">label = clf.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 颜色和标签列表</span></span><br><span class="line">colors_list = [<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>]</span><br><span class="line">labels_list = [<span class="string">'1'</span>,<span class="string">'2'</span>,<span class="string">'3'</span>]</span><br><span class="line">x=X</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    plt.scatter(x[label==i,<span class="number">0</span>], x[label== i,<span class="number">1</span>], s=<span class="number">100</span>,c=colors_list[i],label=labels_list[i])</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 聚类中心点</span></span><br><span class="line">plt.scatter(clf.cluster_centers_[:,<span class="number">0</span>],clf.cluster_centers_[:,<span class="number">1</span>], s=<span class="number">300</span>,c=<span class="string">'black'</span>,label=<span class="string">'Centroids'</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'Annual Income (k$)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Spending Score (1-100)'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/8.png" alt="img"> </p><h1 id="6-源码仓库地址"><a href="#6-源码仓库地址" class="headerlink" title="6. 源码仓库地址"></a>6. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-K-means算法简介&quot;&gt;&lt;a href=&quot;#1-K-means算法简介&quot; class=&quot;headerlink&quot; title=&quot;1. K-means算法简介&quot;&gt;&lt;/a&gt;1. K-means算法简介&lt;/h1&gt;&lt;p&gt;K-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>KNN算法</title>
    <link href="http://yoursite.com/2023/06/16/KNN%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/16/KNN%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-16T02:02:00.000Z</published>
    <updated>2023-06-16T02:15:23.653Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-什么是KNN算法"><a href="#1-什么是KNN算法" class="headerlink" title="1. 什么是KNN算法"></a>1. 什么是KNN算法</h1><p>KNN（K Nearest Neighbors，又称k近邻法）是一种基本的分类和回归方法，是监督学习方法里的一种常用方法。</p><p>KNN算法通过距离判断两个样本是否相似，使用与未知样本最近的k个样本（近邻）的类别来分类，数量最多的标签类别就是新样本的标签类别。<a id="more"></a></p><h1 id="2-KNN算法原理"><a href="#2-KNN算法原理" class="headerlink" title="2. KNN算法原理"></a>2. KNN算法原理</h1><p>KNN算法三要素：距离度量、k值的选择和分类决策规则。常用的距离度量是欧氏距离及更一般的pL距离。k值小时，k近邻模型更复杂，容易发生过拟合；k值大时，k近邻模型更简单，又容易欠拟合。因此k值得选择会对分类结果产生重大影响。k值的选择反映了对近似误差与估计误差之间的权衡，通常由交叉验证选择最优的k。分类决策规则往往是多数表决，即由输入实例的k个邻近输入实例中的多数类决定输入实例的类。</p><h2 id="2-1-距离度量"><a href="#2-1-距离度量" class="headerlink" title="2.1 距离度量"></a>2.1 距离度量</h2><p><img src="/2023/06/16/KNN%E7%AE%97%E6%B3%95/1.png" alt="img"> </p><blockquote><p>一般采用二维欧氏距离。</p></blockquote><h2 id="2-2-交叉验证选取k值"><a href="#2-2-交叉验证选取k值" class="headerlink" title="2.2 交叉验证选取k值"></a>2.2 交叉验证选取k值</h2><p>在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。交叉验证的基本思想是重复地使用数据，把给定的数据进行切分，将切分的数据组合为训练集与测试集，在此基础上反复进行训练测试以及模型的选择。</p><h2 id="2-3-分类决策规则"><a href="#2-3-分类决策规则" class="headerlink" title="2.3 分类决策规则"></a>2.3 分类决策规则</h2><p>KNN使用的分类决策规则是多数表决，如果损失函数为0-1损失函数，那么要使误分类率最小即使经验风险最小，多数表决规则实际上就等同于经验风险最小化。</p><h1 id="3-KNN算法特点及应用场景"><a href="#3-KNN算法特点及应用场景" class="headerlink" title="3. KNN算法特点及应用场景"></a>3. KNN算法特点及应用场景</h1><p>KNN是一种非参的，惰性的算法模型。什么是非参，什么是惰性呢？</p><p>非参的意思并不是说这个算法不需要参数，而是意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。</p><p>惰性又是什么意思呢？举例说，同样是分类算法，逻辑回归需要先对数据进行大量训练，最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。</p><p><strong><em>KNN算法优点：</em></strong><br>（1） 简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。<br>（2）模型训练时间快。<br>（3）预测效果好。<br>（4）对异常值不敏感。</p><p><strong><em>KNN算法缺点：</em></strong><br>（1）对内存要求较高，因为该算法存储了所有训练数据。<br>（2）预测阶段可能很慢。<br>（3）对不相关的功能和数据规模敏感。</p><p>那么什么时候应该选择使用KNN算法呢？一般来说，当需要使用分类算法，且数据比较大的时候就可以尝试使用KNN算法进行分类了。</p><h1 id="4-KNN算法的Python应用"><a href="#4-KNN算法的Python应用" class="headerlink" title="4. KNN算法的Python应用"></a>4. KNN算法的Python应用</h1><h2 id="4-1-KNN算法的Python实现"><a href="#4-1-KNN算法的Python实现" class="headerlink" title="4.1 KNN算法的Python实现"></a>4.1 KNN算法的Python实现</h2><p>如下代码所示，通过提供训练集，包含A和B两类数据，预测测试数据属于哪一类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainData - 训练集，testData - 测试集，labels - 分类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn</span><span class="params">(trainData, testData, labels, k)</span>:</span></span><br><span class="line">   <span class="comment"># 计算训练样本的行数</span></span><br><span class="line">   rowSize = trainData.shape[<span class="number">0</span>]</span><br><span class="line">   <span class="comment"># 计算训练样本和测试样本的差值</span></span><br><span class="line">   diff = np.tile(testData, (rowSize, <span class="number">1</span>)) - trainData</span><br><span class="line">   <span class="comment"># 计算差值的平方和</span></span><br><span class="line">   sqrDiff = diff ** <span class="number">2</span></span><br><span class="line">   sqrDiffSum = sqrDiff.sum(axis=<span class="number">1</span>)</span><br><span class="line">   <span class="comment"># 计算距离</span></span><br><span class="line">   distances = sqrDiffSum ** <span class="number">0.5</span></span><br><span class="line">   <span class="comment"># 对所得的距离从低到高进行排序</span></span><br><span class="line">   sortDistance = distances.argsort()</span><br><span class="line">    </span><br><span class="line">   count = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">       vote = labels[sortDistance[i]]</span><br><span class="line">       count[vote] = count.get(vote, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">   <span class="comment"># 对类别出现的频数从高到低进行排序</span></span><br><span class="line">   sortCount = sorted(count.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">   <span class="comment"># 返回出现频数最高的类别</span></span><br><span class="line">   <span class="keyword">return</span> sortCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">trainData = np.array([[<span class="number">5</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">4</span>]])</span><br><span class="line">labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">testData = [<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">X = knn(trainData, testData, labels, <span class="number">3</span>)</span><br><span class="line">print(X)</span><br></pre></td></tr></table></figure><p> 很明显，测试数据属于A类，实际预测结果也为A类。</p><h2 id="4-2-sklearn-neighbors-KNeighborsClassifier-k近邻分类器"><a href="#4-2-sklearn-neighbors-KNeighborsClassifier-k近邻分类器" class="headerlink" title="4.2 sklearn.neighbors.KNeighborsClassifier(k近邻分类器)"></a>4.2 sklearn.neighbors.KNeighborsClassifier(k近邻分类器)</h2><p>sklearn.neighbors.KNeighborsClassifier()函数是用于实现k近邻投票算法的分类器。其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>,weights=’uniform’, algorithm=’auto’, leaf_size=<span class="number">30</span>, p=<span class="number">2</span>, metric=’minkowski’, metric_params=<span class="literal">None</span>, n_jobs=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：</p><ul><li>n_neighbors：int，optional(default = 5)</li></ul><p>默认情况下kneighbors查询使用的邻居数。就是k-NN的k的值，选取最近的k个点。</p><ul><li>weights：str或callable，可选(默认=‘uniform’)</li></ul><p>默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，即所有的邻近点的权重都是相等的。distance是不均等的权重，距离近的点比距离远的点的影响大。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。</p><ul><li>algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选</li></ul><p>快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。ball_tree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。</p><ul><li>leaf_size：int，optional(默认值=30)</li></ul><p>默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。</p><ul><li>p：整数，可选(默认=2)</li></ul><p>距离度量公式，默认使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。</p><ul><li>metric：字符串或可调用，默认为’minkowski’</li></ul><p>用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。</p><ul><li>metric_params：dict，optional(默认=None)</li></ul><p>距离公式的其他关键参数，使用默认的None即可。</p><ul><li>n_jobs：int或None，可选(默认=None)</li></ul><p>并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。</p></blockquote><p><strong>注意：如果发现两个邻居，邻居k+1和k具有相同距离但不同标签，则结果将取决于训练数据的排序。</strong></p><p> 测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">X = [[<span class="number">5</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">4</span>]]</span><br><span class="line">y = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line"></span><br><span class="line">neigh = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">neigh.fit(X, y) </span><br><span class="line">print(neigh.predict([[<span class="number">3</span>, <span class="number">2</span>]]))</span><br></pre></td></tr></table></figure><p>预测结果同样为A类。</p><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-什么是KNN算法&quot;&gt;&lt;a href=&quot;#1-什么是KNN算法&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是KNN算法&quot;&gt;&lt;/a&gt;1. 什么是KNN算法&lt;/h1&gt;&lt;p&gt;KNN（K Nearest Neighbors，又称k近邻法）是一种基本的分类和回归方法，是监督学习方法里的一种常用方法。&lt;/p&gt;
&lt;p&gt;KNN算法通过距离判断两个样本是否相似，使用与未知样本最近的k个样本（近邻）的类别来分类，数量最多的标签类别就是新样本的标签类别。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>距离和相似性度量</title>
    <link href="http://yoursite.com/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/"/>
    <id>http://yoursite.com/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/</id>
    <published>2023-06-15T02:49:49.000Z</published>
    <updated>2023-06-15T03:13:09.008Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习中，经常需要比较个体间差异的大小，进而评价个体的相似性和类别。最常见的是数据分析中的相关分析，数据挖掘中的分类和聚类算法，如K最近邻（KNN）和K均值（K-Means）等。其中，衡量个体间差异的方法，主要分为距离度量和相似性度量。</p><p>为了方便下面的解释和举例，先设定我们要比较X个体和Y个体间的差异，它们都包含了N个维的特征，即X=（x1, x2, x3, … xn），Y=（y1, y2, y3, … yn）。</p><a id="more"></a><h1 id="1-距离度量"><a href="#1-距离度量" class="headerlink" title="1. 距离度量"></a>1. 距离度量</h1><p>距离度量（Distance）用于衡量个体在空间上存在的距离，距离越远说明个体间的差异越大。对于任意一个定义在两个矢量x和y上的函数d(x,y)，只要满足如下4个性质就可以称作“距离度量”。<br>（1）非负性：d(x,y)≥0<br>（2）对称性：d(x,y)=d(y,x)<br>（3）自反性：d(x,y)=0，当且仅当x=y<br>（4）三角不等式：d(x,y)≤d(x,z)+d(y,z)</p><h2 id="1-1-欧几里得距离-Euclidean-Distance"><a href="#1-1-欧几里得距离-Euclidean-Distance" class="headerlink" title="1.1 欧几里得距离(Euclidean Distance)"></a>1.1 欧几里得距离(Euclidean Distance)</h2><p>欧氏距离是最常见的距离度量，衡量的是多维空间中各个点之间的绝对距离。公式如下：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/1.png" alt="img"> </p><p>因为计算是基于各维度特征的绝对数值，所以欧氏度量需要保证各维度指标在相同的刻度级别，比如对身高（cm）和体重（kg）两个单位不同的指标使用欧式距离可能使结果失效。</p><h2 id="1-2-明可夫斯基距离-Minkowski-Distance"><a href="#1-2-明可夫斯基距离-Minkowski-Distance" class="headerlink" title="1.2 明可夫斯基距离(Minkowski Distance)"></a>1.2 明可夫斯基距离(Minkowski Distance)</h2><p>明氏距离是欧氏距离的推广，是对多个距离度量公式的概括性的表述。公式如下：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/2.png" alt="img"> </p><p>这里的p值是一个变量，当p=2时就得到了上节的欧氏距离。</p><h2 id="1-3-曼哈顿距离-Manhattan-Distance"><a href="#1-3-曼哈顿距离-Manhattan-Distance" class="headerlink" title="1.3 曼哈顿距离(Manhattan Distance)"></a>1.3 曼哈顿距离(Manhattan Distance)</h2><p>曼哈顿距离来源于城市区块距离，是将多个维度上的距离进行求和后的结果，即当上节的明氏距离中p=1时得到的距离度量公式，如下所示：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/3.png" alt="img"> </p><h2 id="1-4-切比雪夫距离-Chebyshev-Distance"><a href="#1-4-切比雪夫距离-Chebyshev-Distance" class="headerlink" title="1.4 切比雪夫距离(Chebyshev Distance)"></a>1.4 切比雪夫距离(Chebyshev Distance)</h2><p>切比雪夫距离起源于国际象棋中国王的走法，我们知道国际象棋国王每次只能往周围的8格中走一步，那么如果要从棋盘中A格(x1, y1)走到B格(x2, y2)最少需要走几步？扩展到多维空间，其实切比雪夫距离就是当p趋向于无穷大时的明氏距离：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/4.png" alt="img"> </p><blockquote><p> P.S. 其实上面的曼哈顿距离、欧氏距离和切比雪夫距离都是明可夫斯基距离在特殊条件下的应用。</p></blockquote><h2 id="1-5-马哈拉诺比斯距离-Mahalanobis-Distance"><a href="#1-5-马哈拉诺比斯距离-Mahalanobis-Distance" class="headerlink" title="1.5 马哈拉诺比斯距离(Mahalanobis Distance)"></a>1.5 马哈拉诺比斯距离(Mahalanobis Distance)</h2><p>既然欧几里得距离无法忽略指标度量的差异，所以在使用欧氏距离之前需要对底层指标进行数据的标准化，而基于各指标维度进行标准化后再使用欧氏距离就衍生出来另外一个距离度量——马哈拉诺比斯距离（Mahalanobis Distance），简称马氏距离。</p><h1 id="2-相似性度量"><a href="#2-相似性度量" class="headerlink" title="2. 相似性度量"></a>2. 相似性度量</h1><p>相似性度量（Similarity），即计算个体间的相似程度，与距离度量相反，相似性度量的值越小，说明个体间相似度越小，差异越大。</p><h2 id="2-1-向量空间余弦相似度-Cosine-Similarity"><a href="#2-1-向量空间余弦相似度-Cosine-Similarity" class="headerlink" title="2.1 向量空间余弦相似度(Cosine Similarity)"></a>2.1 向量空间余弦相似度(Cosine Similarity)</h2><p>余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。公式如下：<img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/6.png" alt="6"></p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/5.png" alt="img"> </p><h2 id="2-2-皮尔森相关系数-Pearson-Correlation-Coefficient"><a href="#2-2-皮尔森相关系数-Pearson-Correlation-Coefficient" class="headerlink" title="2.2 皮尔森相关系数(Pearson Correlation Coefficient)"></a>2.2 皮尔森相关系数(Pearson Correlation Coefficient)</h2><p>即相关分析中的相关系数r，分别对X和Y基于自身总体标准化后计算空间向量的余弦夹角。公式如下：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/6.png" alt="img"> </p><h2 id="2-3-Jaccard相似系数-Jaccard-Coefficient"><a href="#2-3-Jaccard相似系数-Jaccard-Coefficient" class="headerlink" title="2.3 Jaccard相似系数(Jaccard Coefficient)"></a>2.3 Jaccard相似系数(Jaccard Coefficient)</h2><p>Jaccard系数主要用于计算符号度量或布尔值度量的个体间的相似度，因为个体的特征属性都是由符号度量或者布尔值标识，因此无法衡量差异具体值的大小，只能获得“是否相同”这个结果，所以Jaccard系数只关心个体间共同具有的特征是否一致这个问题。如果比较X与Y的Jaccard相似系数，只比较xn和yn中相同的个数，公式如下：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/7.png" alt="img"> </p><h2 id="2-4-调整余弦相似度-Adjusted-Cosine-Similarity"><a href="#2-4-调整余弦相似度-Adjusted-Cosine-Similarity" class="headerlink" title="2.4 调整余弦相似度(Adjusted Cosine Similarity)"></a>2.4 调整余弦相似度(Adjusted Cosine Similarity)</h2><p>虽然余弦相似度对个体间存在的偏见可以进行一定的修正，但是因为只能分辨个体在维之间的差异，没法衡量每个维数值的差异，会导致这样一个情况：比如用户对内容评分，5分制，X和Y两个用户对两个内容的评分分别为(1,2)和(4,5)，使用余弦相似度得出的结果是0.98，两者极为相似，但从评分上看X似乎不喜欢这2个内容，而Y比较喜欢，余弦相似度对数值的不敏感导致了结果的误差，需要修正这种不合理性，就出现了调整余弦相似度，即所有维度上的数值都减去一个均值，比如X和Y的评分均值都是3，那么调整后为(-2,-1)和(1,2)，再用余弦相似度计算，得到-0.8，相似度为负值并且差异不小，但显然更加符合现实。</p><h1 id="3-欧氏距离与余弦相似度的比较"><a href="#3-欧氏距离与余弦相似度的比较" class="headerlink" title="3. 欧氏距离与余弦相似度的比较"></a>3. 欧氏距离与余弦相似度的比较</h1><p>欧氏距离是最常见的距离度量，而余弦相似度则是最常见的相似度度量，很多的距离度量和相似度度量都是基于这两者的变形和衍生，所以下面重点比较下两者在衡量个体差异时实现方式和应用环境上的区别。</p><p>借助三维坐标系来看下欧氏距离和余弦相似度的区别：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/8.png" alt="img"> </p><p>从图上可以看出距离度量衡量的是空间各点间的绝对距离，跟各个点所在的位置坐标（即个体特征维度的数值）直接相关；而余弦相似度衡量的是空间向量的夹角，更加的是体现在方向上的差异，而不是位置。如果保持A点的位置不变，B点朝原方向远离坐标轴原点，那么这个时候余弦相似度cosθ是保持不变的，因为夹角不变，而A、B两点的距离显然在发生改变，这就是欧氏距离和余弦相似度的不同之处。</p><p> 根据欧氏距离和余弦相似度各自的计算方式和衡量特征，分别适用于不同的数据分析模型：</p><ul><li><p>欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异；</p></li><li><p>余弦相似度更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分用户兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦相似度对绝对数值不敏感）。</p></li></ul><h1 id="4-Python计算欧氏距离与余弦相似度"><a href="#4-Python计算欧氏距离与余弦相似度" class="headerlink" title="4. Python计算欧氏距离与余弦相似度"></a>4. Python计算欧氏距离与余弦相似度</h1><h2 id="4-1-Python计算欧氏距离"><a href="#4-1-Python计算欧氏距离" class="headerlink" title="4.1 Python计算欧氏距离"></a>4.1 Python计算欧氏距离</h2><p>以下代码提供了3种计算欧氏距离的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x=np.random.random(<span class="number">10</span>)</span><br><span class="line">y=np.random.random(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法一：根据公式求解</span></span><br><span class="line">d1=np.sqrt(np.sum(np.square(x-y)))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 方法二：根据scipy库求解</span></span><br><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> pdist</span><br><span class="line">X=np.vstack([x,y])  <span class="comment"># 将x，y两个一维数组合并成一个2D数组：[[x1,x2,x3...],[y1,y2,y3...]]</span></span><br><span class="line">d2=pdist(X) <span class="comment"># d2=np.sqrt(（x1-y1)+(x2-y2)+....)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法三：使用numpy.linalg.norm(x, y)可以计算向量 x 和向量 y 的欧氏距离</span></span><br><span class="line">d3=np.linalg.norm(x-y)</span><br><span class="line"></span><br><span class="line">print(d1, d2, d3)</span><br></pre></td></tr></table></figure><p> 以上3种方法得到的结果一致。</p><h2 id="4-2-Python计算余弦相似度"><a href="#4-2-Python计算余弦相似度" class="headerlink" title="4.2 Python计算余弦相似度"></a>4.2 Python计算余弦相似度</h2><p>以下代码提供了3种计算余弦相似度的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x=np.random.random(<span class="number">10</span>)</span><br><span class="line">y=np.random.random(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法一：scipy中的scipy.spatial.distance.cosine函数可计算余弦距离。</span></span><br><span class="line"><span class="comment"># 因此，我们可以用1减去余弦距离得到余弦相似度。</span></span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> spatial</span><br><span class="line">res1 = <span class="number">1</span> - spatial.distance.cosine(x, y)</span><br><span class="line">print(res1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二：numpy中的numpy.dot函数可以两个向量的点积，numpy.linalg.norm函数可以计算向量的欧氏距离。</span></span><br><span class="line"><span class="comment"># 因此，可以通过公式和这两个函数计算向量的余弦相似度。</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> dot</span><br><span class="line"><span class="keyword">from</span> numpy.linalg <span class="keyword">import</span> norm</span><br><span class="line">res2 = dot(x, y) / (norm(x) * norm(y))</span><br><span class="line">print(res2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法三：sklearn中的sklearn.metrics.pairwise.cosine_similarity函数可直接计算出两个向量的余弦相似度。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">res3 = cosine_similarity(x.reshape(<span class="number">1</span>, <span class="number">-1</span>), y.reshape(<span class="number">1</span>, <span class="number">-1</span>)) <span class="comment"># reshape(1, -1)将矩阵转化成1行</span></span><br><span class="line">print(res3)</span><br></pre></td></tr></table></figure><p>以上3种方法得到的结果一致。</p><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在机器学习中，经常需要比较个体间差异的大小，进而评价个体的相似性和类别。最常见的是数据分析中的相关分析，数据挖掘中的分类和聚类算法，如K最近邻（KNN）和K均值（K-Means）等。其中，衡量个体间差异的方法，主要分为距离度量和相似性度量。&lt;/p&gt;
&lt;p&gt;为了方便下面的解释和举例，先设定我们要比较X个体和Y个体间的差异，它们都包含了N个维的特征，即X=（x1, x2, x3, … xn），Y=（y1, y2, y3, … yn）。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PCA算法</title>
    <link href="http://yoursite.com/2023/06/14/PCA%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/14/PCA%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-14T02:21:49.000Z</published>
    <updated>2023-06-14T03:09:07.247Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-数据降维"><a href="#1-数据降维" class="headerlink" title="1. 数据降维"></a>1. 数据降维</h1><p>在许多领域的研究与应用中，通常需要对含有多个变量的数据进行观测，收集大量数据后进行分析寻找规律。多变量大数据集无疑会为研究和应用提供丰富的信息，但是也在一定程度上增加了数据采集的工作量。更重要的是在很多情形下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性。如果分别对每个指标进行分析，分析往往是孤立的，不能完全利用数据中的信息，因此盲目减少指标会损失很多有用的信息，从而产生错误的结论。<a id="more"></a></p><p>因此需要找到一种合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。主成分分析与因子分析就属于这类降维算法。</p><p>降维就是一种对高维度特征数据预处理方法。降维是将高维度的数据保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的。在实际的生产和应用中，降维在一定的信息损失范围内，可以为我们节省大量的时间和成本。降维也成为应用非常广泛的数据预处理方法。</p><p>降维具有如下一些优点：<br>（1）使得数据集更易使用。<br>（2）降低算法的计算开销。<br>（3）去除噪声。<br>（4）使得结果容易理解。</p><p>降维的算法有很多，比如<strong>奇异值分解(SVD)、主成分分析(PCA)、因子分析(FA)、独立成分分析(ICA)</strong>。</p><h1 id="2-PCA原理"><a href="#2-PCA原理" class="headerlink" title="2. PCA原理"></a>2. PCA原理</h1><p>PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。</p><p> 那么我们如何得到这些包含最大差异性的主成分方向呢？事实上，通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。</p><p>由于得到协方差矩阵的特征值特征向量有两种方法：<strong>特征值分解协方差矩阵、奇异值分解协方差矩阵</strong>，所以PCA算法有两种实现方法：<strong>基于特征值分解协方差矩阵实现PCA算法、基于SVD分解协方差矩阵实现PCA算法</strong>。</p><p>下面具体介绍基于特征值分解协方差矩阵实现PCA算法的原理。</p><h2 id="2-1-基变换"><a href="#2-1-基变换" class="headerlink" title="2.1 基变换"></a>2.1 基变换</h2><p>一般来说，欲获得原始数据新的表示空间，最简单的是对原始数据进行线性变换（基变换）：Y=PX，其中Y是样本在新空间的表达，P是基向量，X是原始样本。我们可知选择不同的基可以对一组数据给出不同的表示，同时当基的数量少于原始样本本身的维数则可达到降维的效果，矩阵表示如下：</p><p><img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/1.png" alt="img"> </p><p>其中，p<sub>i</sub>是一个行向量，表示第i个基；a<sub>j</sub>是一个列向量，表示第j个原始数据记录。特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一个N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。从原本X∈R<sup>N*M</sup>降维到Y∈R<sup>R*M</sup>。因此这种矩阵相乘的表示也可以表示降维变换。</p><p>最后，上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。</p><h2 id="2-2-方差"><a href="#2-2-方差" class="headerlink" title="2.2 方差"></a>2.2 方差</h2><p>那么如何选择一个方向或者基才是最优的？观察下图,</p><p><img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/2.png" alt="img"> </p><p>我们将所有的点分别向两条直线做投影，基于PCA最大可分思想，我们要找的方向是降维后损失最小，可以理解为投影后的数据尽可能的分开，那么这种分散程度可以用数学上的方差来表示，方差越大数据越分散。方差公式如下：</p><script type="math/tex; mode=display">Var(a)=\frac{1}{m}\sum_{i=1}^n(a_i-μ)^2</script><p>对数据进行了中心化后（可以方便后面的操作）：</p><script type="math/tex; mode=display">Var(a)=\frac{1}{m}\sum_{i=1}^m(a_i)^2</script><h2 id="2-3-协方差"><a href="#2-3-协方差" class="headerlink" title="2.3 协方差"></a>2.3 协方差</h2><p>从二维降到一维可以使用方差最大来选出能使基变换后数据分散最大的方向（基），但如果遇到高维的变换，当完成第一个方向（基）选择后，第二个投影方向应该与第一个“几乎重合在一起”，这显然是没有用的，因此要有其它的约束条件。我们希望两个字段尽可能表示更多的信息，使其不存在相关性。</p><p>数学上用协方差表示其相关性：</p><script type="math/tex; mode=display">Cov(a,b)=\frac{1}{m}\sum_{i=1}^ma_ib_i</script><p>当Cov(a,b)=0时，表示两个字段完全独立，这也是我们的优化目标。</p><h2 id="2-4-协方差矩阵"><a href="#2-4-协方差矩阵" class="headerlink" title="2.4 协方差矩阵"></a>2.4 协方差矩阵</h2><p>我们想达到的目标与字段内方差及字段间协方差有密切关系，假如只有 、  两个字段，那么我们将它们按行组成矩阵X,表示如下：</p><script type="math/tex; mode=display">X=\left( \begin{matrix} a_1 & a_2 & ... & a_m \\ b_1 & b_2 & ... & b_m \end{matrix} \right)</script><p>然后我们用X乘以X的转置，并乘上系数1/m:</p><script type="math/tex; mode=display">\frac{1}mXX^T=\left( \begin{matrix} \frac{1}{m}\sum_{i=1}^ma_i^2 & \frac{1}{m}\sum_{i=1}^ma_ib_i \\ \frac{1}{m}\sum_{i=1}^ma_ib_i & \frac{1}{m}\sum_{i=1}^mb_i^2 \end{matrix} \right)</script><p>可见，协方差矩阵是一个对称的矩阵，而且对角线是各个维度的方差，而其它元素是a和b的协方差，即两者被统一到了一个矩阵。</p><h2 id="2-5-协方差矩阵对角化"><a href="#2-5-协方差矩阵对角化" class="headerlink" title="2.5 协方差矩阵对角化"></a>2.5 协方差矩阵对角化</h2><p>我们的目标是使<img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/3.png" alt="img">，根据上述推倒，可以看出我们的优化目标  <img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/4.png" alt="img">等价于协方差矩阵对角化。即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：</p><p>设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：</p><p><img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/5.png" alt="img"> </p><p>可见，我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足PCPT是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。</p><p> 我们希望的是投影后的方差最大化，于是我们的优化目标可以写为：</p><script type="math/tex; mode=display">max_p tr(PCP^T)</script><script type="math/tex; mode=display">s.t. PP^T = I</script><p>利用拉格朗日函数可以得到：J(P)=tr(PCP<sup>T</sup>)+ λ(PP<sup>T</sup>-I),对P求导有CP<sup>T</sup>+λP<sup>T</sup>=0，整理下即为：CP<sup>T</sup>=(-λ)P<sup>T</sup>。</p><p>于是，只需对协方差矩阵C进行特征分解，对求得的特征值进行排序，再对P<sup>T</sup>=(P<sub>1</sub>,P<sub>2</sub>,…,P<sub>R</sub>)取前K列组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p><h1 id="3-PCA算法流程"><a href="#3-PCA算法流程" class="headerlink" title="3. PCA算法流程"></a>3. PCA算法流程</h1><p>从上面可以看出，求样本 x<sub>i </sub>的 n’ 维的主成分其实就是求样本集的协方差矩阵的前 n’ 个特征值对应特征向量矩阵 P，然后对于每个样本x<sub>i </sub>，做如下变换y<sub>i </sub>=Px<sub>i </sub>，即达到降维的PCA目的。</p><p>下面是具体的算法流程：<br>输入：n维样本集X=(x<sub>1 </sub>,x<sub>2</sub>,…,x<sub>m</sub>)，要降维到的维数 n’；<br>输出：降维后的样本集Y。</p><p>1.对所有的样本进行中心化；<br>2.计算样本的协方差矩阵；<br>3.求出协方差矩阵的特征值及对应的特征向量；<br>4.将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P；<br>5.Y=PX即为降维到k维后的数据。</p><blockquote><p>注意：有时候，我们不指定降维后的n’的值，而是换种方式，指定一个降维到的主成分比重阈值t。这个阈值t在(0,1]之间。假如我们的n个特征值为λ1≥λ2≥…≥λn,则n’可以通过下式得到:<img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/6.png" alt="img"> </p></blockquote><h1 id="4-PCA算法的特点"><a href="#4-PCA算法的特点" class="headerlink" title="4. PCA算法的特点"></a>4. PCA算法的特点</h1><p>作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如为解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。</p><p>PCA算法的主要优点有：<br>（1） 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　<br>（2）各主成分之间正交，可消除原始数据成分间的相互影响的因素。<br>（3）计算方法简单，主要运算是特征值分解，易于实现。</p><p>PCA算法的主要缺点有：<br>（1）主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。<br>（2）方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。</p><h1 id="5-PCA算法的Python应用"><a href="#5-PCA算法的Python应用" class="headerlink" title="5. PCA算法的Python应用"></a>5. PCA算法的Python应用</h1><p>首先需要实现几个函数，分别是数据中心化、最小化降维造成的损失，确定k、得到最大的k个特征值和特征向量、得到降维后的数据、重构数据，然后通过PCA函数整合，最后调用main函数执行。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据中心化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Z_centered</span><span class="params">(dataMat)</span>:</span></span><br><span class="line">    rows, cols = dataMat.shape</span><br><span class="line">    meanVal = np.mean(dataMat, axis=<span class="number">0</span>)  <span class="comment"># 按列求均值，即求各个特征的均值</span></span><br><span class="line">    meanVal = np.tile(meanVal, (rows, <span class="number">1</span>))</span><br><span class="line">    newdata = dataMat - meanVal</span><br><span class="line">    <span class="keyword">return</span> newdata, meanVal</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小化降维造成的损失，确定k</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Percentage2n</span><span class="params">(eigVals, percentage)</span>:</span></span><br><span class="line">    sortArray = np.sort(eigVals)  <span class="comment"># 升序</span></span><br><span class="line">    sortArray = sortArray[<span class="number">-1</span>::<span class="number">-1</span>]  <span class="comment"># 逆转，即降序</span></span><br><span class="line">    arraySum = sum(sortArray)</span><br><span class="line">    tmpSum = <span class="number">0</span></span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> sortArray:</span><br><span class="line">        tmpSum += i</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> tmpSum &gt;= arraySum * percentage:</span><br><span class="line">            <span class="keyword">return</span> num</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到最大的k个特征值和特征向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EigDV</span><span class="params">(covMat, p)</span>:</span></span><br><span class="line">    D, V = np.linalg.eig(covMat)  <span class="comment"># 得到特征值和特征向量</span></span><br><span class="line">    k = Percentage2n(D, p)  <span class="comment"># 确定k值</span></span><br><span class="line">    print(<span class="string">"保留"</span> + str(p*<span class="number">100</span>) + <span class="string">"%信息，降维后的特征个数："</span> + str(k) + <span class="string">"\n"</span>)</span><br><span class="line">    eigenvalue = np.argsort(D)</span><br><span class="line">    K_eigenValue = eigenvalue[<span class="number">-1</span>:-(k + <span class="number">1</span>):<span class="number">-1</span>]</span><br><span class="line">    K_eigenVector = V[:, K_eigenValue]</span><br><span class="line">    <span class="keyword">return</span> K_eigenValue, K_eigenVector</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到降维后的数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getlowDataMat</span><span class="params">(DataMat, K_eigenVector)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> DataMat * K_eigenVector</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重构数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Reconstruction</span><span class="params">(lowDataMat, K_eigenVector, meanVal)</span>:</span></span><br><span class="line">    reconDataMat = lowDataMat * K_eigenVector.T + meanVal</span><br><span class="line">    <span class="keyword">return</span> reconDataMat</span><br><span class="line"></span><br><span class="line"><span class="comment"># PCA算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PCA</span><span class="params">(data, p)</span>:</span></span><br><span class="line">    dataMat = np.float32(np.mat(data))</span><br><span class="line">    <span class="comment"># 数据中心化</span></span><br><span class="line">    dataMat, meanVal = Z_centered(dataMat)</span><br><span class="line">    <span class="comment"># 计算协方差矩阵</span></span><br><span class="line">    <span class="comment"># covMat = Cov(dataMat)</span></span><br><span class="line">    covMat = np.cov(dataMat, rowvar=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 得到最大的k个特征值和特征向量</span></span><br><span class="line">    D, V = EigDV(covMat, p)</span><br><span class="line">    <span class="comment"># 得到降维后的数据</span></span><br><span class="line">    lowDataMat = getlowDataMat(dataMat, V)</span><br><span class="line">    <span class="comment"># 重构数据</span></span><br><span class="line">    reconDataMat = Reconstruction(lowDataMat, V, meanVal)</span><br><span class="line">    <span class="keyword">return</span> reconDataMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    imagePath = <span class="string">'lenna.jpg'</span></span><br><span class="line">    image = cv.imread(imagePath)</span><br><span class="line">    image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)</span><br><span class="line">    rows, cols = image.shape</span><br><span class="line">    print(<span class="string">"降维前的特征个数："</span> + str(cols) + <span class="string">"\n"</span>)</span><br><span class="line">    print(image)</span><br><span class="line">    print(<span class="string">'----------------------------------------'</span>)</span><br><span class="line">    reconImage = PCA(image, <span class="number">0.99</span>) <span class="comment"># 通过改变保留信息的程度来看这个图片的特征值 </span></span><br><span class="line">    reconImage = reconImage.astype(np.uint8)</span><br><span class="line">    print(reconImage)</span><br><span class="line">    cv.imshow(<span class="string">'test'</span>, reconImage)</span><br><span class="line">    cv.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>保留不同信息量，降维后特征个数如下：</p><p><img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/7.png" alt="img"> </p><p>特征个数为52和95的降维后图片如下：</p><p><img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/8.png" alt="img"> </p><p>可以发现，降维后保留的特征越多，图片越清晰。</p><blockquote><p> P.S. 在Python的sklearn的库里面集成很多机器学习算法的库，其中也包括主成分分析的方法。</p></blockquote><h1 id="6-源码仓库地址"><a href="#6-源码仓库地址" class="headerlink" title="6. 源码仓库地址"></a>6. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-数据降维&quot;&gt;&lt;a href=&quot;#1-数据降维&quot; class=&quot;headerlink&quot; title=&quot;1. 数据降维&quot;&gt;&lt;/a&gt;1. 数据降维&lt;/h1&gt;&lt;p&gt;在许多领域的研究与应用中，通常需要对含有多个变量的数据进行观测，收集大量数据后进行分析寻找规律。多变量大数据集无疑会为研究和应用提供丰富的信息，但是也在一定程度上增加了数据采集的工作量。更重要的是在很多情形下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性。如果分别对每个指标进行分析，分析往往是孤立的，不能完全利用数据中的信息，因此盲目减少指标会损失很多有用的信息，从而产生错误的结论。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SIFT算法</title>
    <link href="http://yoursite.com/2023/06/13/SIFT%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/13/SIFT%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-13T01:31:11.000Z</published>
    <updated>2023-06-13T03:14:58.318Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-SIFT算法简介"><a href="#1-SIFT算法简介" class="headerlink" title="1. SIFT算法简介"></a>1. SIFT算法简介</h1><p>SIFT (Scale-invariant feature transform)，尺度不变特征转换，是一种图像局部特征提取算法，它通过在不同的尺度空间中寻找极值点（特征点，关键点）的精确定位和主方向，构建关键点描述符来提取特征。SIFT提取的关键点具有尺度不变性、旋转不变性，而且不会因光照、仿射变换和噪音等因素而干扰。SIFT所查找到的关键点是一些十分突出、不会因光照、仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。<a id="more"></a></p><h2 id="1-1-SIFT特征检测步骤"><a href="#1-1-SIFT特征检测步骤" class="headerlink" title="1.1 SIFT特征检测步骤"></a>1.1 SIFT特征检测步骤</h2><p>SIFT特征检测主要分为以下四步：<br>（1）尺度空间的极值检测：搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和旋转不变的兴趣点；<br>（2）特征点定位：在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度；<br>（3）特征方向赋值：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性；<br>（4）特征点描述：在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/1.png" alt="img"> </p><h2 id="1-2-SIFT算法的特点"><a href="#1-2-SIFT算法的特点" class="headerlink" title="1.2 SIFT算法的特点"></a>1.2 SIFT算法的特点</h2><p>（1）图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。<br>（2）独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。<br>（3）多量性，即使是很少几个物体也可以产生大量的SIFT特征。<br>（4）高速性，经优化的SIFT匹配算法甚至可以达到实时性。<br>（5）可扩展性，可以很方便的与其他的特征向量进行联合。</p><h1 id="2-SIFT算法原理"><a href="#2-SIFT算法原理" class="headerlink" title="2. SIFT算法原理"></a>2. SIFT算法原理</h1><h2 id="2-1-尺度空间"><a href="#2-1-尺度空间" class="headerlink" title="2.1 尺度空间"></a>2.1 尺度空间</h2><p>在一定的范围内，无论物体是大还是小，人眼都可以分辨出来，然而计算机要有相同的能力却很难，在未知的场景中，计算机视觉并不能够提供物体的尺度大小，其中的一种方法是把物体不同尺度下的图像都提供给机器，让机器能够对物体在不同尺度下有一个统一的认知，在建立统一认知的过程中，要考虑的就是图像在不同的尺度下都存在的特点。</p><h3 id="2-1-1-多分辨率金字塔"><a href="#2-1-1-多分辨率金字塔" class="headerlink" title="2.1.1 多分辨率金字塔"></a>2.1.1 多分辨率金字塔</h3><p>早期的图像多尺度通常使用图像金字塔的表示形式，图像金字塔是同一图像在不同分辨率下得到的一组结果，生成过程包含：<br>（1）对原始图像进行平滑；<br>（2）对平滑后的图像进行下采样。</p><p>降采样后得到一系列不断尺寸缩小的图像。显然，一个传统的金字塔中，每一层的图像是其上一层图像长、高的各一半。多分辨率的图像金字塔虽然生成简单，但其本质是降采样，图像的局部特征则难以保持，也就是无法保持特征的尺度不变性。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/2.png" alt="img" style="zoom:80%;"> </p><h3 id="2-1-2-高斯金字塔"><a href="#2-1-2-高斯金字塔" class="headerlink" title="2.1.2 高斯金字塔"></a>2.1.2 高斯金字塔</h3><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/3.png" alt="img" style="zoom:80%;"> </p><p><strong><em>高斯金字塔的构建可以分为两步：</em></strong><br>（1）对图像做高斯平滑；<br>（2）对平滑后的图像做下采样。</p><p>为了让尺度体系其连续性，在简单下采样的基础上增加了高斯滤波，一副图像可以产生几组（octave）图像，一组图像包括几层（interval）图像。</p><p><strong><em>高斯金字塔分布情况（o组s层）：</em></strong></p><p>容易看出，高斯金字塔有多组，每组又有多层，一组的多个层之间的尺度是不一样的，也就是使用的高斯参数σ不同，相邻两层之间的尺度相差一个比例因子k，如果每组有S层，则k=21/S，上一组图像的最底层图像是由下一组中尺度为2σ的图像进行因子为2的降采样得到的（高斯金字塔是从底层开始建立的），高斯金字塔构建完成之后，将相邻的金字塔相减就得到了DoG金字塔。</p><p>高斯金字塔的组数：</p><script type="math/tex; mode=display">o=[log_2min(m,n)]-a</script><p>o表示高斯金字塔的层数，m，n分别是图像的行和列。减去的系数a可以在 0 – log<sub>2</sub>min(m,n) 之间的任意值，和具体需要的金字塔的顶层图像的大小有关。</p><p>高斯模糊参数可由下面的关系得到：</p><script type="math/tex; mode=display">σ(o,s)=σ_0*2^\frac{o+s}{S}</script><p>其中o为所在的组，s为所在的层，σ<sub>0</sub>为初始的尺度，S为每组的层数。</p><p><strong><em>同组内相邻层的图像尺度间的关系：</em></strong></p><script type="math/tex; mode=display">σ_{s+1}=k*σ_s=2^\frac{1}{S}*σ_s</script><p><strong><em>相邻组之间的尺度关系：</em></strong></p><script type="math/tex; mode=display">σ_{o+1}=2*σ_o</script><p>上一组图像的底层是由前一组图像的倒数第二层图像隔点采样生成的，这样可以保证尺度的连续性。</p><h3 id="2-1-3-高斯尺度空间（使用不同的参数）"><a href="#2-1-3-高斯尺度空间（使用不同的参数）" class="headerlink" title="2.1.3 高斯尺度空间（使用不同的参数）"></a>2.1.3 高斯尺度空间（使用不同的参数）</h3><p>我们要精确表示的物体都是通过一定的尺度来反映的，现实世界的物体也总是通过不同尺度的观察而得到不同的变化。</p><p>尺度空间的理论最早在1962年提出，主要思想是通过对原始图像进行尺度变换，获得图像多尺度下的尺度空间表示序列，对这些序列进行尺度空间主轮廓的提取，并以该主轮廓作为一种特征向量，实现边缘、角点检测和不同分辨率上的特征提取等。</p><p>尺度空间中各个尺度图像的模糊程度逐渐变大，能够模拟人在距离由近到远时目标在视网膜上的形成过程，尺度越大，图像越模糊。</p><p>图像和高斯函数进行卷积运算能够对图像进行模糊，且不同尺度的高斯核可以得到不同程度的模糊图像，一幅图像的高斯尺度空间可以通过图像和不同尺度的高斯核卷积得到：</p><script type="math/tex; mode=display">L(x,y,σ)=G(x,y,σ)*I(x,y)</script><p>其中，G是高斯函数：</p><script type="math/tex; mode=display">G(x,y,σ)=\frac{1}{2πσ^2}e^\frac{x^2+y^2}{2σ^2}</script><p>其中，σ是尺度空间因子，是高斯正态分布的标准差，反映了图像被模糊的程度，其值越大图像越模糊，对应的尺度也就越大，L(x,y,σ)对应高斯尺度空间。</p><p> T Lindeber在文献《Scale-space theory: a basic tool for analyzing structures at different scales》中证明，高斯核是唯一可以产生多尺度空间的核。</p><p><strong><em>高斯模糊性质：</em></strong><br>（1）高斯模糊具有圆对称性。<br>（2）高斯模糊具有线性可分的性质，可以在二维图像上对两个独立的一维空间分别进行计算，大大的减小了运算次数。<br>（3）对一副图像进行多次连续高斯模糊的效果与一次更大的高斯模糊可以产生同样的效果，大的高斯模型的半径是所用多个高斯模糊半径平方和的平方根。</p><p>例如：使用半径分别为6和8的两次高斯模糊变换得到的效果等同于一次半径为10的高斯模糊的效果：sqrt(6<sup>2</sup>+8<sup>2</sup>)=10，根据这个关系，使用多个连续较小的高斯模糊处理不会比单个高斯较大处理时间要少。</p><p><strong><em>构造尺度空间的目的：</em></strong></p><p>为了检测出来在不同尺度下都存在的特征点，而检测特征点较好的算子是高斯拉普拉斯（LoG），即Δ<sup>2</sup>G：</p><script type="math/tex; mode=display">Δ^2=\frac{ə^2}{əx^2}+\frac{ə^2}{əy^2}</script><p>LoG的缺点：虽然其能够较好的检测到图像中的特征点，但是运算量过大。通常可以使用DoG（差分高斯，Different of Gaussian）来近似计算LoG。设k为相邻两个高斯尺度空间的比例因子，则DoG定义为：</p><script type="math/tex; mode=display">D(x,y,σ)=[G(x,y,kσ)−G(x,y,σ)]∗I(x,y)=L(x,y,kσ)−L(x,y,σ)</script><p>其中，L(x,y,σ)是图像的高斯尺度空间。</p><p>DoG是如何得到的：将相邻的两个高斯空间的图像相减即可得到DoG响应图像，为了得到DoG响应图像，要先构建高斯尺度空间，而高斯的尺度空间可以在图像金字塔将采用的基础上加上高斯滤波得到，也就是对图像金字塔的每层图像使用不同的参数σ进行高斯模糊，使得每层金字塔有多张高斯模糊过的图像，降采样时，金字塔上边一组图像的第一章是由其下面一组图像倒数第三张降采样得到的。</p><h2 id="2-2-DoG空间极值检测（查找关键点）"><a href="#2-2-DoG空间极值检测（查找关键点）" class="headerlink" title="2.2 DoG空间极值检测（查找关键点）"></a>2.2 DoG空间极值检测（查找关键点）</h2><p>关键点是一些十分突出的点，不会因光照条件的改变而消失，比如角点、边缘点、暗区域的亮点和亮区域的暗点，既然两幅图像中有相同的景物，那么使用某种方法分别提取各自的稳定点，这些点之间会有相应的匹配点。</p><p>所谓关键点，就是在不同尺度空间的图像下检测出的具有方向信息的局部极值点。</p><p>为了寻找尺度空间的极值点，每个像素点要和其图像域（同一尺度空间）和尺度域（相邻的尺度空间）的所有相邻点进行比较，当其大于（或者小于）所有相邻点时，该点就是极值点。如图所示，中间的检测点要和其所在图像的3×3邻域8个像素点，以及其相邻的上下两层的3×3领域18个像素点，共26个像素点进行比较。</p><p>从上面的描述中可以知道，每组图像的第一层和最后一层是无法进行比较取得极值的。为了满足尺度变换的连续性，在每一组图像的顶层继续使用高斯模糊生成3幅图像，高斯金字塔每组有S+3层图像，DoG金字塔的每组有S+2组图像。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/4.png" alt="img"> </p><p>DoG在计算上只需相邻尺度高斯平滑后图像相减，因此简化了计算。</p><h2 id="2-3-删除不好的极值点（特征点）"><a href="#2-3-删除不好的极值点（特征点）" class="headerlink" title="2.3 删除不好的极值点（特征点）"></a>2.3 删除不好的极值点（特征点）</h2><p>通过比较检测得到的DoG的局部极值点实在离散的空间搜索得到的，由于离散空间是对连续空间采样得到的结果，因此在离散空间找到的极值点不一定是真正意义上的极值点，因此要设法将不满足条件的点剔除掉。可以通过尺度空间DoG函数进行曲线拟合寻找极值点，这一步的本质是去掉DoG局部曲率非常不对称的点。</p><p>要剔除掉的不符合要求的点主要有两种：<br>（1）低对比度的特征点<br>（2）不稳定的边缘响应点</p><h2 id="2-4-求取特征点的主方向"><a href="#2-4-求取特征点的主方向" class="headerlink" title="2.4 求取特征点的主方向"></a>2.4 求取特征点的主方向</h2><p>经过上面的步骤已经找到了在不同尺度下都存在的特征点，为了实现图像旋转不变性，需要给特征点的方向进行赋值。利用特征点邻域像素的梯度分布特性来确定其方向参数，再利用图像的梯度直方图求取关键点局部结构的稳定方向。</p><p>找到了特征点，也就可以得到该特征点的尺度σ，也就可以得到特征点所在的尺度图像：L(x,y,σ)=G(x,y,σ)∗I(x,y)。</p><p>计算以特征点为中心、以3×1.5σ * 3×1.5σ为半径的区域图像的幅角和幅值，每个点L(x,y)的梯度的模m(x,y)以及方向θ(x,y)可通过下面的公式求得：</p><script type="math/tex; mode=display">m(x,y)=\sqrt{[L(x+1,y)-L(x-1,y)]^2+[L(x,y+1)-L(x,y-1)]^2}</script><script type="math/tex; mode=display">θ(x,y)=arctan\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}</script><p>计算得到梯度方向后，就要使用直方图统计特征点邻域内像素对应的梯度方向和幅值。梯度方向的直方图的横轴是梯度方向的角度（梯度方向的范围是0到360度，直方图每36度一个柱共10个柱，或者没45度一个柱共8个柱），纵轴是梯度方向对应梯度幅值的累加，在直方图的峰值就是特征点的主方向。使用高斯函数对直方图进行平滑以增强特征点近的邻域点对关键点方向的作用，并减少突变的影响。</p><p>得到特征点的主方向后，对于每个特征点可以得到三个信息(x,y,σ,θ)，即位置、尺度和方向。由此可以确定一个SIFT特征区域，一个SIFT特征区域由三个值表示，中心表示特征点位置，半径表示关键点的尺度，箭头表示主方向。具有多个方向的关键点可以被复制成多份，然后将方向值分别赋给复制后的特征点，一个特征点就产生了多个坐标、尺度相等，但是方向不同的特征点。</p><h2 id="2-5-生成特征描述"><a href="#2-5-生成特征描述" class="headerlink" title="2.5 生成特征描述"></a>2.5 生成特征描述</h2><p>通过以上的步骤已经找到了SIFT特征点位置、尺度和方向信息，下面就需要使用一组向量来描述关键点也就是生成特征点描述子，这个描述符不只包含特征点，也含有特征点周围对其有贡献的像素点。描述子应具有较高的独立性，以保证匹配率。</p><p>特征描述符的生成大致有三个步骤：<br>（1）校正旋转主方向，确保旋转不变性。<br>（2）生成描述子，最终形成一个128维的特征向量。<br>（3）归一化处理，将特征向量长度进行归一化处理，进一步去除光照的影响。</p><p>为了保证特征矢量的旋转不变性，要以特征点为中心，在附近邻域内将坐标轴旋转θθ角度，即将坐标轴旋转为特征点的主方向，旋转后邻域内的像素的新坐标为：</p><script type="math/tex; mode=display">\left[ \begin{matrix} x^\text{'} \\ y^\text{'} \end{matrix} \right]=\left[ \begin{matrix} cosθ & -sinθ \\ sinθ & cosθ \end{matrix}\right]\left[ \begin{matrix} x \\ y \end{matrix} \right]</script><p>旋转之后的主方向为中心取8x8的窗口，左图中央为当前关键点的位置，每个小格代表Wie关键点邻域所在尺度空间的一个像素，求取每个像素的梯度幅值和方向，箭头方向代表梯度方向，长度代表梯度幅值，然后利用高斯窗口对其进行加权运算，最后在每个4x4的小块上绘制8个方向的梯度直方图，计算每个梯度方向的累加值，即可形成一个种子点，如右图所示，每个特征的由4个种子点组成，每个种子点有8个方向的向量信息，这种邻域方向性信息联合增强了算法的抗噪能力，同时对于含有定位误差的特征匹配也提供了比较理性的容错性。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/5.png" alt="img" style="zoom:50%;"> </p><p>不同于求主方向，此时每个种子区域的梯度直方图在0-360之间划分为8个方向区间，每个区间为45度，即每个种子点有8个方向的梯度强度信息。</p><p>在实际计算的过程中，为了增强匹配的稳健性，Lowe建立对每个关键点使用4x4共16个种子点来描述，这样一个关键点就会产生128维的SIFT特征向量。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/6.png" alt="img" style="zoom:80%;"> </p><p>通过对特征点周围的像素进行分块，计算块内梯度直方图，生成具有独特性的向量，这个向量是该区域图像信息的一种抽象，具有唯一性。</p><hr><p>综上，SIFT特征对旋转、尺度缩放、亮度等有鲁棒性，是一种非常稳定的局部特征，在图像处理和计算机视觉领域具有很重要的作用，其本身也比较复杂。</p><p>（1）DoG尺度空间的极值检测：</p><p>首先构造DoG尺度空间，在SIFT中使用不同参数的高斯模糊来表示不同的尺度空间，而构造尺度空间是为了检测在不同尺度下都存在的特征点，特征点的检测比较常用的方法是高斯拉普拉斯，但是LoG的运算量是比较大的，Marr和Hidreth指出可以使用DoG（高斯差分）来近似计算LoG，所以在DoG的尺度空间下检测极值点。</p><p>（2）删除不稳定的极值点：</p><p>低对比度的极值点+不稳定的边缘响应点</p><p>（3）确定特征的的主方向：</p><p>以特征点的为中心、以3×1.5σ * 3×1.5σ为半径的领域内计算各个像素点的梯度的幅角和幅值，然后使用直方图对梯度的幅角进行统计。直方图的横轴是梯度的方向，纵轴为梯度方向对应梯度幅值的累加值，直方图中最高峰所对应的方向即为特征点的方向。</p><p>（4）生成特征描述子：</p><p>首先将坐标轴旋转为特征点的方向，以特征点为中心的16x16的窗口的像素的梯度幅值和方向，将窗口内的像素分为16块，每块是其像素内8个方向的直方图统计，共可以形成128维的特征向量。</p><h1 id="3-SIFT算法在OpenCV中的应用"><a href="#3-SIFT算法在OpenCV中的应用" class="headerlink" title="3. SIFT算法在OpenCV中的应用"></a>3. SIFT算法在OpenCV中的应用</h1><p>SIFT算法在OpenCV中主要包括以下几个函数：</p><ol><li><p>cv2.xfeatures2d.SIFT_create()：实例化SIFT</p></li><li><p>sift.detect()：找出关键点</p></li><li><p>cv2.drawKeypoints()：画出关键点</p></li><li><p>sift.compute()：根据关键点计算SIFT向量</p></li></ol><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到特征点</span></span><br><span class="line">sift = cv2.xfeatures2d.SIFT_create()</span><br><span class="line">kp = sift.detect(gray, <span class="literal">None</span>)  <span class="comment"># 关键点</span></span><br><span class="line">img = cv2.drawKeypoints(gray, kp, img)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(gray, <span class="string">'gray'</span>), plt.title(<span class="string">'Gray Image'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(img, <span class="string">'gray'</span>), plt.title(<span class="string">'Keypoints Image'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算特征</span></span><br><span class="line"><span class="comment"># kp为关键点keypoints</span></span><br><span class="line"><span class="comment"># des为描述子descriptors</span></span><br><span class="line">kp, des = sift.compute(gray, kp)</span><br><span class="line">print(np.array(kp).shape) <span class="comment">#(203,)</span></span><br><span class="line">print(des.shape) <span class="comment">#(203, 128)，128维向量</span></span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/7.png" alt="img"> </p><h1 id="4-SIFT算法进行图像分类的思路"><a href="#4-SIFT算法进行图像分类的思路" class="headerlink" title="4. SIFT算法进行图像分类的思路"></a>4. SIFT算法进行图像分类的思路</h1><p>通过比较待识别图片与训练图集中的每一张图片的sift描述子的个数，找出匹配度最高的那张图片所在的类别，则该类别就被判定为待识别图片的类别。</p><p>如以下代码所示，有40类图片，每类图片5张：首先计算待识别图片的des描述子，然后依次比较每一类中每一张图片与该描述子的匹配数之和（平均匹配数），则匹配数之和（平均匹配数）最大的一类判定为与待识别图片为同一类。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/8.png" alt="img"> </p><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-SIFT算法简介&quot;&gt;&lt;a href=&quot;#1-SIFT算法简介&quot; class=&quot;headerlink&quot; title=&quot;1. SIFT算法简介&quot;&gt;&lt;/a&gt;1. SIFT算法简介&lt;/h1&gt;&lt;p&gt;SIFT (Scale-invariant feature transform)，尺度不变特征转换，是一种图像局部特征提取算法，它通过在不同的尺度空间中寻找极值点（特征点，关键点）的精确定位和主方向，构建关键点描述符来提取特征。SIFT提取的关键点具有尺度不变性、旋转不变性，而且不会因光照、仿射变换和噪音等因素而干扰。SIFT所查找到的关键点是一些十分突出、不会因光照、仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="特征提取" scheme="http://yoursite.com/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
  </entry>
  
</feed>
