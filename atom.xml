<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>crossoverpptx&#39;s Blog</title>
  
  <subtitle>极度的痛苦才是精神的最大解放者。唯有此种痛苦，才迫使我们大彻如悟。</subtitle>
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2023-06-20T06:18:37.972Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>crossoverpptx</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>决策树</title>
    <link href="http://yoursite.com/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://yoursite.com/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2023-06-20T05:57:53.000Z</published>
    <updated>2023-06-20T06:18:37.972Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-决策树简介"><a href="#1-决策树简介" class="headerlink" title="1. 决策树简介"></a>1. 决策树简介</h1><p>决策树，顾名思义，就是帮我们做出决策的树。现实生活中我们往往会遇到各种各样的抉择，把我们的决策过程整理一下，就可以发现，该过程实际上就是一个树的模型。<a id="more"></a></p><p>决策树分为分类树和回归树两种，分类树对离散变量做决策树，回归树对连续变量做决策树，这里我们只讨论分类树。</p><p> 比如选择好瓜的时候：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/1.png" alt="img" style="zoom:80%;"> </p><p>我们可以认为色泽、根蒂、敲声是一个西瓜的三个特征，每次我们做出抉择都是基于这三个特征来把一个节点分成好几个新的节点。</p><p>在上面的例子中，色泽、根蒂、声音特征选取完成后，开始进行决策，在我们的问题中，决策的内容实际上是将结果分成两类，即是（1）否（0）好瓜。这一类智能决策问题称为分类问题，决策树是一种简单的处理分类问题的算法。</p><h1 id="2-决策树原理"><a href="#2-决策树原理" class="headerlink" title="2. 决策树原理"></a>2. 决策树原理</h1><h2 id="2-1-引例"><a href="#2-1-引例" class="headerlink" title="2.1 引例"></a>2.1 引例</h2><p>一颗完整的决策树包含以下三个部分：<br>（1）根节点：就是树最顶端的节点，比如上面图中的“色泽”。<br>（2）叶子节点：树最底部的那些节点，也就是决策结果，好瓜还是坏瓜。<br>（3）内部节点，除了叶子结点，都是内部节点。</p><p>树中每个内部节点表示在一个属性特征上的测试，每个分支代表一个测试输出，每个叶节点表示一种类别。</p><p>给定一个决策树的实例：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/2.png" alt="img" style="zoom:80%;"> </p><p>构造决策树如下：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/3.png" alt="img" style="zoom:80%;"> </p><p><strong><em>第一层</em></strong></p><p>根节点：被分成17份，8是/9否，总体的信息熵为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/4.png" alt="img"> </p><p><strong><em>第二层</em></strong></p><p>清晰：被分成9份，7是/2否，它的信息熵为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/5.png" alt="img"> </p><p>稍糊：被分成5份，1是/4否，它的信息熵为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/6.png" alt="img"> </p><p>模糊：被分成3份，0是/3否，它的信息熵为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/7.png" alt="img"> </p><p>我们规定，假设我们选取纹理为分类依据，把它作为根节点，那么第二层的加权信息熵可以定义为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/8.png" alt="img"> </p><p>我们规定，H’&lt; H<sub>0</sub>，也就是随着决策的进行，其不确定度要减小才行，决策肯定是一个由不确定到确定状态的转变。</p><p>因此，决策树采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为0，此时每个叶子节点中的实例都属于同一类。</p><h2 id="2-2-生成算法"><a href="#2-2-生成算法" class="headerlink" title="2.2 生成算法"></a>2.2 生成算法</h2><p>构建决策树时，首先要选择一个根节点，而究竟选择谁来当根节点的准则，有以下三种。</p><h3 id="2-2-1-ID3（信息增益）"><a href="#2-2-1-ID3（信息增益）" class="headerlink" title="2.2.1 ID3（信息增益）"></a>2.2.1 ID3（信息增益）</h3><p>从信息论的知识中我们知道：信息熵越大，样本的纯度越低。ID3 算法的核心思想就是以信息增益来度量特征选择，选择信息增益最大的特征进行分裂。</p><p>信息增益 = 信息熵 - 条件熵：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/9.png" alt="img"> </p><p>也可以表示为H0 - H1，比如上面实例中我选择纹理作为根节点，将根节点一分为三，则：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/10.png" alt="img"> </p><p>意思是，没有选择纹理特征前，是否是好瓜的信息熵为0.998，在我选择了纹理这一特征之后，信息熵下降为0.764，信息熵下降了0.234，也就是信息增益为0.234。</p><h3 id="2-2-2-C4-5（信息增益率）"><a href="#2-2-2-C4-5（信息增益率）" class="headerlink" title="2.2.2 C4.5（信息增益率）"></a>2.2.2 C4.5（信息增益率）</h3><p>C4.5算法最大的特点是克服了ID3对特征数目的偏重这一缺点，引入信息增益率来作为分类标准。</p><p>信息增益率=信息增益/特征本身的熵：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/11.png" alt="img"> </p><p>信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此C4.5并不是直接用增益率最大的特征进行划分，而是使用一个启发式方法：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。</p><p>例如上述的例子，我们考虑纹理本身的熵，也就是是否是好瓜的熵。</p><p>纹理本身有三种可能，每种概率都已知，则纹理的熵为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/12.png" alt="img"> </p><p>那么选择纹理作为分类依据时，信息增益率为：</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/13.png" alt="img"> </p><h3 id="2-2-3-CART（基尼指数）"><a href="#2-2-3-CART（基尼指数）" class="headerlink" title="2.2.3 CART（基尼指数）"></a>2.2.3 CART（基尼指数）</h3><p>基尼指数（基尼不纯度）：表示在样本集合中一个随机选中的样本被分错的概率。</p><p>基尼系数越小，不纯度越低，特征越好。这和信息增益（率）正好相反。基尼指数可以用来度量任何不均匀分布，是介于0-1之间的数，0是完全相等，1是完全不相等。</p><p><img src="/2023/06/20/%E5%86%B3%E7%AD%96%E6%A0%91/14.png" alt="img"> </p><h2 id="2-3-三种算法的对比"><a href="#2-3-三种算法的对比" class="headerlink" title="2.3 三种算法的对比"></a>2.3 三种算法的对比</h2><p><strong><em>适用范围：</em></strong></p><p>ID3算法只能处理离散特征的分类问题，C4.5能够处理离散特征和连续特征的分类问题，CART算法可以处理离散和连续特征的分类与回归问题。</p><p><strong><em>假设空间：</em></strong></p><p>ID3和C4.5算法使用的决策树可以是多分叉的，而CART算法的决策树必须是二叉树。</p><p><strong><em>优化算法：</em></strong></p><p>ID3算法没有剪枝策略，当叶子节点上的样本都属于同一个类别或者所有特征都使用过了的情况下决策树停止生长。</p><p>C4.5算法使用预剪枝策略，当分裂后的增益小于给定阈值或者叶子上的样本数量小于某个阈值或者叶子节点数量达到限定值或者树的深度达到限定值，决策树停止生长。</p><p>CART决策树主要使用后剪枝策略。</p><h2 id="2-4-剪枝处理"><a href="#2-4-剪枝处理" class="headerlink" title="2.4 剪枝处理"></a>2.4 剪枝处理</h2><p>决策树算法很容易过拟合，剪枝算法就是用来防止决策树过拟合，提高泛华性能的方法。剪枝分为<strong>预剪枝</strong>与<strong>后剪枝</strong>。</p><h3 id="2-4-1-预剪枝"><a href="#2-4-1-预剪枝" class="headerlink" title="2.4.1 预剪枝"></a>2.4.1 预剪枝</h3><p>预剪枝是指在决策树的生成过程中，对每个节点在划分前先进行评估，若当前的划分不能带来泛化性能的提升，则停止划分，并将当前节点标记为叶节点。</p><p>预剪枝方法有：<br>（1）当叶节点的实例个数小于某个阈值时停止生长；<br>（2）当决策树达到预定高度时停止生长；<br>（3）当每次拓展对系统性能的增益小于某个阈值时停止生长；</p><p>预剪枝不足就是剪枝后决策树可能会不满足需求就被过早停止决策树的生长。</p><h3 id="2-4-2-后剪枝"><a href="#2-4-2-后剪枝" class="headerlink" title="2.4.2 后剪枝"></a>2.4.2 后剪枝</h3><p>后剪枝是指先从训练集生成一颗完整的决策树，然后自底向上对非叶节点进行考察，若将该节点对应的子树替换为叶节点，能带来泛化性能的提升，则将该子树替换为叶节点。</p><p>后剪枝决策树通常比预剪枝决策树保留了更多的分枝，一般情形下，后剪枝决策树的欠拟合风险很小，泛化能力往往优于预剪枝决策树。但后剪枝决策树是在生产完全决策树之后进行的，并且要自底向上地对所有非叶子节点进行逐一考察，因此其训练时间开销比未剪枝的决策树和预剪枝的决策树都要大很多。</p><h1 id="3-决策树特点"><a href="#3-决策树特点" class="headerlink" title="3. 决策树特点"></a>3. 决策树特点</h1><p><strong><em>优点：</em></strong><br>容易理解，可解释性较好<br>可以用于小数据集<br>时间复杂度较小<br>可以处理多输入问题，可以处理不相关特征数据<br>对缺失值不敏感 </p><p><strong><em>缺点：</em></strong><br>在处理特征关联性比较强的数据时，表现得不太好<br>当样本中各类别不均匀时，信息增益会偏向于那些具有更多数值的特征<br>对连续性的字段比较难预测<br>容易出现过拟合<br>当类别太多时，错误可能会增加得比较快</p><h1 id="4-决策树的Python应用"><a href="#4-决策树的Python应用" class="headerlink" title="4. 决策树的Python应用"></a>4. 决策树的Python应用</h1><p>在sklearn库中提供了DecisionTreeClassifier函数来实现决策树算法，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>,max_deepth=<span class="literal">None</span>,random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：<br>在python中决策数中默认的是gini系数，也可以选择信息增益的熵’entropy’<br>max_depth：树的深度大小<br>random_state：随机数种子</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line">  </span><br><span class="line">iris = load_iris()<span class="comment"># 数据集导入</span></span><br><span class="line">features = iris.data<span class="comment"># 属性特征</span></span><br><span class="line">labels = iris.target<span class="comment"># 分类标签</span></span><br><span class="line">train_features, test_features, train_labels, test_labels = \</span><br><span class="line">    train_test_split(features, labels, test_size=<span class="number">0.3</span>, random_state=<span class="number">1</span>)<span class="comment"># 训练集，测试集分类</span></span><br><span class="line">clf = tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>,max_depth=<span class="number">3</span>)</span><br><span class="line">clf = clf.fit(train_features, train_labels)<span class="comment">#X,Y分别是属性特征和分类label</span></span><br><span class="line">test_labels_predict = clf.predict(test_features)<span class="comment"># 预测测试集的标签</span></span><br><span class="line">score = accuracy_score(test_labels, test_labels_predict)<span class="comment"># 将预测后的结果与实际结果进行对比</span></span><br><span class="line">print(<span class="string">"CART分类树的准确率 %.4lf"</span> % score)<span class="comment"># 输出结果</span></span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=<span class="string">'iris_tree.dot'</span>)<span class="comment"># 生成决策树可视化的dot文件</span></span><br></pre></td></tr></table></figure><p> 输出结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CART分类树的准确率 <span class="number">0.9556</span></span><br></pre></td></tr></table></figure><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-决策树简介&quot;&gt;&lt;a href=&quot;#1-决策树简介&quot; class=&quot;headerlink&quot; title=&quot;1. 决策树简介&quot;&gt;&lt;/a&gt;1. 决策树简介&lt;/h1&gt;&lt;p&gt;决策树，顾名思义，就是帮我们做出决策的树。现实生活中我们往往会遇到各种各样的抉择，把我们的决策过程整理一下，就可以发现，该过程实际上就是一个树的模型。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯算法</title>
    <link href="http://yoursite.com/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-19T02:36:12.000Z</published>
    <updated>2023-06-19T04:08:18.300Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-贝叶斯定理"><a href="#1-贝叶斯定理" class="headerlink" title="1. 贝叶斯定理"></a>1. 贝叶斯定理</h1><p>先验概率：即基于统计的概率，是基于以往历史经验和分析得到的结果，不需要依赖当前发生的条件。</p><p>后验概率：则是从条件概率而来，由因推果，是基于当下发生了事件之后计算的概率，依赖于当前发生的条件。<a id="more"></a></p><p>条件概率：记事件A发生的概率为P(A)，事件B发生的概率为P(B)，则在B事件发生的前提下，A事件发生的概率即为条件概率，记为P(A|B)。<!-- more --></p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/1.png" alt="img"> </p><p>贝叶斯公式：贝叶斯公式便是基于条件概率，通过P(B|A)来求P(A|B)，如下：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/2.png" alt="img"> </p><p>将A看成“规律”，B看成“现象”，那么贝叶斯公式可以看成：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/3.png" alt="img"> </p><p>全概率公式：表示若事件A1,A2,…,An构成一个完备事件组且都有正概率，则对任意一个事件B都有公式成立：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/4.png" alt="img"> </p><p>将全概率公式带入贝叶斯公式中，得到：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/5.png" alt="img"> </p><h1 id="2-朴素贝叶斯"><a href="#2-朴素贝叶斯" class="headerlink" title="2. 朴素贝叶斯"></a>2. 朴素贝叶斯</h1><h2 id="2-1-朴素贝叶斯原理"><a href="#2-1-朴素贝叶斯原理" class="headerlink" title="2.1 朴素贝叶斯原理"></a>2.1 朴素贝叶斯原理</h2><p>特征条件假设：假设每个特征之间没有联系，给定训练数据集，其中每个样本x都包括n维特征，即x = (x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>)，类标记集合含有k种类别，即y = (y<sub>1</sub>,y<sub>2</sub>,…,y<sub>k</sub>)。</p><p>对于给定的新样本x，判断其属于哪个标记的类别，根据贝叶斯定理，可以得到x属于y<sub>k</sub>类别的概率P(y<sub>k</sub>|x)：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/6.png" alt="img"> </p><p>最大的类别记为预测类别，即：<img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/7.png" alt="img"></p><p>朴素贝叶斯算法对条件概率分布作出了独立性的假设，通俗地讲就是说假设各个维度的特征x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>互相独立，在这个假设的前提上，条件概率可以转化为：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/8.png" alt="img"> </p><p>代入上面贝叶斯公式中，得到：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/9.png" alt="img"> </p><p>于是，朴素贝叶斯分类器可表示为：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/10.png" alt="img"> </p><p>因为对所有的y<sub>k</sub>，上式中的分母的值都是一样的，所以可以忽略分母部分，朴素贝叶斯分类器最终表示为：</p><p><img src="/2023/06/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/11.png" alt="img"> </p><h2 id="2-2-朴素贝叶斯适用范围"><a href="#2-2-朴素贝叶斯适用范围" class="headerlink" title="2.2 朴素贝叶斯适用范围"></a>2.2 朴素贝叶斯适用范围</h2><p>朴素贝叶斯只适用于特征之间是条件独立的情况下，否则分类效果不好，这里的朴素指的就是条件独立。</p><p>朴素贝叶斯主要被广泛地使用在文档分类中。</p><h2 id="2-3-朴素贝叶斯常用模型"><a href="#2-3-朴素贝叶斯常用模型" class="headerlink" title="2.3 朴素贝叶斯常用模型"></a>2.3 朴素贝叶斯常用模型</h2><p>朴素贝叶斯常用的模型如下：<br>（1）高斯模型：处理特征是连续型变量的情况。<br>（2）多项式模型：最常见，要求特征是离散数据。<br>（3）伯努利模型：要求特征是离散的，且为布尔类型，即true和false，或者1和0。</p><h1 id="3-朴素贝叶斯算法的特点"><a href="#3-朴素贝叶斯算法的特点" class="headerlink" title="3. 朴素贝叶斯算法的特点"></a>3. 朴素贝叶斯算法的特点</h1><p><strong><em>优点：</em></strong><br>1、朴素贝叶斯模型有稳定的分类效率。<br>2、对小规模的数据表现很好，能处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练。<br>3、对缺失数据不太敏感，算法也比较简单，常用于文本分类。</p><p><strong><em>缺点：</em></strong><br>1、需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。<br>2、对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。</p><h1 id="4-朴素贝叶斯的Python应用"><a href="#4-朴素贝叶斯的Python应用" class="headerlink" title="4. 朴素贝叶斯的Python应用"></a>4. 朴素贝叶斯的Python应用</h1><p>在sklearn库中提供了GaussianNB、MultinomialNB和BernoulliNB 3种朴素贝叶斯模型，下面以GaussianNB为例。</p><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB  <span class="comment"># 高斯分布，假定特征服从正态分布的</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment"># 数据集划分</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分数据集,random_state:随机数种子</span></span><br><span class="line">train_x,test_x,train_y,test_y = train_test_split(iris.data,iris.target,random_state=<span class="number">12</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 建模</span></span><br><span class="line">gnb_clf = GaussianNB()</span><br><span class="line">gnb_clf.fit(train_x,train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行预测</span></span><br><span class="line"><span class="comment"># predict()：直接给出预测的类别</span></span><br><span class="line"><span class="comment"># predict_proba()：输出的是每个样本属于某种类别的概率</span></span><br><span class="line">predict_class = gnb_clf.predict(test_x)</span><br><span class="line"><span class="comment"># predict_class_proba = gnb_clf.predict_proba(test_x)</span></span><br><span class="line">print(<span class="string">"测试集准确率为："</span>,accuracy_score(test_y,predict_class))</span><br></pre></td></tr></table></figure><p> 输出结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">测试集准确率为： <span class="number">0.9736842105263158</span>。</span><br></pre></td></tr></table></figure><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-贝叶斯定理&quot;&gt;&lt;a href=&quot;#1-贝叶斯定理&quot; class=&quot;headerlink&quot; title=&quot;1. 贝叶斯定理&quot;&gt;&lt;/a&gt;1. 贝叶斯定理&lt;/h1&gt;&lt;p&gt;先验概率：即基于统计的概率，是基于以往历史经验和分析得到的结果，不需要依赖当前发生的条件。&lt;/p&gt;
&lt;p&gt;后验概率：则是从条件概率而来，由因推果，是基于当下发生了事件之后计算的概率，依赖于当前发生的条件。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归</title>
    <link href="http://yoursite.com/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</id>
    <published>2023-06-18T02:37:42.000Z</published>
    <updated>2023-06-18T03:13:44.729Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-逻辑回归简介"><a href="#1-逻辑回归简介" class="headerlink" title="1. 逻辑回归简介"></a>1. 逻辑回归简介</h1><p>逻辑回归（Logistic Regression）虽然被称为回归，但其实际上是分类模型，并常用于二分类。逻辑回归与线性回归本质上是类似的，相较线性回归只是多了一个Logistic函数（或称为Sigmoid函数）。<a id="more"></a></p><h2 id="1-1-分类和回归"><a href="#1-1-分类和回归" class="headerlink" title="1.1 分类和回归"></a>1.1 分类和回归</h2><p>分类和回归是机器学习可以解决的两大主要问题，从预测值的类型上看，连续变量预测的定量输出称为回归；离散变量预测的定性输出称为分类。例如：预测明天多少度，是一个回归任务；预测明天阴还是晴，就是一个分类任务。</p><h2 id="1-2-逻辑回归"><a href="#1-2-逻辑回归" class="headerlink" title="1.2 逻辑回归"></a>1.2 逻辑回归</h2><p>Logistic Regression原理与Linear Regression回归类似，其主要流程如下：<br>（1）构建预测函数。一般来说在构建之前，需要根据数据来确定函数模型，是线性还是非线性。<br>（2）构建Cost函数(损失函数)。该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。<br>（3）采用梯度下降算法，minJ(θ)。在数据量很大时，梯度下降算法执行起来会较慢。因此出现了随机梯度下降等算法进行优化。</p><h1 id="2-逻辑回归原理"><a href="#2-逻辑回归原理" class="headerlink" title="2. 逻辑回归原理"></a>2. 逻辑回归原理</h1><h2 id="2-1-构造预测函数"><a href="#2-1-构造预测函数" class="headerlink" title="2.1 构造预测函数"></a>2.1 构造预测函数</h2><p>在线性回归中模型中预测函数为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/1.png" alt="img"> </p><p>逻辑回归作为分类问题，结果h = {1 or 0}即可。若逻辑回归采用线性回归预测函数则会产生远大于1或远小于0得值，不便于预测。因此线性回归预测函数需要做一定改进。设想如果有一个函数h(x)能够把预测结果值压缩到0-1这个区间，那么我们就可以设定一个阈值s，若h(x) &gt;= s,则认定为预测结果为1，否之为0。</p><p>实际中也存在这样的函数：Logistic函数（或称为Sigmoid函数），函数表达式为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/2.png" alt="img"> </p><p>其图像如下图所示：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/3.png" alt="img"> </p><p>对于线性边界的情况，边界形式如下：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/4.png" alt="img"> </p><p>构造预测函数为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/5.png" alt="img"> </p><p>函数h<sub>θ</sub>(x)的值有特殊的含义，它表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/6.png" alt="img">    公式（1）</p><h2 id="2-2-构造损失函数-J"><a href="#2-2-构造损失函数-J" class="headerlink" title="2.2 构造损失函数 J"></a>2.2 构造损失函数 J</h2><p>Cost函数和J函数如下，它们是基于最大似然估计推导得到的：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/7.png" alt="img"> </p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/8.png" alt="img"> </p><p>下面详细说明推导的过程：</p><p>2.1节公式(1) 综合起来可以写成：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/9.png" alt="img"> </p><p>取似然函数为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/10.png" alt="img"> </p><p>对数似然函数为：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/11.png" alt="img"> </p><p>最大似然估计就是求使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将取为下式，即：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/12.png" alt="img"> </p><h2 id="2-3-梯度下降法求J-θ-的最小值"><a href="#2-3-梯度下降法求J-θ-的最小值" class="headerlink" title="2.3 梯度下降法求J(θ)的最小值"></a>2.3 梯度下降法求J(θ)的最小值</h2><p>θ更新过程如下：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/13.png" alt="img"> </p><p>经化简后θ的最终更新过程可以写成：</p><p><img src="/2023/06/18/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/14.png" alt="img"> </p><p>另外，可以通过向量化，即使用矩阵计算来代替for循环，以简化计算过程，提高效率；可以通过正则化解决过拟合问题（过拟合即过分拟合了训练数据，使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力））。</p><h1 id="3-逻辑回归特点"><a href="#3-逻辑回归特点" class="headerlink" title="3. 逻辑回归特点"></a>3. 逻辑回归特点</h1><p><strong><em>优点：</em></strong><br>（1）简单实现，模型的可解释性很好。<br>（2）训练速度快。计算的量只和特征的数目有关，不需要缩放输入特征等等。<br>（3）资源占用小。因为只需要存储各个维度的特征值。</p><p><strong><em>缺点：</em></strong><br>（1）准确率不是很高，就是因为模型（非常类似线性模型），很难拟合数据的真实分布，容易导致过拟合。<br>（2）难处理数据不平衡的问题。<br>（3）逻辑回归本身无法解决非线性问题，因为它的决策边界是线性的。<br>（4）高度依赖正确的数据表示。</p><h1 id="4-逻辑回归的应用场景"><a href="#4-逻辑回归的应用场景" class="headerlink" title="4. 逻辑回归的应用场景"></a>4. 逻辑回归的应用场景</h1><p>当Y变量只有两个值时，当面临分类问题时，可以考虑使用逻辑回归。</p><p>逻辑回归也用于多分类别分类。有很多种多分类算法，如随机森林分类器或者朴素贝叶斯分类器等等。逻辑回归也可以用于多分类任务。可以通过一些技巧，分两种策略：</p><p>一对多策略：基本思想是将第i种类型的所有样本作为正例，将剩下的所有样本作为负例。进行训练得出一个分类器。这样，我们就得到N个分类器。预测的时候，将样本给N个分类器，获得N个结果，选择其中概率值最大的那个作为结果。</p><p>一对一策略：这种策略，假设有N个类别，不同的类别之间，训练一个分类器，训练得到的结果有</p><script type="math/tex; mode=display">C_N^2</script><p>种不同的分类器。预测的时候，将样本给所有的分类器，会有N(N-1)个结果，最终结果通过“投票”产生。</p><h1 id="5-逻辑回归的Python应用"><a href="#5-逻辑回归的Python应用" class="headerlink" title="5. 逻辑回归的Python应用"></a>5. 逻辑回归的Python应用</h1><h2 id="5-1-自定义函数实现逻辑回归"><a href="#5-1-自定义函数实现逻辑回归" class="headerlink" title="5.1 自定义函数实现逻辑回归"></a>5.1 自定义函数实现逻辑回归</h2><p>可以自定义sigmod函数、损失函数、梯度下降等实现逻辑回归，如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># sigmod函数，即得分函数,计算数据的概率是0还是1；得到y大于等于0.5是1，y小于等于0.5为0。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmod</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="comment"># hx是概率估计值，是sigmod(x)得来的值，y是样本真值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(hx, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -y * np.log(hx) - (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - hx)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(current_para, x, y, learning_rate)</span>:</span></span><br><span class="line">    m = len(y)</span><br><span class="line">    matrix_gradient = np.zeros(len(x[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        current_x = x[i]</span><br><span class="line">        current_y = y[i]</span><br><span class="line">        current_x = np.asarray(current_x)</span><br><span class="line">        matrix_gradient += (sigmod(np.dot(current_para, current_x)) - current_y) * current_x</span><br><span class="line">    new_para = current_para - learning_rate * matrix_gradient</span><br><span class="line">    <span class="keyword">return</span> new_para</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 误差计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error</span><span class="params">(para, x, y)</span>:</span></span><br><span class="line">    total = len(y)</span><br><span class="line">    error_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total):</span><br><span class="line">        current_x = x[i]</span><br><span class="line">        current_y = y[i]</span><br><span class="line">        hx = sigmod(np.dot(para, current_x))  <span class="comment"># LR算法</span></span><br><span class="line">        <span class="keyword">if</span> cost(hx, current_y) &gt; <span class="number">0.5</span>:  <span class="comment"># 进一步计算损失</span></span><br><span class="line">            error_num += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> error_num / total</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(initial_para, x, y, learning_rate, num_iter)</span>:</span></span><br><span class="line">    dataMat = np.asarray(x)</span><br><span class="line">    labelMat = np.asarray(y)</span><br><span class="line">    para = initial_para</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iter + <span class="number">1</span>):</span><br><span class="line">        para = gradient(para, dataMat, labelMat, learning_rate)  <span class="comment"># 梯度下降法</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            err = error(para, dataMat, labelMat)</span><br><span class="line">            print(<span class="string">"iter:"</span> + str(i) + <span class="string">" ; error:"</span> + str(err))</span><br><span class="line">    <span class="keyword">return</span> para</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 数据集加载</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"logistic_regression_binary.csv"</span>, <span class="string">"r+"</span>) <span class="keyword">as</span> file_object:</span><br><span class="line">        lines = file_object.readlines()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            line_array = line.strip().split()</span><br><span class="line">            dataMat.append([<span class="number">1.0</span>, float(line_array[<span class="number">0</span>]), float(line_array[<span class="number">1</span>])])  <span class="comment"># 数据</span></span><br><span class="line">            labelMat.append(int(line_array[<span class="number">2</span>]))  <span class="comment"># 标签</span></span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 绘制图形</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span><span class="params">(wei, data, label)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> type(wei).__name__ == <span class="string">'ndarray'</span>:</span><br><span class="line">        weights = wei</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        weights = wei.getA()</span><br><span class="line">    fig = plt.figure(<span class="number">0</span>)</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    xxx = np.arange(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line">    yyy = - weights[<span class="number">0</span>] / weights[<span class="number">2</span>] - weights[<span class="number">1</span>] / weights[<span class="number">2</span>] * xxx</span><br><span class="line">    ax.plot(xxx, yyy)</span><br><span class="line">    cord1 = []</span><br><span class="line">    cord0 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(label)):</span><br><span class="line">        <span class="keyword">if</span> label[i] == <span class="number">1</span>:</span><br><span class="line">            cord1.append(data[i][<span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cord0.append(data[i][<span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">    cord1 = np.array(cord1)</span><br><span class="line">    cord0 = np.array(cord0)</span><br><span class="line">    ax.scatter(cord1[:, <span class="number">0</span>], cord1[:, <span class="number">1</span>], c=<span class="string">'g'</span>)</span><br><span class="line">    ax.scatter(cord0[:, <span class="number">0</span>], cord0[:, <span class="number">1</span>], c=<span class="string">'r'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span><span class="params">()</span>:</span></span><br><span class="line">    x, y = load_dataset()</span><br><span class="line">    n = len(x[<span class="number">0</span>])</span><br><span class="line">    initial_para = np.ones(n)</span><br><span class="line">    learning_rate = <span class="number">0.001</span></span><br><span class="line">    num_iter = <span class="number">1000</span></span><br><span class="line">    print(<span class="string">"初始参数："</span>, initial_para)</span><br><span class="line">    para = train(initial_para, x, y, learning_rate, num_iter)</span><br><span class="line">    print(<span class="string">"训练所得参数："</span>, para)</span><br><span class="line">    plotBestFit(para, x, y)</span><br><span class="line"> </span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    logistic_regression()</span><br></pre></td></tr></table></figure><h2 id="5-2-sklearn库LogisticRegression函数的应用"><a href="#5-2-sklearn库LogisticRegression函数的应用" class="headerlink" title="5.2 sklearn库LogisticRegression函数的应用"></a>5.2 sklearn库LogisticRegression函数的应用</h2><p>另外，可以通过sklearn库的逻辑回归函数来训练模型并预测，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LogisticRegression(C=<span class="number">1.0</span>,class_weight=<span class="literal">None</span>,dual=<span class="literal">False</span>,fit_intercept=<span class="literal">True</span>,intercept_scaling=<span class="number">1</span>,max_iter=<span class="number">100</span>,multi_class=<span class="string">'ovr'</span>,n_jobs=<span class="number">1</span>,penalty=<span class="string">'l2'</span>,random_state=<span class="literal">None</span>,solver=<span class="string">'liblinear'</span>,tol=<span class="number">0.0001</span>,verbose=<span class="number">0</span>,warm_start=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明:</p><p>C：正则化系数λ的倒数，默认为1.0。</p><p>class_weight：用于标示分类模型中各种类型的权重，不考虑权重，即为None。</p><p>dual：对偶或原始方法，bool类型，默认为False。</p><p>fit_intercept：是否存在截距或偏差，bool类型，默认为True。</p><p>intercept_scaling：仅在正则化项为”liblinear”，且fit_intercept设置为True时有用，float类型，默认为1。</p><p>max_iter：算法收敛最大迭代次数，int类型，默认为10，仅在正则化优化算法为newton-cg, sag和lbfgs才有用。</p><p>multi_class：分类方式选择参数，str类型，可选参数为ovr和multinomial，默认为ovr。</p><p>n_jobs：并行数，int类型，默认为1。</p><p>penalty：用于指定惩罚项中使用的规范，str类型，可选参数为l1和l2，默认为l2。</p><p>random_state：随机数种子，int类型，可选参数，默认为无。</p><p>solver：优化算法选择参数，决定了我们对逻辑回归损失函数的优化方法。有五个可选参数，即newtoncg,lbfgs,liblinear,sag,saga。对于小型数据集来说，‘liblinear’是一个不错的选择，而‘sag’和‘saga’对于大型数据集则更快。</p><p>tol：停止求解的标准，float类型，默认为1e-4。</p><p>verbose：日志冗长度，int类型，默认为0。</p><p>warm_start：热启动参数，bool类型，默认为False。</p></blockquote><p>下面以通过泰坦尼克号数据集预测乘客生还情况为例，说明上述函数的使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 建立模型用的训练数据集和验证数据集</span></span><br><span class="line">train_X, test_X, train_y, test_y = train_test_split(source_X , source_y, train_size=<span class="number">.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment"># 创建模型：逻辑回归（logisic regression）</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit( train_X , train_y )</span><br><span class="line">LogisticRegression(C=<span class="number">1.0</span>, class_weight=<span class="literal">None</span>, dual=<span class="literal">False</span>, fit_intercept=<span class="literal">True</span>,</span><br><span class="line">          intercept_scaling=<span class="number">1</span>, max_iter=<span class="number">100</span>, multi_class=<span class="string">'ovr'</span>, n_jobs=<span class="number">1</span>,</span><br><span class="line">          penalty=<span class="string">'l2'</span>, random_state=<span class="literal">None</span>, solver=<span class="string">'liblinear'</span>, tol=<span class="number">0.0001</span>,</span><br><span class="line">          verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="comment"># 分类问题，score得到的是模型的正确率</span></span><br><span class="line">model.score(test_X , test_y )</span><br><span class="line"><span class="comment"># k折交叉验证</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="comment"># 将训练集分成5份，4份用来训练模型，1份用来预测，这样就可以用不同的训练集在一个模型中训练</span></span><br><span class="line">print(model_selection.cross_val_score(model, source_X, source_y, cv=<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果预测</span></span><br><span class="line">pred_Y = model.predict(pred_X)</span><br><span class="line"><span class="comment"># 生成的预测值是浮点数（0.0,1,0）,所以要对数据类型进行转换</span></span><br><span class="line">pred_Y=pred_Y.astype(int)</span><br><span class="line"><span class="comment"># 乘客id</span></span><br><span class="line">passenger_id = full.loc[sourceRow:,<span class="string">'PassengerId'</span>]</span><br><span class="line"><span class="comment"># 数据框：乘客id，预测生存情况的值</span></span><br><span class="line">predDf = pd.DataFrame(</span><br><span class="line">    &#123; <span class="string">'PassengerId'</span>: passenger_id ,</span><br><span class="line">     <span class="string">'Survived'</span>: pred_Y &#125; )</span><br><span class="line">predDf.shape</span><br><span class="line">print(predDf.head())</span><br><span class="line"><span class="comment"># 保存结果</span></span><br><span class="line">predDf.to_csv(<span class="string">'titanic_pred.csv'</span>, index = <span class="literal">False</span> )</span><br></pre></td></tr></table></figure><h1 id="6-源码仓库地址"><a href="#6-源码仓库地址" class="headerlink" title="6. 源码仓库地址"></a>6. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-逻辑回归简介&quot;&gt;&lt;a href=&quot;#1-逻辑回归简介&quot; class=&quot;headerlink&quot; title=&quot;1. 逻辑回归简介&quot;&gt;&lt;/a&gt;1. 逻辑回归简介&lt;/h1&gt;&lt;p&gt;逻辑回归（Logistic Regression）虽然被称为回归，但其实际上是分类模型，并常用于二分类。逻辑回归与线性回归本质上是类似的，相较线性回归只是多了一个Logistic函数（或称为Sigmoid函数）。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>K-means算法</title>
    <link href="http://yoursite.com/2023/06/17/K-means%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/17/K-means%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-17T12:20:15.000Z</published>
    <updated>2023-06-17T12:39:29.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-K-means算法简介"><a href="#1-K-means算法简介" class="headerlink" title="1. K-means算法简介"></a>1. K-means算法简介</h1><p>K-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。<a id="more"></a></p><h1 id="2-K-means算法原理"><a href="#2-K-means算法原理" class="headerlink" title="2. K-means算法原理"></a>2. K-means算法原理</h1><p>K-means算法解决的问题是，在事先不知道如何分类的情况下（即无监督），让程序根据距离的远近，把N个对象（局部）最优的划分为k个类。它是无监督算法中比较常见的一种算法，原理比较简单易懂。本质是通过循环，不断迭代类中心点，计算各个对象到新的类中心点的距离并根据距离最近的原则重新归类，当类内距离最小、类间距离最大时，即可停止迭代（使用中，常常会限定迭代次数，防止陷入死循环。当达到预先设定得循环次数或类中心点不再发生变化时，最后一次迭代得到的结果，即为最终聚类结果）。</p><h2 id="2-1-算法具体步骤"><a href="#2-1-算法具体步骤" class="headerlink" title="2.1 算法具体步骤"></a>2.1 算法具体步骤</h2><p>算法具体步骤如下：<br>第一步：指定聚类类数k（此处涉及k的选择方法）；<br>第二步：选定初始化聚类中心。随机或指定k个对象，作为初始化聚类中心（此处随机选的方法可以升级，以达到更好的聚类效果，比如kmeans++聚类算法）；<br>第三步：得到初始化聚类结果。计算每个对象到k个聚类中心的距离，把每个对象分配给离它最近的聚类中心所代表的类别中，全部分配完毕即得到初始化聚类结果，聚类中心连同分配给它的对象作为一类；<br>第四步：重新计算聚类中心。得到初始化聚类结果后，重新计算每类的类中心点（计算均值），得到新的聚类中心；<br>第五步：迭代循环，得到最终聚类结果。重复第三步和第四步，直到满足迭代终止条件。</p><h2 id="2-2-k取值方法"><a href="#2-2-k取值方法" class="headerlink" title="2.2 k取值方法"></a>2.2 k取值方法</h2><p>结合书本中理论和在实际业务中遇到的情况，总结出以下几点k值的选择方法供参考：<br>1、如果实际业务中，数据维度不超过三维，可先通过画散点图的方法大致确定聚类数目。<br>2、工作中，结合业务方需求背景或经验，可敲定聚类数目。如，做用户的RFM模型（k=3）、判断是否为作弊用户（k=2）等。<br>3、实际业务中做探索性分析时，没有经验等做参考，可使用肘方法（elbow method）确定分类数。此处结合第一部分提到的误差平方和最小来一起理解。要使误差平方和变小，一种方法就是增加类数，这样有助于降低每个类的类内误差平方和，从而降低整体的误差平方和。但若类数太多，一是归类后解释困难，另一个降低类内误差平方和的边际效应可能下降（即增加类数k，误差平方和降低的不显著）。此时，k取误差平方和关于k的曲线的拐点。<br>4、交叉验证方法。将N个对象分为m个部分，用m-1个部分建立聚类模型，并用剩下的一部分检验聚类质量。<br>5、轮廓系数法。计算k取不同值时的轮廓系数，选择轮廓系数较接近1的分类数。</p><p>下面具体介绍<strong>手肘法</strong>和<strong>轮廓系数法</strong>。</p><h3 id="2-2-1-手肘法"><a href="#2-2-1-手肘法" class="headerlink" title="2.2.1 手肘法"></a>2.2.1 手肘法</h3><p>核心公式：SSE(sum of the squared errors，误差平方和)</p><p><img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/1.png" alt="img"> </p><p>其中，C<sub>i</sub>是第i个簇；x是C<sub>i</sub>中的样本点；m<sub>i</sub>是C<sub>i</sub>的质心（C<sub>i</sub>中所有样本的均值）；SSE是所有样本的聚类误差，代表了聚类效果的好坏。</p><p>随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。</p><p><img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/2.png" alt="img" style="zoom: 67%;"> </p><p>显然，肘部对于的k值为4，梯度最大，下降最快，故对于这个数据集的聚类而言，最佳聚类数应该选4。</p><h3 id="2-2-2-轮廓系数法"><a href="#2-2-2-轮廓系数法" class="headerlink" title="2.2.2 轮廓系数法"></a>2.2.2 轮廓系数法</h3><p>具体方法如下：<br>1）计算样本i到同簇其他样本的平均距离a<sub>i</sub>。a<sub>i</sub>越小，说明样本i越应该被聚类到该簇。将a<sub>i</sub>称为样本i的簇内不相似度。簇C中所有样本的均值称为簇C的簇不相似度。<br>2）计算样本i到其他某簇C<sub>j</sub>的所有样本的平均距离b<sub>i</sub><sup>j</sup>，称为样本i与簇C<sub>j</sub>的不相似度。定义为样本i的簇间不相似度：<img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/3.png" alt="img">，bi越大，说明样本i越不属于其他簇。<br>3）根据样本i的簇内不相似度ai和簇间不相似度bi，定义样本i的轮廓系数。</p><p><img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/4.png" alt="img"> </p><p>轮廓系数范围在[-1,1]之间。该值越大，越合理。s<sub>i</sub>接近1，则说明样本i聚类合理；接近-1，则说明样本i更应该分类到另外的簇；若s<sub>i</sub>近似为0，则说明样本i在两个簇的边界上。</p><p>所有样本的s<sub>i</sub>的均值称为聚类结果的轮廓系数，是该聚类是否合理、有效的度量。使用轮廓系数(silhouette coefficient)来确定，选择使系数较大所对应的k值。</p><h2 id="2-3-K-means"><a href="#2-3-K-means" class="headerlink" title="2.3 K-means++"></a>2.3 K-means++</h2><p>我们知道初始值的选取对结果的影响很大，对初始值选择的改进是很重要的一部分。在所有的改进算法中，K-means++最有名。K-means++算法步骤如下所示：（1）随机选取一个中心点a<sub>1</sub>；<br>（2）计算数据到之前n个聚类中心最远的距离D(x)，并以一定概率<img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/5.png" alt="img">选择新中心点a<sub>i</sub>；<br>（3）重复第二步。</p><p>简单的来说，K-means++就是选择离已选中心点最远的点。这也比较符合常理，聚类中心当然是互相离得越远越好。但是这个算法的缺点在于，难以并行化。所以k-meansII改变取样策略，并非按照k-means++那样每次遍历只取样一个样本，而是每次遍历取样k个，重复该取样过程log(n)次，则得到klog(n)个样本点组成的集合，然后从这些点中选取k个。当然一般也不需要log(n)次取样，5次即可。</p><h2 id="2-4-算法终止条件"><a href="#2-4-算法终止条件" class="headerlink" title="2.4 算法终止条件"></a>2.4 算法终止条件</h2><p>终止条件一般为以下几类：<br>a、达到预先设定的迭代次数，如20次。<br>b、类中心点不再发生变化或没有对象被分配给新的类。<br>c、误差平方和最小，误差平方和公式：<img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/6.png" alt="img">，其中Xi代表被分到第i类的对象集合，μ<sub>c(i)</sub>代表第i个聚类的均值（即类中心），c可以理解为迭代这个步骤，因为c随着迭代而发生变化，所以也是个变量。</p><p>在实际编程实现算法时，a常配合着c（误差平方和最小化）一起使用，由于使误差平方和最小有时会陷入死循环或迭代多步类中心变化不大，因此常会限制迭代次数。</p><h1 id="3-K-means算法特点"><a href="#3-K-means算法特点" class="headerlink" title="3. K-means算法特点"></a>3. K-means算法特点</h1><p><strong><em>优点：</em></strong><br>1）容易理解，聚类效果不错，虽然是局部最优，但往往局部最优就够了。<br>2）处理大数据集的时候，该算法可以保证较好的伸缩性。<br>3）当簇近似高斯分布的时候，效果非常不错。<br>4）算法复杂度低。<br>5）主要需要调参的参数仅仅是簇数k。</p><p><strong><em>缺点：</em></strong><br>1）K值的选取不好把握。<br>2）结果的好坏依赖于初始类中心的选择。<br>3）算法常陷入局部最优，更换初始聚类中心后，新的聚类结果可能效果更优。<br>4）对孤立点敏感，如数据集存在异常突出点，会影响聚类效果。<br>5）不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。</p><h1 id="4-K-means算法应用场景"><a href="#4-K-means算法应用场景" class="headerlink" title="4. K-means算法应用场景"></a>4. K-means算法应用场景</h1><p>1、隐含类别的数据较为平衡的情况，如隐含类别的数据量差别较大，则聚类的效果就较差。<br>2、数据最好是凸数据，即隐含类别间的差异越大，则聚类效果越好，因为中心点不再变化所需要的迭代次数较少，比较容易收敛。<br>3、一般作为数据预处理，或者用于辅助分类贴标签使用，因为在已经经过分类的数据上再进行聚类，准确度会非常高。</p><h1 id="5-K-means算法的Python应用"><a href="#5-K-means算法的Python应用" class="headerlink" title="5. K-means算法的Python应用"></a>5. K-means算法的Python应用</h1><h2 id="5-1-K-means算法的Python实现"><a href="#5-1-K-means算法的Python实现" class="headerlink" title="5.1 K-means算法的Python实现"></a>5.1 K-means算法的Python实现</h2><p>以一系列二维点作为原始数据，通过K-means算法来预测新的点属于哪一类。测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">K_Means</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># k是分组数；tolerance'中心点误差'；max_iter是迭代次数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k=<span class="number">2</span>, tolerance=<span class="number">0.0001</span>, max_iter=<span class="number">300</span>)</span>:</span></span><br><span class="line">        self.k_ = k</span><br><span class="line">        self.tolerance_ = tolerance</span><br><span class="line">        self.max_iter_ = max_iter</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.centers_ = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.k_):</span><br><span class="line">            self.centers_[i] = data[random.randint(<span class="number">0</span>,len(data))]</span><br><span class="line">        <span class="comment"># print('center', self.centers_)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.max_iter_):</span><br><span class="line">            self.clf_ = &#123;&#125; <span class="comment">#用于装归属到每个类中的点[k,len(data)]</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.k_):</span><br><span class="line">                self.clf_[i] = []</span><br><span class="line">            <span class="comment"># print("质点:",self.centers_)</span></span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> data:</span><br><span class="line">                distances = [] <span class="comment">#装中心点到每个点的距离[k]</span></span><br><span class="line">                <span class="keyword">for</span> center <span class="keyword">in</span> self.centers_:</span><br><span class="line">                    <span class="comment"># 欧拉距离</span></span><br><span class="line">                    distances.append(np.linalg.norm(feature - self.centers_[center]))</span><br><span class="line">                classification = distances.index(min(distances))</span><br><span class="line">                self.clf_[classification].append(feature)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># print("分组情况:",self.clf_)</span></span><br><span class="line">            prev_centers = dict(self.centers_)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> self.clf_:</span><br><span class="line">                self.centers_[c] = np.average(self.clf_[c], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># '中心点'是否在误差范围</span></span><br><span class="line">            optimized = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">for</span> center <span class="keyword">in</span> self.centers_:</span><br><span class="line">                org_centers = prev_centers[center]</span><br><span class="line">                cur_centers = self.centers_[center]</span><br><span class="line">                <span class="keyword">if</span> np.sum((cur_centers - org_centers) / org_centers * <span class="number">100.0</span>) &gt; self.tolerance_:</span><br><span class="line">                    optimized = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> optimized:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, p_data)</span>:</span></span><br><span class="line">        distances = [np.linalg.norm(p_data - self.centers_[center]) <span class="keyword">for</span> center <span class="keyword">in</span> self.centers_]</span><br><span class="line">        index = distances.index(min(distances))</span><br><span class="line">        <span class="keyword">return</span> index</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1.5</span>, <span class="number">1.8</span>], [<span class="number">5</span>, <span class="number">8</span>], [<span class="number">8</span>, <span class="number">8</span>], [<span class="number">1</span>, <span class="number">0.6</span>], [<span class="number">9</span>, <span class="number">11</span>]])</span><br><span class="line">    k_means = K_Means(k=<span class="number">2</span>)</span><br><span class="line">    k_means.fit(x)</span><br><span class="line">    <span class="keyword">for</span> center <span class="keyword">in</span> k_means.centers_:</span><br><span class="line">        pyplot.scatter(k_means.centers_[center][<span class="number">0</span>], k_means.centers_[center][<span class="number">1</span>], marker=<span class="string">'*'</span>, s=<span class="number">150</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> cat <span class="keyword">in</span> k_means.clf_:</span><br><span class="line">        <span class="keyword">for</span> point <span class="keyword">in</span> k_means.clf_[cat]:</span><br><span class="line">            pyplot.scatter(point[<span class="number">0</span>], point[<span class="number">1</span>], c=(<span class="string">'r'</span> <span class="keyword">if</span> cat == <span class="number">0</span> <span class="keyword">else</span> <span class="string">'b'</span>))</span><br><span class="line"></span><br><span class="line">    predict = [[<span class="number">2</span>, <span class="number">1</span>], [<span class="number">6</span>, <span class="number">9</span>]]</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> predict:</span><br><span class="line">        cat = k_means.predict(feature)</span><br><span class="line">        pyplot.scatter(feature[<span class="number">0</span>], feature[<span class="number">1</span>], c=(<span class="string">'r'</span> <span class="keyword">if</span> cat == <span class="number">0</span> <span class="keyword">else</span> <span class="string">'b'</span>), marker=<span class="string">'x'</span>)</span><br><span class="line"></span><br><span class="line">    pyplot.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/7.png" alt="img"> </p><p>上图可见，所有点被分成了两类（圆点）：红色和蓝色，预测的点（×）被正确分到了所属类别。</p><h2 id="5-2-sklearn-cluster-Kmeans函数的应用"><a href="#5-2-sklearn-cluster-Kmeans函数的应用" class="headerlink" title="5.2 sklearn.cluster.Kmeans函数的应用"></a>5.2 sklearn.cluster.Kmeans函数的应用</h2><p>sklearn库使用sklearn.cluster.KMeans函数来实现K-Means算法，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.cluster.KMeans(n_clusters=<span class="number">8</span>,*,init=<span class="string">'k-means++'</span>, n_init=<span class="number">10</span>,max_iter=<span class="number">300</span>,tol=<span class="number">0.0001</span>,precompute_distances=<span class="string">'deprecated'</span>,verbose=<span class="number">0</span>,random_state=<span class="literal">None</span>,copy_x=<span class="literal">True</span>,n_jobs=<span class="string">'deprecated'</span>,algorithm=<span class="string">'auto'</span>)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：</p><p>n_clusters：int, default=8，簇的个数，即你想聚成几类。</p><p>init：{‘k-means++’, ‘random’, ndarray, callable}, default=’k-means++’，初始簇中心的获取方法，’k-means ++’：以一种聪明的方式为k-mean聚类选择初始聚类中心，以加快收敛速度。有关更多详细信息，请参见k_init中的注释部分。’random’：n_clusters从初始质心的数据中随机选择观察（行）。如果传递了ndarray，则其形状应为（n_clusters，n_features），并给出初始中心。如果传递了callable，则应使用参数X，n_clusters和随机状态并返回初始化。</p><p>n_init：int, default=10，获取初始簇中心的更迭次数，k均值算法将在不同质心种子下运行的次数。</p><p>max_iter：int, default=300，最大迭代次数（因为kmeans算法的实现需要迭代），单次运行的k均值算法的最大迭代次数。</p><p>tol：float, default=1e-4，容忍度，即kmeans运行准则收敛的条件，关于Frobenius范数的相对容差，该范数表示两个连续迭代的聚类中心的差异，以声明收敛。</p><p>precompute_distances：{‘auto’, True, False}, default=’auto’，是否需要提前计算距离，这个参数会在空间和时间之间做权衡，如果是True 会把整个距离矩阵都放到内存中，auto 会默认在数据样本大于featurs*samples 的数量大于12e6 的时候False,False 时核心实现的方法是利用Cpython 来实现的。</p><p>verbose：int, default=0，冗长模式（不太懂是啥意思，反正一般不去改默认值）。</p><p>random_state：int, RandomState instance, default=None，确定质心初始化的随机数生成。使用整数使随机性具有确定性。</p><p>copy_x：bool, default=True， 对是否修改数据的一个标记，如果True，即复制了就不会修改数据。bool 在scikit-learn 很多接口中都会有这个参数的，就是是否对输入数据继续copy 操作，以便不修改用户的输入数据。这个要理解Python 的内存机制才会比较清楚。</p><p>n_job：sint, default=None，并行设置。</p><p>algorithm：{‘auto’, ‘full’, ‘elkan’}, default=’auto’，kmeans的实现算法，经典的EM风格算法是’full’的。通过使用三角形不等式，’elkan’变异对于定义良好的聚类的数据更有效。但是，由于分配了额外的形状数组（n_samples，n_clusters），因此需要更多的内存。目前，’auto’（保持向后兼容性）选择’elkan’，但为了更好的启发式，将来可能会更改。</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入可视化工具包</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">X=load_iris().data</span><br><span class="line">clf = KMeans(n_clusters=<span class="number">3</span>,random_state=<span class="number">0</span>)</span><br><span class="line">clf.fit(X)</span><br><span class="line">label = clf.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 颜色和标签列表</span></span><br><span class="line">colors_list = [<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>]</span><br><span class="line">labels_list = [<span class="string">'1'</span>,<span class="string">'2'</span>,<span class="string">'3'</span>]</span><br><span class="line">x=X</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    plt.scatter(x[label==i,<span class="number">0</span>], x[label== i,<span class="number">1</span>], s=<span class="number">100</span>,c=colors_list[i],label=labels_list[i])</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 聚类中心点</span></span><br><span class="line">plt.scatter(clf.cluster_centers_[:,<span class="number">0</span>],clf.cluster_centers_[:,<span class="number">1</span>], s=<span class="number">300</span>,c=<span class="string">'black'</span>,label=<span class="string">'Centroids'</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'Annual Income (k$)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Spending Score (1-100)'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/17/K-means%E7%AE%97%E6%B3%95/8.png" alt="img"> </p><h1 id="6-源码仓库地址"><a href="#6-源码仓库地址" class="headerlink" title="6. 源码仓库地址"></a>6. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-K-means算法简介&quot;&gt;&lt;a href=&quot;#1-K-means算法简介&quot; class=&quot;headerlink&quot; title=&quot;1. K-means算法简介&quot;&gt;&lt;/a&gt;1. K-means算法简介&lt;/h1&gt;&lt;p&gt;K-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>KNN算法</title>
    <link href="http://yoursite.com/2023/06/16/KNN%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/16/KNN%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-16T02:02:00.000Z</published>
    <updated>2023-06-16T02:15:23.653Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-什么是KNN算法"><a href="#1-什么是KNN算法" class="headerlink" title="1. 什么是KNN算法"></a>1. 什么是KNN算法</h1><p>KNN（K Nearest Neighbors，又称k近邻法）是一种基本的分类和回归方法，是监督学习方法里的一种常用方法。</p><p>KNN算法通过距离判断两个样本是否相似，使用与未知样本最近的k个样本（近邻）的类别来分类，数量最多的标签类别就是新样本的标签类别。<a id="more"></a></p><h1 id="2-KNN算法原理"><a href="#2-KNN算法原理" class="headerlink" title="2. KNN算法原理"></a>2. KNN算法原理</h1><p>KNN算法三要素：距离度量、k值的选择和分类决策规则。常用的距离度量是欧氏距离及更一般的pL距离。k值小时，k近邻模型更复杂，容易发生过拟合；k值大时，k近邻模型更简单，又容易欠拟合。因此k值得选择会对分类结果产生重大影响。k值的选择反映了对近似误差与估计误差之间的权衡，通常由交叉验证选择最优的k。分类决策规则往往是多数表决，即由输入实例的k个邻近输入实例中的多数类决定输入实例的类。</p><h2 id="2-1-距离度量"><a href="#2-1-距离度量" class="headerlink" title="2.1 距离度量"></a>2.1 距离度量</h2><p><img src="/2023/06/16/KNN%E7%AE%97%E6%B3%95/1.png" alt="img"> </p><blockquote><p>一般采用二维欧氏距离。</p></blockquote><h2 id="2-2-交叉验证选取k值"><a href="#2-2-交叉验证选取k值" class="headerlink" title="2.2 交叉验证选取k值"></a>2.2 交叉验证选取k值</h2><p>在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。交叉验证的基本思想是重复地使用数据，把给定的数据进行切分，将切分的数据组合为训练集与测试集，在此基础上反复进行训练测试以及模型的选择。</p><h2 id="2-3-分类决策规则"><a href="#2-3-分类决策规则" class="headerlink" title="2.3 分类决策规则"></a>2.3 分类决策规则</h2><p>KNN使用的分类决策规则是多数表决，如果损失函数为0-1损失函数，那么要使误分类率最小即使经验风险最小，多数表决规则实际上就等同于经验风险最小化。</p><h1 id="3-KNN算法特点及应用场景"><a href="#3-KNN算法特点及应用场景" class="headerlink" title="3. KNN算法特点及应用场景"></a>3. KNN算法特点及应用场景</h1><p>KNN是一种非参的，惰性的算法模型。什么是非参，什么是惰性呢？</p><p>非参的意思并不是说这个算法不需要参数，而是意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。</p><p>惰性又是什么意思呢？举例说，同样是分类算法，逻辑回归需要先对数据进行大量训练，最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。</p><p><strong><em>KNN算法优点：</em></strong><br>（1） 简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。<br>（2）模型训练时间快。<br>（3）预测效果好。<br>（4）对异常值不敏感。</p><p><strong><em>KNN算法缺点：</em></strong><br>（1）对内存要求较高，因为该算法存储了所有训练数据。<br>（2）预测阶段可能很慢。<br>（3）对不相关的功能和数据规模敏感。</p><p>那么什么时候应该选择使用KNN算法呢？一般来说，当需要使用分类算法，且数据比较大的时候就可以尝试使用KNN算法进行分类了。</p><h1 id="4-KNN算法的Python应用"><a href="#4-KNN算法的Python应用" class="headerlink" title="4. KNN算法的Python应用"></a>4. KNN算法的Python应用</h1><h2 id="4-1-KNN算法的Python实现"><a href="#4-1-KNN算法的Python实现" class="headerlink" title="4.1 KNN算法的Python实现"></a>4.1 KNN算法的Python实现</h2><p>如下代码所示，通过提供训练集，包含A和B两类数据，预测测试数据属于哪一类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainData - 训练集，testData - 测试集，labels - 分类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knn</span><span class="params">(trainData, testData, labels, k)</span>:</span></span><br><span class="line">   <span class="comment"># 计算训练样本的行数</span></span><br><span class="line">   rowSize = trainData.shape[<span class="number">0</span>]</span><br><span class="line">   <span class="comment"># 计算训练样本和测试样本的差值</span></span><br><span class="line">   diff = np.tile(testData, (rowSize, <span class="number">1</span>)) - trainData</span><br><span class="line">   <span class="comment"># 计算差值的平方和</span></span><br><span class="line">   sqrDiff = diff ** <span class="number">2</span></span><br><span class="line">   sqrDiffSum = sqrDiff.sum(axis=<span class="number">1</span>)</span><br><span class="line">   <span class="comment"># 计算距离</span></span><br><span class="line">   distances = sqrDiffSum ** <span class="number">0.5</span></span><br><span class="line">   <span class="comment"># 对所得的距离从低到高进行排序</span></span><br><span class="line">   sortDistance = distances.argsort()</span><br><span class="line">    </span><br><span class="line">   count = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">       vote = labels[sortDistance[i]]</span><br><span class="line">       count[vote] = count.get(vote, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">   <span class="comment"># 对类别出现的频数从高到低进行排序</span></span><br><span class="line">   sortCount = sorted(count.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">   <span class="comment"># 返回出现频数最高的类别</span></span><br><span class="line">   <span class="keyword">return</span> sortCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">trainData = np.array([[<span class="number">5</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">4</span>]])</span><br><span class="line">labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">testData = [<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">X = knn(trainData, testData, labels, <span class="number">3</span>)</span><br><span class="line">print(X)</span><br></pre></td></tr></table></figure><p> 很明显，测试数据属于A类，实际预测结果也为A类。</p><h2 id="4-2-sklearn-neighbors-KNeighborsClassifier-k近邻分类器"><a href="#4-2-sklearn-neighbors-KNeighborsClassifier-k近邻分类器" class="headerlink" title="4.2 sklearn.neighbors.KNeighborsClassifier(k近邻分类器)"></a>4.2 sklearn.neighbors.KNeighborsClassifier(k近邻分类器)</h2><p>sklearn.neighbors.KNeighborsClassifier()函数是用于实现k近邻投票算法的分类器。其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>,weights=’uniform’, algorithm=’auto’, leaf_size=<span class="number">30</span>, p=<span class="number">2</span>, metric=’minkowski’, metric_params=<span class="literal">None</span>, n_jobs=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：</p><ul><li>n_neighbors：int，optional(default = 5)</li></ul><p>默认情况下kneighbors查询使用的邻居数。就是k-NN的k的值，选取最近的k个点。</p><ul><li>weights：str或callable，可选(默认=‘uniform’)</li></ul><p>默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，即所有的邻近点的权重都是相等的。distance是不均等的权重，距离近的点比距离远的点的影响大。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。</p><ul><li>algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选</li></ul><p>快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。ball_tree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。</p><ul><li>leaf_size：int，optional(默认值=30)</li></ul><p>默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。</p><ul><li>p：整数，可选(默认=2)</li></ul><p>距离度量公式，默认使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。</p><ul><li>metric：字符串或可调用，默认为’minkowski’</li></ul><p>用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。</p><ul><li>metric_params：dict，optional(默认=None)</li></ul><p>距离公式的其他关键参数，使用默认的None即可。</p><ul><li>n_jobs：int或None，可选(默认=None)</li></ul><p>并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。</p></blockquote><p><strong>注意：如果发现两个邻居，邻居k+1和k具有相同距离但不同标签，则结果将取决于训练数据的排序。</strong></p><p> 测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">X = [[<span class="number">5</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">4</span>]]</span><br><span class="line">y = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line"></span><br><span class="line">neigh = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">neigh.fit(X, y) </span><br><span class="line">print(neigh.predict([[<span class="number">3</span>, <span class="number">2</span>]]))</span><br></pre></td></tr></table></figure><p>预测结果同样为A类。</p><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-什么是KNN算法&quot;&gt;&lt;a href=&quot;#1-什么是KNN算法&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是KNN算法&quot;&gt;&lt;/a&gt;1. 什么是KNN算法&lt;/h1&gt;&lt;p&gt;KNN（K Nearest Neighbors，又称k近邻法）是一种基本的分类和回归方法，是监督学习方法里的一种常用方法。&lt;/p&gt;
&lt;p&gt;KNN算法通过距离判断两个样本是否相似，使用与未知样本最近的k个样本（近邻）的类别来分类，数量最多的标签类别就是新样本的标签类别。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>距离和相似性度量</title>
    <link href="http://yoursite.com/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/"/>
    <id>http://yoursite.com/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/</id>
    <published>2023-06-15T02:49:49.000Z</published>
    <updated>2023-06-15T03:13:09.008Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习中，经常需要比较个体间差异的大小，进而评价个体的相似性和类别。最常见的是数据分析中的相关分析，数据挖掘中的分类和聚类算法，如K最近邻（KNN）和K均值（K-Means）等。其中，衡量个体间差异的方法，主要分为距离度量和相似性度量。</p><p>为了方便下面的解释和举例，先设定我们要比较X个体和Y个体间的差异，它们都包含了N个维的特征，即X=（x1, x2, x3, … xn），Y=（y1, y2, y3, … yn）。</p><a id="more"></a><h1 id="1-距离度量"><a href="#1-距离度量" class="headerlink" title="1. 距离度量"></a>1. 距离度量</h1><p>距离度量（Distance）用于衡量个体在空间上存在的距离，距离越远说明个体间的差异越大。对于任意一个定义在两个矢量x和y上的函数d(x,y)，只要满足如下4个性质就可以称作“距离度量”。<br>（1）非负性：d(x,y)≥0<br>（2）对称性：d(x,y)=d(y,x)<br>（3）自反性：d(x,y)=0，当且仅当x=y<br>（4）三角不等式：d(x,y)≤d(x,z)+d(y,z)</p><h2 id="1-1-欧几里得距离-Euclidean-Distance"><a href="#1-1-欧几里得距离-Euclidean-Distance" class="headerlink" title="1.1 欧几里得距离(Euclidean Distance)"></a>1.1 欧几里得距离(Euclidean Distance)</h2><p>欧氏距离是最常见的距离度量，衡量的是多维空间中各个点之间的绝对距离。公式如下：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/1.png" alt="img"> </p><p>因为计算是基于各维度特征的绝对数值，所以欧氏度量需要保证各维度指标在相同的刻度级别，比如对身高（cm）和体重（kg）两个单位不同的指标使用欧式距离可能使结果失效。</p><h2 id="1-2-明可夫斯基距离-Minkowski-Distance"><a href="#1-2-明可夫斯基距离-Minkowski-Distance" class="headerlink" title="1.2 明可夫斯基距离(Minkowski Distance)"></a>1.2 明可夫斯基距离(Minkowski Distance)</h2><p>明氏距离是欧氏距离的推广，是对多个距离度量公式的概括性的表述。公式如下：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/2.png" alt="img"> </p><p>这里的p值是一个变量，当p=2时就得到了上节的欧氏距离。</p><h2 id="1-3-曼哈顿距离-Manhattan-Distance"><a href="#1-3-曼哈顿距离-Manhattan-Distance" class="headerlink" title="1.3 曼哈顿距离(Manhattan Distance)"></a>1.3 曼哈顿距离(Manhattan Distance)</h2><p>曼哈顿距离来源于城市区块距离，是将多个维度上的距离进行求和后的结果，即当上节的明氏距离中p=1时得到的距离度量公式，如下所示：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/3.png" alt="img"> </p><h2 id="1-4-切比雪夫距离-Chebyshev-Distance"><a href="#1-4-切比雪夫距离-Chebyshev-Distance" class="headerlink" title="1.4 切比雪夫距离(Chebyshev Distance)"></a>1.4 切比雪夫距离(Chebyshev Distance)</h2><p>切比雪夫距离起源于国际象棋中国王的走法，我们知道国际象棋国王每次只能往周围的8格中走一步，那么如果要从棋盘中A格(x1, y1)走到B格(x2, y2)最少需要走几步？扩展到多维空间，其实切比雪夫距离就是当p趋向于无穷大时的明氏距离：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/4.png" alt="img"> </p><blockquote><p> P.S. 其实上面的曼哈顿距离、欧氏距离和切比雪夫距离都是明可夫斯基距离在特殊条件下的应用。</p></blockquote><h2 id="1-5-马哈拉诺比斯距离-Mahalanobis-Distance"><a href="#1-5-马哈拉诺比斯距离-Mahalanobis-Distance" class="headerlink" title="1.5 马哈拉诺比斯距离(Mahalanobis Distance)"></a>1.5 马哈拉诺比斯距离(Mahalanobis Distance)</h2><p>既然欧几里得距离无法忽略指标度量的差异，所以在使用欧氏距离之前需要对底层指标进行数据的标准化，而基于各指标维度进行标准化后再使用欧氏距离就衍生出来另外一个距离度量——马哈拉诺比斯距离（Mahalanobis Distance），简称马氏距离。</p><h1 id="2-相似性度量"><a href="#2-相似性度量" class="headerlink" title="2. 相似性度量"></a>2. 相似性度量</h1><p>相似性度量（Similarity），即计算个体间的相似程度，与距离度量相反，相似性度量的值越小，说明个体间相似度越小，差异越大。</p><h2 id="2-1-向量空间余弦相似度-Cosine-Similarity"><a href="#2-1-向量空间余弦相似度-Cosine-Similarity" class="headerlink" title="2.1 向量空间余弦相似度(Cosine Similarity)"></a>2.1 向量空间余弦相似度(Cosine Similarity)</h2><p>余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。公式如下：<img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/6.png" alt="6"></p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/5.png" alt="img"> </p><h2 id="2-2-皮尔森相关系数-Pearson-Correlation-Coefficient"><a href="#2-2-皮尔森相关系数-Pearson-Correlation-Coefficient" class="headerlink" title="2.2 皮尔森相关系数(Pearson Correlation Coefficient)"></a>2.2 皮尔森相关系数(Pearson Correlation Coefficient)</h2><p>即相关分析中的相关系数r，分别对X和Y基于自身总体标准化后计算空间向量的余弦夹角。公式如下：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/6.png" alt="img"> </p><h2 id="2-3-Jaccard相似系数-Jaccard-Coefficient"><a href="#2-3-Jaccard相似系数-Jaccard-Coefficient" class="headerlink" title="2.3 Jaccard相似系数(Jaccard Coefficient)"></a>2.3 Jaccard相似系数(Jaccard Coefficient)</h2><p>Jaccard系数主要用于计算符号度量或布尔值度量的个体间的相似度，因为个体的特征属性都是由符号度量或者布尔值标识，因此无法衡量差异具体值的大小，只能获得“是否相同”这个结果，所以Jaccard系数只关心个体间共同具有的特征是否一致这个问题。如果比较X与Y的Jaccard相似系数，只比较xn和yn中相同的个数，公式如下：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/7.png" alt="img"> </p><h2 id="2-4-调整余弦相似度-Adjusted-Cosine-Similarity"><a href="#2-4-调整余弦相似度-Adjusted-Cosine-Similarity" class="headerlink" title="2.4 调整余弦相似度(Adjusted Cosine Similarity)"></a>2.4 调整余弦相似度(Adjusted Cosine Similarity)</h2><p>虽然余弦相似度对个体间存在的偏见可以进行一定的修正，但是因为只能分辨个体在维之间的差异，没法衡量每个维数值的差异，会导致这样一个情况：比如用户对内容评分，5分制，X和Y两个用户对两个内容的评分分别为(1,2)和(4,5)，使用余弦相似度得出的结果是0.98，两者极为相似，但从评分上看X似乎不喜欢这2个内容，而Y比较喜欢，余弦相似度对数值的不敏感导致了结果的误差，需要修正这种不合理性，就出现了调整余弦相似度，即所有维度上的数值都减去一个均值，比如X和Y的评分均值都是3，那么调整后为(-2,-1)和(1,2)，再用余弦相似度计算，得到-0.8，相似度为负值并且差异不小，但显然更加符合现实。</p><h1 id="3-欧氏距离与余弦相似度的比较"><a href="#3-欧氏距离与余弦相似度的比较" class="headerlink" title="3. 欧氏距离与余弦相似度的比较"></a>3. 欧氏距离与余弦相似度的比较</h1><p>欧氏距离是最常见的距离度量，而余弦相似度则是最常见的相似度度量，很多的距离度量和相似度度量都是基于这两者的变形和衍生，所以下面重点比较下两者在衡量个体差异时实现方式和应用环境上的区别。</p><p>借助三维坐标系来看下欧氏距离和余弦相似度的区别：</p><p><img src="/2023/06/15/%E8%B7%9D%E7%A6%BB%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F/8.png" alt="img"> </p><p>从图上可以看出距离度量衡量的是空间各点间的绝对距离，跟各个点所在的位置坐标（即个体特征维度的数值）直接相关；而余弦相似度衡量的是空间向量的夹角，更加的是体现在方向上的差异，而不是位置。如果保持A点的位置不变，B点朝原方向远离坐标轴原点，那么这个时候余弦相似度cosθ是保持不变的，因为夹角不变，而A、B两点的距离显然在发生改变，这就是欧氏距离和余弦相似度的不同之处。</p><p> 根据欧氏距离和余弦相似度各自的计算方式和衡量特征，分别适用于不同的数据分析模型：</p><ul><li><p>欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异；</p></li><li><p>余弦相似度更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分用户兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦相似度对绝对数值不敏感）。</p></li></ul><h1 id="4-Python计算欧氏距离与余弦相似度"><a href="#4-Python计算欧氏距离与余弦相似度" class="headerlink" title="4. Python计算欧氏距离与余弦相似度"></a>4. Python计算欧氏距离与余弦相似度</h1><h2 id="4-1-Python计算欧氏距离"><a href="#4-1-Python计算欧氏距离" class="headerlink" title="4.1 Python计算欧氏距离"></a>4.1 Python计算欧氏距离</h2><p>以下代码提供了3种计算欧氏距离的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x=np.random.random(<span class="number">10</span>)</span><br><span class="line">y=np.random.random(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法一：根据公式求解</span></span><br><span class="line">d1=np.sqrt(np.sum(np.square(x-y)))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 方法二：根据scipy库求解</span></span><br><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> pdist</span><br><span class="line">X=np.vstack([x,y])  <span class="comment"># 将x，y两个一维数组合并成一个2D数组：[[x1,x2,x3...],[y1,y2,y3...]]</span></span><br><span class="line">d2=pdist(X) <span class="comment"># d2=np.sqrt(（x1-y1)+(x2-y2)+....)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法三：使用numpy.linalg.norm(x, y)可以计算向量 x 和向量 y 的欧氏距离</span></span><br><span class="line">d3=np.linalg.norm(x-y)</span><br><span class="line"></span><br><span class="line">print(d1, d2, d3)</span><br></pre></td></tr></table></figure><p> 以上3种方法得到的结果一致。</p><h2 id="4-2-Python计算余弦相似度"><a href="#4-2-Python计算余弦相似度" class="headerlink" title="4.2 Python计算余弦相似度"></a>4.2 Python计算余弦相似度</h2><p>以下代码提供了3种计算余弦相似度的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x=np.random.random(<span class="number">10</span>)</span><br><span class="line">y=np.random.random(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法一：scipy中的scipy.spatial.distance.cosine函数可计算余弦距离。</span></span><br><span class="line"><span class="comment"># 因此，我们可以用1减去余弦距离得到余弦相似度。</span></span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> spatial</span><br><span class="line">res1 = <span class="number">1</span> - spatial.distance.cosine(x, y)</span><br><span class="line">print(res1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二：numpy中的numpy.dot函数可以两个向量的点积，numpy.linalg.norm函数可以计算向量的欧氏距离。</span></span><br><span class="line"><span class="comment"># 因此，可以通过公式和这两个函数计算向量的余弦相似度。</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> dot</span><br><span class="line"><span class="keyword">from</span> numpy.linalg <span class="keyword">import</span> norm</span><br><span class="line">res2 = dot(x, y) / (norm(x) * norm(y))</span><br><span class="line">print(res2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法三：sklearn中的sklearn.metrics.pairwise.cosine_similarity函数可直接计算出两个向量的余弦相似度。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">res3 = cosine_similarity(x.reshape(<span class="number">1</span>, <span class="number">-1</span>), y.reshape(<span class="number">1</span>, <span class="number">-1</span>)) <span class="comment"># reshape(1, -1)将矩阵转化成1行</span></span><br><span class="line">print(res3)</span><br></pre></td></tr></table></figure><p>以上3种方法得到的结果一致。</p><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在机器学习中，经常需要比较个体间差异的大小，进而评价个体的相似性和类别。最常见的是数据分析中的相关分析，数据挖掘中的分类和聚类算法，如K最近邻（KNN）和K均值（K-Means）等。其中，衡量个体间差异的方法，主要分为距离度量和相似性度量。&lt;/p&gt;
&lt;p&gt;为了方便下面的解释和举例，先设定我们要比较X个体和Y个体间的差异，它们都包含了N个维的特征，即X=（x1, x2, x3, … xn），Y=（y1, y2, y3, … yn）。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PCA算法</title>
    <link href="http://yoursite.com/2023/06/14/PCA%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/14/PCA%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-14T02:21:49.000Z</published>
    <updated>2023-06-14T03:09:07.247Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-数据降维"><a href="#1-数据降维" class="headerlink" title="1. 数据降维"></a>1. 数据降维</h1><p>在许多领域的研究与应用中，通常需要对含有多个变量的数据进行观测，收集大量数据后进行分析寻找规律。多变量大数据集无疑会为研究和应用提供丰富的信息，但是也在一定程度上增加了数据采集的工作量。更重要的是在很多情形下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性。如果分别对每个指标进行分析，分析往往是孤立的，不能完全利用数据中的信息，因此盲目减少指标会损失很多有用的信息，从而产生错误的结论。<a id="more"></a></p><p>因此需要找到一种合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。主成分分析与因子分析就属于这类降维算法。</p><p>降维就是一种对高维度特征数据预处理方法。降维是将高维度的数据保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的。在实际的生产和应用中，降维在一定的信息损失范围内，可以为我们节省大量的时间和成本。降维也成为应用非常广泛的数据预处理方法。</p><p>降维具有如下一些优点：<br>（1）使得数据集更易使用。<br>（2）降低算法的计算开销。<br>（3）去除噪声。<br>（4）使得结果容易理解。</p><p>降维的算法有很多，比如<strong>奇异值分解(SVD)、主成分分析(PCA)、因子分析(FA)、独立成分分析(ICA)</strong>。</p><h1 id="2-PCA原理"><a href="#2-PCA原理" class="headerlink" title="2. PCA原理"></a>2. PCA原理</h1><p>PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。</p><p> 那么我们如何得到这些包含最大差异性的主成分方向呢？事实上，通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。</p><p>由于得到协方差矩阵的特征值特征向量有两种方法：<strong>特征值分解协方差矩阵、奇异值分解协方差矩阵</strong>，所以PCA算法有两种实现方法：<strong>基于特征值分解协方差矩阵实现PCA算法、基于SVD分解协方差矩阵实现PCA算法</strong>。</p><p>下面具体介绍基于特征值分解协方差矩阵实现PCA算法的原理。</p><h2 id="2-1-基变换"><a href="#2-1-基变换" class="headerlink" title="2.1 基变换"></a>2.1 基变换</h2><p>一般来说，欲获得原始数据新的表示空间，最简单的是对原始数据进行线性变换（基变换）：Y=PX，其中Y是样本在新空间的表达，P是基向量，X是原始样本。我们可知选择不同的基可以对一组数据给出不同的表示，同时当基的数量少于原始样本本身的维数则可达到降维的效果，矩阵表示如下：</p><p><img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/1.png" alt="img"> </p><p>其中，p<sub>i</sub>是一个行向量，表示第i个基；a<sub>j</sub>是一个列向量，表示第j个原始数据记录。特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一个N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。从原本X∈R<sup>N*M</sup>降维到Y∈R<sup>R*M</sup>。因此这种矩阵相乘的表示也可以表示降维变换。</p><p>最后，上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。</p><h2 id="2-2-方差"><a href="#2-2-方差" class="headerlink" title="2.2 方差"></a>2.2 方差</h2><p>那么如何选择一个方向或者基才是最优的？观察下图,</p><p><img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/2.png" alt="img"> </p><p>我们将所有的点分别向两条直线做投影，基于PCA最大可分思想，我们要找的方向是降维后损失最小，可以理解为投影后的数据尽可能的分开，那么这种分散程度可以用数学上的方差来表示，方差越大数据越分散。方差公式如下：</p><script type="math/tex; mode=display">Var(a)=\frac{1}{m}\sum_{i=1}^n(a_i-μ)^2</script><p>对数据进行了中心化后（可以方便后面的操作）：</p><script type="math/tex; mode=display">Var(a)=\frac{1}{m}\sum_{i=1}^m(a_i)^2</script><h2 id="2-3-协方差"><a href="#2-3-协方差" class="headerlink" title="2.3 协方差"></a>2.3 协方差</h2><p>从二维降到一维可以使用方差最大来选出能使基变换后数据分散最大的方向（基），但如果遇到高维的变换，当完成第一个方向（基）选择后，第二个投影方向应该与第一个“几乎重合在一起”，这显然是没有用的，因此要有其它的约束条件。我们希望两个字段尽可能表示更多的信息，使其不存在相关性。</p><p>数学上用协方差表示其相关性：</p><script type="math/tex; mode=display">Cov(a,b)=\frac{1}{m}\sum_{i=1}^ma_ib_i</script><p>当Cov(a,b)=0时，表示两个字段完全独立，这也是我们的优化目标。</p><h2 id="2-4-协方差矩阵"><a href="#2-4-协方差矩阵" class="headerlink" title="2.4 协方差矩阵"></a>2.4 协方差矩阵</h2><p>我们想达到的目标与字段内方差及字段间协方差有密切关系，假如只有 、  两个字段，那么我们将它们按行组成矩阵X,表示如下：</p><script type="math/tex; mode=display">X=\left( \begin{matrix} a_1 & a_2 & ... & a_m \\ b_1 & b_2 & ... & b_m \end{matrix} \right)</script><p>然后我们用X乘以X的转置，并乘上系数1/m:</p><script type="math/tex; mode=display">\frac{1}mXX^T=\left( \begin{matrix} \frac{1}{m}\sum_{i=1}^ma_i^2 & \frac{1}{m}\sum_{i=1}^ma_ib_i \\ \frac{1}{m}\sum_{i=1}^ma_ib_i & \frac{1}{m}\sum_{i=1}^mb_i^2 \end{matrix} \right)</script><p>可见，协方差矩阵是一个对称的矩阵，而且对角线是各个维度的方差，而其它元素是a和b的协方差，即两者被统一到了一个矩阵。</p><h2 id="2-5-协方差矩阵对角化"><a href="#2-5-协方差矩阵对角化" class="headerlink" title="2.5 协方差矩阵对角化"></a>2.5 协方差矩阵对角化</h2><p>我们的目标是使<img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/3.png" alt="img">，根据上述推倒，可以看出我们的优化目标  <img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/4.png" alt="img">等价于协方差矩阵对角化。即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：</p><p>设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：</p><p><img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/5.png" alt="img"> </p><p>可见，我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足PCPT是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。</p><p> 我们希望的是投影后的方差最大化，于是我们的优化目标可以写为：</p><script type="math/tex; mode=display">max_p tr(PCP^T)</script><script type="math/tex; mode=display">s.t. PP^T = I</script><p>利用拉格朗日函数可以得到：J(P)=tr(PCP<sup>T</sup>)+ λ(PP<sup>T</sup>-I),对P求导有CP<sup>T</sup>+λP<sup>T</sup>=0，整理下即为：CP<sup>T</sup>=(-λ)P<sup>T</sup>。</p><p>于是，只需对协方差矩阵C进行特征分解，对求得的特征值进行排序，再对P<sup>T</sup>=(P<sub>1</sub>,P<sub>2</sub>,…,P<sub>R</sub>)取前K列组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p><h1 id="3-PCA算法流程"><a href="#3-PCA算法流程" class="headerlink" title="3. PCA算法流程"></a>3. PCA算法流程</h1><p>从上面可以看出，求样本 x<sub>i </sub>的 n’ 维的主成分其实就是求样本集的协方差矩阵的前 n’ 个特征值对应特征向量矩阵 P，然后对于每个样本x<sub>i </sub>，做如下变换y<sub>i </sub>=Px<sub>i </sub>，即达到降维的PCA目的。</p><p>下面是具体的算法流程：<br>输入：n维样本集X=(x<sub>1 </sub>,x<sub>2</sub>,…,x<sub>m</sub>)，要降维到的维数 n’；<br>输出：降维后的样本集Y。</p><p>1.对所有的样本进行中心化；<br>2.计算样本的协方差矩阵；<br>3.求出协方差矩阵的特征值及对应的特征向量；<br>4.将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P；<br>5.Y=PX即为降维到k维后的数据。</p><blockquote><p>注意：有时候，我们不指定降维后的n’的值，而是换种方式，指定一个降维到的主成分比重阈值t。这个阈值t在(0,1]之间。假如我们的n个特征值为λ1≥λ2≥…≥λn,则n’可以通过下式得到:<img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/6.png" alt="img"> </p></blockquote><h1 id="4-PCA算法的特点"><a href="#4-PCA算法的特点" class="headerlink" title="4. PCA算法的特点"></a>4. PCA算法的特点</h1><p>作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如为解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。</p><p>PCA算法的主要优点有：<br>（1） 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　<br>（2）各主成分之间正交，可消除原始数据成分间的相互影响的因素。<br>（3）计算方法简单，主要运算是特征值分解，易于实现。</p><p>PCA算法的主要缺点有：<br>（1）主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。<br>（2）方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。</p><h1 id="5-PCA算法的Python应用"><a href="#5-PCA算法的Python应用" class="headerlink" title="5. PCA算法的Python应用"></a>5. PCA算法的Python应用</h1><p>首先需要实现几个函数，分别是数据中心化、最小化降维造成的损失，确定k、得到最大的k个特征值和特征向量、得到降维后的数据、重构数据，然后通过PCA函数整合，最后调用main函数执行。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据中心化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Z_centered</span><span class="params">(dataMat)</span>:</span></span><br><span class="line">    rows, cols = dataMat.shape</span><br><span class="line">    meanVal = np.mean(dataMat, axis=<span class="number">0</span>)  <span class="comment"># 按列求均值，即求各个特征的均值</span></span><br><span class="line">    meanVal = np.tile(meanVal, (rows, <span class="number">1</span>))</span><br><span class="line">    newdata = dataMat - meanVal</span><br><span class="line">    <span class="keyword">return</span> newdata, meanVal</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小化降维造成的损失，确定k</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Percentage2n</span><span class="params">(eigVals, percentage)</span>:</span></span><br><span class="line">    sortArray = np.sort(eigVals)  <span class="comment"># 升序</span></span><br><span class="line">    sortArray = sortArray[<span class="number">-1</span>::<span class="number">-1</span>]  <span class="comment"># 逆转，即降序</span></span><br><span class="line">    arraySum = sum(sortArray)</span><br><span class="line">    tmpSum = <span class="number">0</span></span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> sortArray:</span><br><span class="line">        tmpSum += i</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> tmpSum &gt;= arraySum * percentage:</span><br><span class="line">            <span class="keyword">return</span> num</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到最大的k个特征值和特征向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EigDV</span><span class="params">(covMat, p)</span>:</span></span><br><span class="line">    D, V = np.linalg.eig(covMat)  <span class="comment"># 得到特征值和特征向量</span></span><br><span class="line">    k = Percentage2n(D, p)  <span class="comment"># 确定k值</span></span><br><span class="line">    print(<span class="string">"保留"</span> + str(p*<span class="number">100</span>) + <span class="string">"%信息，降维后的特征个数："</span> + str(k) + <span class="string">"\n"</span>)</span><br><span class="line">    eigenvalue = np.argsort(D)</span><br><span class="line">    K_eigenValue = eigenvalue[<span class="number">-1</span>:-(k + <span class="number">1</span>):<span class="number">-1</span>]</span><br><span class="line">    K_eigenVector = V[:, K_eigenValue]</span><br><span class="line">    <span class="keyword">return</span> K_eigenValue, K_eigenVector</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到降维后的数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getlowDataMat</span><span class="params">(DataMat, K_eigenVector)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> DataMat * K_eigenVector</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重构数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Reconstruction</span><span class="params">(lowDataMat, K_eigenVector, meanVal)</span>:</span></span><br><span class="line">    reconDataMat = lowDataMat * K_eigenVector.T + meanVal</span><br><span class="line">    <span class="keyword">return</span> reconDataMat</span><br><span class="line"></span><br><span class="line"><span class="comment"># PCA算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PCA</span><span class="params">(data, p)</span>:</span></span><br><span class="line">    dataMat = np.float32(np.mat(data))</span><br><span class="line">    <span class="comment"># 数据中心化</span></span><br><span class="line">    dataMat, meanVal = Z_centered(dataMat)</span><br><span class="line">    <span class="comment"># 计算协方差矩阵</span></span><br><span class="line">    <span class="comment"># covMat = Cov(dataMat)</span></span><br><span class="line">    covMat = np.cov(dataMat, rowvar=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 得到最大的k个特征值和特征向量</span></span><br><span class="line">    D, V = EigDV(covMat, p)</span><br><span class="line">    <span class="comment"># 得到降维后的数据</span></span><br><span class="line">    lowDataMat = getlowDataMat(dataMat, V)</span><br><span class="line">    <span class="comment"># 重构数据</span></span><br><span class="line">    reconDataMat = Reconstruction(lowDataMat, V, meanVal)</span><br><span class="line">    <span class="keyword">return</span> reconDataMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    imagePath = <span class="string">'lenna.jpg'</span></span><br><span class="line">    image = cv.imread(imagePath)</span><br><span class="line">    image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)</span><br><span class="line">    rows, cols = image.shape</span><br><span class="line">    print(<span class="string">"降维前的特征个数："</span> + str(cols) + <span class="string">"\n"</span>)</span><br><span class="line">    print(image)</span><br><span class="line">    print(<span class="string">'----------------------------------------'</span>)</span><br><span class="line">    reconImage = PCA(image, <span class="number">0.99</span>) <span class="comment"># 通过改变保留信息的程度来看这个图片的特征值 </span></span><br><span class="line">    reconImage = reconImage.astype(np.uint8)</span><br><span class="line">    print(reconImage)</span><br><span class="line">    cv.imshow(<span class="string">'test'</span>, reconImage)</span><br><span class="line">    cv.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv.destroyAllWindows()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>保留不同信息量，降维后特征个数如下：</p><p><img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/7.png" alt="img"> </p><p>特征个数为52和95的降维后图片如下：</p><p><img src="/2023/06/14/PCA%E7%AE%97%E6%B3%95/8.png" alt="img"> </p><p>可以发现，降维后保留的特征越多，图片越清晰。</p><blockquote><p> P.S. 在Python的sklearn的库里面集成很多机器学习算法的库，其中也包括主成分分析的方法。</p></blockquote><h1 id="6-源码仓库地址"><a href="#6-源码仓库地址" class="headerlink" title="6. 源码仓库地址"></a>6. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-数据降维&quot;&gt;&lt;a href=&quot;#1-数据降维&quot; class=&quot;headerlink&quot; title=&quot;1. 数据降维&quot;&gt;&lt;/a&gt;1. 数据降维&lt;/h1&gt;&lt;p&gt;在许多领域的研究与应用中，通常需要对含有多个变量的数据进行观测，收集大量数据后进行分析寻找规律。多变量大数据集无疑会为研究和应用提供丰富的信息，但是也在一定程度上增加了数据采集的工作量。更重要的是在很多情形下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性。如果分别对每个指标进行分析，分析往往是孤立的，不能完全利用数据中的信息，因此盲目减少指标会损失很多有用的信息，从而产生错误的结论。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SIFT算法</title>
    <link href="http://yoursite.com/2023/06/13/SIFT%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/13/SIFT%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-13T01:31:11.000Z</published>
    <updated>2023-06-13T03:14:58.318Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-SIFT算法简介"><a href="#1-SIFT算法简介" class="headerlink" title="1. SIFT算法简介"></a>1. SIFT算法简介</h1><p>SIFT (Scale-invariant feature transform)，尺度不变特征转换，是一种图像局部特征提取算法，它通过在不同的尺度空间中寻找极值点（特征点，关键点）的精确定位和主方向，构建关键点描述符来提取特征。SIFT提取的关键点具有尺度不变性、旋转不变性，而且不会因光照、仿射变换和噪音等因素而干扰。SIFT所查找到的关键点是一些十分突出、不会因光照、仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。<a id="more"></a></p><h2 id="1-1-SIFT特征检测步骤"><a href="#1-1-SIFT特征检测步骤" class="headerlink" title="1.1 SIFT特征检测步骤"></a>1.1 SIFT特征检测步骤</h2><p>SIFT特征检测主要分为以下四步：<br>（1）尺度空间的极值检测：搜索所有尺度空间上的图像，通过高斯微分函数来识别潜在的对尺度和旋转不变的兴趣点；<br>（2）特征点定位：在每个候选的位置上，通过一个拟合精细模型来确定位置尺度，关键点的选取依据他们的稳定程度；<br>（3）特征方向赋值：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向，后续的所有操作都是对于关键点的方向、尺度和位置进行变换，从而提供这些特征的不变性；<br>（4）特征点描述：在每个特征点周围的邻域内，在选定的尺度上测量图像的局部梯度，这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变换。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/1.png" alt="img"> </p><h2 id="1-2-SIFT算法的特点"><a href="#1-2-SIFT算法的特点" class="headerlink" title="1.2 SIFT算法的特点"></a>1.2 SIFT算法的特点</h2><p>（1）图像的局部特征，对旋转、尺度缩放、亮度变化保持不变，对视角变化、仿射变换、噪声也保持一定程度的稳定性。<br>（2）独特性好，信息量丰富，适用于海量特征库进行快速、准确的匹配。<br>（3）多量性，即使是很少几个物体也可以产生大量的SIFT特征。<br>（4）高速性，经优化的SIFT匹配算法甚至可以达到实时性。<br>（5）可扩展性，可以很方便的与其他的特征向量进行联合。</p><h1 id="2-SIFT算法原理"><a href="#2-SIFT算法原理" class="headerlink" title="2. SIFT算法原理"></a>2. SIFT算法原理</h1><h2 id="2-1-尺度空间"><a href="#2-1-尺度空间" class="headerlink" title="2.1 尺度空间"></a>2.1 尺度空间</h2><p>在一定的范围内，无论物体是大还是小，人眼都可以分辨出来，然而计算机要有相同的能力却很难，在未知的场景中，计算机视觉并不能够提供物体的尺度大小，其中的一种方法是把物体不同尺度下的图像都提供给机器，让机器能够对物体在不同尺度下有一个统一的认知，在建立统一认知的过程中，要考虑的就是图像在不同的尺度下都存在的特点。</p><h3 id="2-1-1-多分辨率金字塔"><a href="#2-1-1-多分辨率金字塔" class="headerlink" title="2.1.1 多分辨率金字塔"></a>2.1.1 多分辨率金字塔</h3><p>早期的图像多尺度通常使用图像金字塔的表示形式，图像金字塔是同一图像在不同分辨率下得到的一组结果，生成过程包含：<br>（1）对原始图像进行平滑；<br>（2）对平滑后的图像进行下采样。</p><p>降采样后得到一系列不断尺寸缩小的图像。显然，一个传统的金字塔中，每一层的图像是其上一层图像长、高的各一半。多分辨率的图像金字塔虽然生成简单，但其本质是降采样，图像的局部特征则难以保持，也就是无法保持特征的尺度不变性。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/2.png" alt="img" style="zoom:80%;"> </p><h3 id="2-1-2-高斯金字塔"><a href="#2-1-2-高斯金字塔" class="headerlink" title="2.1.2 高斯金字塔"></a>2.1.2 高斯金字塔</h3><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/3.png" alt="img" style="zoom:80%;"> </p><p><strong><em>高斯金字塔的构建可以分为两步：</em></strong><br>（1）对图像做高斯平滑；<br>（2）对平滑后的图像做下采样。</p><p>为了让尺度体系其连续性，在简单下采样的基础上增加了高斯滤波，一副图像可以产生几组（octave）图像，一组图像包括几层（interval）图像。</p><p><strong><em>高斯金字塔分布情况（o组s层）：</em></strong></p><p>容易看出，高斯金字塔有多组，每组又有多层，一组的多个层之间的尺度是不一样的，也就是使用的高斯参数σ不同，相邻两层之间的尺度相差一个比例因子k，如果每组有S层，则k=21/S，上一组图像的最底层图像是由下一组中尺度为2σ的图像进行因子为2的降采样得到的（高斯金字塔是从底层开始建立的），高斯金字塔构建完成之后，将相邻的金字塔相减就得到了DoG金字塔。</p><p>高斯金字塔的组数：</p><script type="math/tex; mode=display">o=[log_2min(m,n)]-a</script><p>o表示高斯金字塔的层数，m，n分别是图像的行和列。减去的系数a可以在 0 – log<sub>2</sub>min(m,n) 之间的任意值，和具体需要的金字塔的顶层图像的大小有关。</p><p>高斯模糊参数可由下面的关系得到：</p><script type="math/tex; mode=display">σ(o,s)=σ_0*2^\frac{o+s}{S}</script><p>其中o为所在的组，s为所在的层，σ<sub>0</sub>为初始的尺度，S为每组的层数。</p><p><strong><em>同组内相邻层的图像尺度间的关系：</em></strong></p><script type="math/tex; mode=display">σ_{s+1}=k*σ_s=2^\frac{1}{S}*σ_s</script><p><strong><em>相邻组之间的尺度关系：</em></strong></p><script type="math/tex; mode=display">σ_{o+1}=2*σ_o</script><p>上一组图像的底层是由前一组图像的倒数第二层图像隔点采样生成的，这样可以保证尺度的连续性。</p><h3 id="2-1-3-高斯尺度空间（使用不同的参数）"><a href="#2-1-3-高斯尺度空间（使用不同的参数）" class="headerlink" title="2.1.3 高斯尺度空间（使用不同的参数）"></a>2.1.3 高斯尺度空间（使用不同的参数）</h3><p>我们要精确表示的物体都是通过一定的尺度来反映的，现实世界的物体也总是通过不同尺度的观察而得到不同的变化。</p><p>尺度空间的理论最早在1962年提出，主要思想是通过对原始图像进行尺度变换，获得图像多尺度下的尺度空间表示序列，对这些序列进行尺度空间主轮廓的提取，并以该主轮廓作为一种特征向量，实现边缘、角点检测和不同分辨率上的特征提取等。</p><p>尺度空间中各个尺度图像的模糊程度逐渐变大，能够模拟人在距离由近到远时目标在视网膜上的形成过程，尺度越大，图像越模糊。</p><p>图像和高斯函数进行卷积运算能够对图像进行模糊，且不同尺度的高斯核可以得到不同程度的模糊图像，一幅图像的高斯尺度空间可以通过图像和不同尺度的高斯核卷积得到：</p><script type="math/tex; mode=display">L(x,y,σ)=G(x,y,σ)*I(x,y)</script><p>其中，G是高斯函数：</p><script type="math/tex; mode=display">G(x,y,σ)=\frac{1}{2πσ^2}e^\frac{x^2+y^2}{2σ^2}</script><p>其中，σ是尺度空间因子，是高斯正态分布的标准差，反映了图像被模糊的程度，其值越大图像越模糊，对应的尺度也就越大，L(x,y,σ)对应高斯尺度空间。</p><p> T Lindeber在文献《Scale-space theory: a basic tool for analyzing structures at different scales》中证明，高斯核是唯一可以产生多尺度空间的核。</p><p><strong><em>高斯模糊性质：</em></strong><br>（1）高斯模糊具有圆对称性。<br>（2）高斯模糊具有线性可分的性质，可以在二维图像上对两个独立的一维空间分别进行计算，大大的减小了运算次数。<br>（3）对一副图像进行多次连续高斯模糊的效果与一次更大的高斯模糊可以产生同样的效果，大的高斯模型的半径是所用多个高斯模糊半径平方和的平方根。</p><p>例如：使用半径分别为6和8的两次高斯模糊变换得到的效果等同于一次半径为10的高斯模糊的效果：sqrt(6<sup>2</sup>+8<sup>2</sup>)=10，根据这个关系，使用多个连续较小的高斯模糊处理不会比单个高斯较大处理时间要少。</p><p><strong><em>构造尺度空间的目的：</em></strong></p><p>为了检测出来在不同尺度下都存在的特征点，而检测特征点较好的算子是高斯拉普拉斯（LoG），即Δ<sup>2</sup>G：</p><script type="math/tex; mode=display">Δ^2=\frac{ə^2}{əx^2}+\frac{ə^2}{əy^2}</script><p>LoG的缺点：虽然其能够较好的检测到图像中的特征点，但是运算量过大。通常可以使用DoG（差分高斯，Different of Gaussian）来近似计算LoG。设k为相邻两个高斯尺度空间的比例因子，则DoG定义为：</p><script type="math/tex; mode=display">D(x,y,σ)=[G(x,y,kσ)−G(x,y,σ)]∗I(x,y)=L(x,y,kσ)−L(x,y,σ)</script><p>其中，L(x,y,σ)是图像的高斯尺度空间。</p><p>DoG是如何得到的：将相邻的两个高斯空间的图像相减即可得到DoG响应图像，为了得到DoG响应图像，要先构建高斯尺度空间，而高斯的尺度空间可以在图像金字塔将采用的基础上加上高斯滤波得到，也就是对图像金字塔的每层图像使用不同的参数σ进行高斯模糊，使得每层金字塔有多张高斯模糊过的图像，降采样时，金字塔上边一组图像的第一章是由其下面一组图像倒数第三张降采样得到的。</p><h2 id="2-2-DoG空间极值检测（查找关键点）"><a href="#2-2-DoG空间极值检测（查找关键点）" class="headerlink" title="2.2 DoG空间极值检测（查找关键点）"></a>2.2 DoG空间极值检测（查找关键点）</h2><p>关键点是一些十分突出的点，不会因光照条件的改变而消失，比如角点、边缘点、暗区域的亮点和亮区域的暗点，既然两幅图像中有相同的景物，那么使用某种方法分别提取各自的稳定点，这些点之间会有相应的匹配点。</p><p>所谓关键点，就是在不同尺度空间的图像下检测出的具有方向信息的局部极值点。</p><p>为了寻找尺度空间的极值点，每个像素点要和其图像域（同一尺度空间）和尺度域（相邻的尺度空间）的所有相邻点进行比较，当其大于（或者小于）所有相邻点时，该点就是极值点。如图所示，中间的检测点要和其所在图像的3×3邻域8个像素点，以及其相邻的上下两层的3×3领域18个像素点，共26个像素点进行比较。</p><p>从上面的描述中可以知道，每组图像的第一层和最后一层是无法进行比较取得极值的。为了满足尺度变换的连续性，在每一组图像的顶层继续使用高斯模糊生成3幅图像，高斯金字塔每组有S+3层图像，DoG金字塔的每组有S+2组图像。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/4.png" alt="img"> </p><p>DoG在计算上只需相邻尺度高斯平滑后图像相减，因此简化了计算。</p><h2 id="2-3-删除不好的极值点（特征点）"><a href="#2-3-删除不好的极值点（特征点）" class="headerlink" title="2.3 删除不好的极值点（特征点）"></a>2.3 删除不好的极值点（特征点）</h2><p>通过比较检测得到的DoG的局部极值点实在离散的空间搜索得到的，由于离散空间是对连续空间采样得到的结果，因此在离散空间找到的极值点不一定是真正意义上的极值点，因此要设法将不满足条件的点剔除掉。可以通过尺度空间DoG函数进行曲线拟合寻找极值点，这一步的本质是去掉DoG局部曲率非常不对称的点。</p><p>要剔除掉的不符合要求的点主要有两种：<br>（1）低对比度的特征点<br>（2）不稳定的边缘响应点</p><h2 id="2-4-求取特征点的主方向"><a href="#2-4-求取特征点的主方向" class="headerlink" title="2.4 求取特征点的主方向"></a>2.4 求取特征点的主方向</h2><p>经过上面的步骤已经找到了在不同尺度下都存在的特征点，为了实现图像旋转不变性，需要给特征点的方向进行赋值。利用特征点邻域像素的梯度分布特性来确定其方向参数，再利用图像的梯度直方图求取关键点局部结构的稳定方向。</p><p>找到了特征点，也就可以得到该特征点的尺度σ，也就可以得到特征点所在的尺度图像：L(x,y,σ)=G(x,y,σ)∗I(x,y)。</p><p>计算以特征点为中心、以3×1.5σ * 3×1.5σ为半径的区域图像的幅角和幅值，每个点L(x,y)的梯度的模m(x,y)以及方向θ(x,y)可通过下面的公式求得：</p><script type="math/tex; mode=display">m(x,y)=\sqrt{[L(x+1,y)-L(x-1,y)]^2+[L(x,y+1)-L(x,y-1)]^2}</script><script type="math/tex; mode=display">θ(x,y)=arctan\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}</script><p>计算得到梯度方向后，就要使用直方图统计特征点邻域内像素对应的梯度方向和幅值。梯度方向的直方图的横轴是梯度方向的角度（梯度方向的范围是0到360度，直方图每36度一个柱共10个柱，或者没45度一个柱共8个柱），纵轴是梯度方向对应梯度幅值的累加，在直方图的峰值就是特征点的主方向。使用高斯函数对直方图进行平滑以增强特征点近的邻域点对关键点方向的作用，并减少突变的影响。</p><p>得到特征点的主方向后，对于每个特征点可以得到三个信息(x,y,σ,θ)，即位置、尺度和方向。由此可以确定一个SIFT特征区域，一个SIFT特征区域由三个值表示，中心表示特征点位置，半径表示关键点的尺度，箭头表示主方向。具有多个方向的关键点可以被复制成多份，然后将方向值分别赋给复制后的特征点，一个特征点就产生了多个坐标、尺度相等，但是方向不同的特征点。</p><h2 id="2-5-生成特征描述"><a href="#2-5-生成特征描述" class="headerlink" title="2.5 生成特征描述"></a>2.5 生成特征描述</h2><p>通过以上的步骤已经找到了SIFT特征点位置、尺度和方向信息，下面就需要使用一组向量来描述关键点也就是生成特征点描述子，这个描述符不只包含特征点，也含有特征点周围对其有贡献的像素点。描述子应具有较高的独立性，以保证匹配率。</p><p>特征描述符的生成大致有三个步骤：<br>（1）校正旋转主方向，确保旋转不变性。<br>（2）生成描述子，最终形成一个128维的特征向量。<br>（3）归一化处理，将特征向量长度进行归一化处理，进一步去除光照的影响。</p><p>为了保证特征矢量的旋转不变性，要以特征点为中心，在附近邻域内将坐标轴旋转θθ角度，即将坐标轴旋转为特征点的主方向，旋转后邻域内的像素的新坐标为：</p><script type="math/tex; mode=display">\left[ \begin{matrix} x^\text{'} \\ y^\text{'} \end{matrix} \right]=\left[ \begin{matrix} cosθ & -sinθ \\ sinθ & cosθ \end{matrix}\right]\left[ \begin{matrix} x \\ y \end{matrix} \right]</script><p>旋转之后的主方向为中心取8x8的窗口，左图中央为当前关键点的位置，每个小格代表Wie关键点邻域所在尺度空间的一个像素，求取每个像素的梯度幅值和方向，箭头方向代表梯度方向，长度代表梯度幅值，然后利用高斯窗口对其进行加权运算，最后在每个4x4的小块上绘制8个方向的梯度直方图，计算每个梯度方向的累加值，即可形成一个种子点，如右图所示，每个特征的由4个种子点组成，每个种子点有8个方向的向量信息，这种邻域方向性信息联合增强了算法的抗噪能力，同时对于含有定位误差的特征匹配也提供了比较理性的容错性。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/5.png" alt="img" style="zoom:50%;"> </p><p>不同于求主方向，此时每个种子区域的梯度直方图在0-360之间划分为8个方向区间，每个区间为45度，即每个种子点有8个方向的梯度强度信息。</p><p>在实际计算的过程中，为了增强匹配的稳健性，Lowe建立对每个关键点使用4x4共16个种子点来描述，这样一个关键点就会产生128维的SIFT特征向量。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/6.png" alt="img" style="zoom:80%;"> </p><p>通过对特征点周围的像素进行分块，计算块内梯度直方图，生成具有独特性的向量，这个向量是该区域图像信息的一种抽象，具有唯一性。</p><hr><p>综上，SIFT特征对旋转、尺度缩放、亮度等有鲁棒性，是一种非常稳定的局部特征，在图像处理和计算机视觉领域具有很重要的作用，其本身也比较复杂。</p><p>（1）DoG尺度空间的极值检测：</p><p>首先构造DoG尺度空间，在SIFT中使用不同参数的高斯模糊来表示不同的尺度空间，而构造尺度空间是为了检测在不同尺度下都存在的特征点，特征点的检测比较常用的方法是高斯拉普拉斯，但是LoG的运算量是比较大的，Marr和Hidreth指出可以使用DoG（高斯差分）来近似计算LoG，所以在DoG的尺度空间下检测极值点。</p><p>（2）删除不稳定的极值点：</p><p>低对比度的极值点+不稳定的边缘响应点</p><p>（3）确定特征的的主方向：</p><p>以特征点的为中心、以3×1.5σ * 3×1.5σ为半径的领域内计算各个像素点的梯度的幅角和幅值，然后使用直方图对梯度的幅角进行统计。直方图的横轴是梯度的方向，纵轴为梯度方向对应梯度幅值的累加值，直方图中最高峰所对应的方向即为特征点的方向。</p><p>（4）生成特征描述子：</p><p>首先将坐标轴旋转为特征点的方向，以特征点为中心的16x16的窗口的像素的梯度幅值和方向，将窗口内的像素分为16块，每块是其像素内8个方向的直方图统计，共可以形成128维的特征向量。</p><h1 id="3-SIFT算法在OpenCV中的应用"><a href="#3-SIFT算法在OpenCV中的应用" class="headerlink" title="3. SIFT算法在OpenCV中的应用"></a>3. SIFT算法在OpenCV中的应用</h1><p>SIFT算法在OpenCV中主要包括以下几个函数：</p><ol><li><p>cv2.xfeatures2d.SIFT_create()：实例化SIFT</p></li><li><p>sift.detect()：找出关键点</p></li><li><p>cv2.drawKeypoints()：画出关键点</p></li><li><p>sift.compute()：根据关键点计算SIFT向量</p></li></ol><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到特征点</span></span><br><span class="line">sift = cv2.xfeatures2d.SIFT_create()</span><br><span class="line">kp = sift.detect(gray, <span class="literal">None</span>)  <span class="comment"># 关键点</span></span><br><span class="line">img = cv2.drawKeypoints(gray, kp, img)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(gray, <span class="string">'gray'</span>), plt.title(<span class="string">'Gray Image'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(img, <span class="string">'gray'</span>), plt.title(<span class="string">'Keypoints Image'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算特征</span></span><br><span class="line"><span class="comment"># kp为关键点keypoints</span></span><br><span class="line"><span class="comment"># des为描述子descriptors</span></span><br><span class="line">kp, des = sift.compute(gray, kp)</span><br><span class="line">print(np.array(kp).shape) <span class="comment">#(203,)</span></span><br><span class="line">print(des.shape) <span class="comment">#(203, 128)，128维向量</span></span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/7.png" alt="img"> </p><h1 id="4-SIFT算法进行图像分类的思路"><a href="#4-SIFT算法进行图像分类的思路" class="headerlink" title="4. SIFT算法进行图像分类的思路"></a>4. SIFT算法进行图像分类的思路</h1><p>通过比较待识别图片与训练图集中的每一张图片的sift描述子的个数，找出匹配度最高的那张图片所在的类别，则该类别就被判定为待识别图片的类别。</p><p>如以下代码所示，有40类图片，每类图片5张：首先计算待识别图片的des描述子，然后依次比较每一类中每一张图片与该描述子的匹配数之和（平均匹配数），则匹配数之和（平均匹配数）最大的一类判定为与待识别图片为同一类。</p><p><img src="/2023/06/13/SIFT%E7%AE%97%E6%B3%95/8.png" alt="img"> </p><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing-and-MachineLearning" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-SIFT算法简介&quot;&gt;&lt;a href=&quot;#1-SIFT算法简介&quot; class=&quot;headerlink&quot; title=&quot;1. SIFT算法简介&quot;&gt;&lt;/a&gt;1. SIFT算法简介&lt;/h1&gt;&lt;p&gt;SIFT (Scale-invariant feature transform)，尺度不变特征转换，是一种图像局部特征提取算法，它通过在不同的尺度空间中寻找极值点（特征点，关键点）的精确定位和主方向，构建关键点描述符来提取特征。SIFT提取的关键点具有尺度不变性、旋转不变性，而且不会因光照、仿射变换和噪音等因素而干扰。SIFT所查找到的关键点是一些十分突出、不会因光照、仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="特征提取" scheme="http://yoursite.com/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>LBP特征提取</title>
    <link href="http://yoursite.com/2023/06/12/LBP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    <id>http://yoursite.com/2023/06/12/LBP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/</id>
    <published>2023-06-12T00:49:17.000Z</published>
    <updated>2023-06-12T01:47:54.339Z</updated>
    
    <content type="html"><![CDATA[<p>LBP指局部二值模式，英文全称：Local Binary Pattern，是一种用来描述图像局部特征的算子，LBP特征具有灰度不变性和旋转不变性等显著优点。<a id="more"></a></p><h1 id="1-LBP原理及其扩展"><a href="#1-LBP原理及其扩展" class="headerlink" title="1 LBP原理及其扩展"></a>1 LBP原理及其扩展</h1><h2 id="1-1-原始LBP"><a href="#1-1-原始LBP" class="headerlink" title="1.1 原始LBP"></a>1.1 原始LBP</h2><p>原始的LBP算子定义在像素3<em>3的邻域内，以邻域中心像素为阈值，相邻的8个像素的灰度值与邻域中心的像素值进行比较，若周围像素大于中心像素值，则该像素点的位置被标记为1，否则为0。这样，3</em>3邻域内的8个点经过比较可产生8位二进制数，将这8位二进制数依次排列形成一个二进制数字，这个二进制数字就是中心像素的LBP值，因此LBP值有256种。中心像素的LBP值反映了该像素周围区域的纹理信息。需要注意的是，计算LBP特征的图像必须是灰度图，如果是彩色图，需要先转换成灰度图。 </p><h2 id="1-2-圆形LBP"><a href="#1-2-圆形LBP" class="headerlink" title="1.2 圆形LBP"></a>1.2 圆形LBP</h2><p>由于原始LBP特征使用的是固定邻域内的灰度值，因此当图像的尺度发生变化时，LBP特征的编码将会发生错误，LBP特征将不能正确的反映像素点周围的纹理信息，因此研究人员对其进行了改进。基本的LBP算子的最大缺陷在于它只覆盖了一个固定半径范围内的小区域，这显然不能满足不同尺寸和频率纹理的需要。为了适应不同尺度的纹理特征，并达到灰度和旋转不变性的要求，Ojala等对LBP算子进行了改进，将3*3邻域扩展到任意邻域，并用圆形邻域代替了正方形邻域，改进后的LBP算子允许在半径为R的圆形邻域内有任意多个像素点。从而得到了诸如半径为R的圆形区域内含有P个采样点的LBP算子，这种LBP特征叫做Extended LBP，也叫Circular LBP。</p><h2 id="1-3-旋转不变LBP"><a href="#1-3-旋转不变LBP" class="headerlink" title="1.3 旋转不变LBP"></a>1.3 旋转不变LBP</h2><p>上面的LBP特征具有灰度不变性，但还不具备旋转不变性，因此研究人员又在上面的基础上进行了扩展，提出了具有旋转不变性的LBP特征。首先不断的旋转圆形邻域内的LBP特征，根据选择得到一系列的LBP特征值，从这些LBP特征值选择LBP特征值最小的作为中心像素点的LBP特征。</p><h2 id="1-4-Uniform-Pattern-LBP"><a href="#1-4-Uniform-Pattern-LBP" class="headerlink" title="1.4 Uniform Pattern LBP"></a>1.4 Uniform Pattern LBP</h2><p>为了解决二进制模式过多的问题，提高统计性，Ojala提出了采用一种“等价模式”(Uniform Pattern)来对LBP算子的模式种类进行降维。Ojala等认为，在实际图像中，绝大多数LBP模式最多只包含两次从1到0或从0到1的跳变。因此，Ojala将“等价模式”定义为：当某个LBP所对应的循环二进制数从0到1或从1到0最多有两次跳变时，该LBP所对应的二进制就称为一个等价模式类。如00000000(0次跳变)，00000111(只含一次从0到1的跳变)，10001111(先由1跳到0，再由0跳到1，共两次跳变)都是等价模式类。除等价模式类以外的模式都归为另一类，称为混合模式类，例如10010111(共四次跳变)。通过这样的改进，二进制模式的种类大大减少，而不会丢失任何信息。模式数量由原来的2P种减少为P(P-1)+2种，其中P表示邻域集内的采样点数。对于3*3邻域内8个采样点来说，二进制模式由原始的256种减少为58种，即：它把值分为59类，58个uniform pattern为一类，其它的所有值为第59类。这样直方图从原来的256维变成59维。这使得特征向量的维数更少，并且可以减少高频噪声带来的影响。具体实现如下：</p><p>采样点数目为8个，即LBP特征值有28种，共256个值，正好对应灰度图像的0-255，因此原始的LBP特征图像是一幅正常的灰度图像，而等价模式LBP特征，根据0-1跳变次数，将这256个LBP特征值分为了59类，从跳变次数上划分：跳变0次—2个，跳变1次—0个，跳变2次—56个，跳变3次—0个，跳变4次—140个，跳变5次—0个，跳变6次—56个，跳变7次—0个，跳变8次—2个。共9种跳变情况，将这256个值进行分配，跳变小于2次的为等价模式类，共58个，他们对应的值按照从小到大分别编码为1-58，即它们在LBP特征图像中的灰度值为1-58，而除了等价模式类之外的混合模式类被编码为0，即它们在LBP特征中的灰度值为0，因此等价模式LBP特征图像整体偏暗。</p><h2 id="1-5-MB-LBP"><a href="#1-5-MB-LBP" class="headerlink" title="1.5 MB-LBP"></a>1.5 MB-LBP</h2><p>MB-LBP的原理：将图像分成一个个小块（Block），每个小块再分为一个个的小区域（类似于HOG中的cell），小区域内的灰度平均值作为当前小区域的灰度值，与周围小区域灰度进行比较形成LBP特征，生成的特征称为MB-LBP，Block大小为3<em>3，则小区域的大小为1，就是原始的LBP特征，Block大小为9</em>9，小区域的大小为3*3。接下来对得到LBP特征进行均值模式编码，通过对得到的特征图求直方图，得到了LBP特征值0-255之间(0-255即直方图中的bin)的特征数量，通过对bin中的数值进行排序，通过权衡，将排序在前63位的特征值看作是等价模式类，其他的为混合模式类，总共64类，作者在论文中称之为SEMB-LBP(Statistically Effective MB-LBP)。类似于等价模式LBP，等价模式的LBP的等价模式类为58种，混合模式类1种，共59种。二者除了等价模式类的数量不同之外，主要区别在于：对等价模式类的定义不同，等价模式LBP是根据0-1的跳变次数定义的，而SEMB-LBP是通过对直方图排序得到的。</p><h1 id="2-对于边缘点的处理"><a href="#2-对于边缘点的处理" class="headerlink" title="2. 对于边缘点的处理"></a>2. 对于边缘点的处理</h1><p>对于图像边缘的点，由于通常不能用上述方法来处理成LBP特征点，一般来说有以下几种处理方法：<br>（1）将边缘的提取不到周围特征的点不进行LBP处理，结果LBP特征图片的像素点会比之前少。<br>（2）将边缘不存在的邻域点值设置为0。<br>（3）将边缘的提取不到周围特征的点用原像素点的值取代。</p><h1 id="3-LBP的Python应用"><a href="#3-LBP的Python应用" class="headerlink" title="3. LBP的Python应用"></a>3. LBP的Python应用</h1><p>Python skimage库中使用local_binary_pattern函数实现LBP提取特征，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">local_binary_pattern(image, P, R, method=<span class="string">'default'</span>)</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：<br>image：（N，M）阵列Graylevel图像。<br>P：int圆对称邻居设置点的数量（角度空间的量化）。<br>R：float圆的半径（操作员的空间分辨率）。<br>method：{‘default’，‘ror’，‘uniform’，‘var’}确定模式的方法，如下所示：<br>        ‘default’：原始的局部二值模式，它是灰度但不是旋转不变的。<br>        ‘ror’：扩展灰度和旋转不变的默认实现。<br>        ‘uniform’：改进的旋转不变性和均匀的模式以及角度空间的更精细的量化，灰度和旋转不变。<br>        ‘nri_uniform’：非旋转不变的均匀图案变体，它只是灰度不变的R199。<br>        ‘VAR’：局部对比度的旋转不变方差度量，图像纹理是旋转但不是灰度不变的。</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> skimage.feature <span class="keyword">import</span> local_binary_pattern</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment"># settings for LBP</span></span><br><span class="line">radius = <span class="number">1</span>  <span class="comment"># LBP算法中范围半径的取值</span></span><br><span class="line">n_points = <span class="number">8</span> * radius   <span class="comment"># 领域像素点数</span></span><br><span class="line"></span><br><span class="line">image = cv2.imread(<span class="string">'lenna.jpg'</span>)  <span class="comment"># 读取图像</span></span><br><span class="line">image1 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) <span class="comment"># 按照RGB顺序展示原图</span></span><br><span class="line">image2 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)   <span class="comment"># 灰度转换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># LBP处理</span></span><br><span class="line">lbp = local_binary_pattern(image2, n_points, radius)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">131</span>), plt.imshow(image1), plt.title(<span class="string">'Original Image'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">132</span>), plt.imshow(image2, <span class="string">'gray'</span>), plt.title(<span class="string">'Gray Image'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">133</span>), plt.imshow(lbp, <span class="string">'gray'</span>), plt.title(<span class="string">'LBP Image'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/12/LBP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/3.png" alt="img"> </p><h1 id="4-LBPH算法及其应用"><a href="#4-LBPH算法及其应用" class="headerlink" title="4. LBPH算法及其应用"></a>4. LBPH算法及其应用</h1><h2 id="4-1-LBPH算法原理"><a href="#4-1-LBPH算法原理" class="headerlink" title="4.1 LBPH算法原理"></a>4.1 LBPH算法原理</h2><p>LBPH(Local Binary Patterns Histograms)局部二进制编码直方图，是一种人脸识别算法，用于识别人脸，它以其性能以及如何能够从正面和侧面识别人脸而闻名。建立在LBPH基础之上的人脸识别法基本思想如下：首先以每个像素为中心，判断与周围像素灰度值大小关系，对其进行二进制编码，从而获得整幅图像的LBP编码图像；再将LBP图像分为个区域，获取每个区域的LBP编码直方图，继而得到整幅图像的LBP编码直方图，通过比较不同人脸图像LBP编码直方图达到人脸识别的目的,其优点是不会受到光照、缩放、旋转和平移的影响。</p><p>一幅图像具体的计算LBPH的过程如下：<br>a.    计算图像的LBP特征图像；<br>b.    将LBP特征图像进行分块，Opencv中默认将LBP特征图像分成8行8列64块区域；<br>c.    计算每块区域特征图像的直方图cell_LBPH，将直方图进行归一化；<br>d.    将上面计算的每块区域特征图像的直方图按分块的空间顺序依次排列成一行，形成LBP特征向量；<br>e.    用机器学习的方法对LBP特征向量进行训练，用来检测和识别目标。</p><h2 id="4-2-LBPH识别应用"><a href="#4-2-LBPH识别应用" class="headerlink" title="4.2 LBPH识别应用"></a>4.2 LBPH识别应用</h2><p>如下是LBPH人脸识别的部分测试代码：</p><p><img src="/2023/06/12/LBP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/4.png" alt="img"> </p><p>根据上述代码，基于LBPH的人脸识别基本步骤可以概括如下：</p><p>a. 创建空列表分别存储用于训练的人脸和人脸标签；</p><p>b. 遍历训练集，将训练人脸及便签存进空列表；</p><p>c. 通过OpenCV的LBPH识别函数对训练集中的人脸和人脸标签生成识别器；</p><p>d. 读取测试图像，通过识别器进行预测识别该人脸属于哪一类。</p><h1 id="5-源码仓库地址"><a href="#5-源码仓库地址" class="headerlink" title="5. 源码仓库地址"></a>5. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;LBP指局部二值模式，英文全称：Local Binary Pattern，是一种用来描述图像局部特征的算子，LBP特征具有灰度不变性和旋转不变性等显著优点。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="特征提取" scheme="http://yoursite.com/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>霍夫变换</title>
    <link href="http://yoursite.com/2023/06/11/%E9%9C%8D%E5%A4%AB%E5%8F%98%E6%8D%A2/"/>
    <id>http://yoursite.com/2023/06/11/%E9%9C%8D%E5%A4%AB%E5%8F%98%E6%8D%A2/</id>
    <published>2023-06-11T05:54:41.000Z</published>
    <updated>2023-06-11T06:11:00.326Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-什么是霍夫变换"><a href="#1-什么是霍夫变换" class="headerlink" title="1. 什么是霍夫变换"></a>1. 什么是霍夫变换</h1><p>霍夫变换(Hough Transform)是图像处理中的一种特征提取技术，该过程在一个参数空间中通过计算累计结果的局部最大值得到一个符合该特定形状的集合作为霍夫变换结果。</p><p>霍夫变换于1962年由PaulHough首次提出，最初的Hough变换是设计用来检测直线和曲线，起初的方法要求知道物体边界线的解析方程，但不需要有关区域位置的先验知识。这种方法的一个突出优点是分割结果的Robustness,即对数据的不完全或噪声不是非常敏感。然而，要获得描述边界的解析表达常常是不可能的。后于1972年由Richard Duda &amp; Peter Hart推广使用，经典霍夫变换用来检测图像中的直线，后来霍夫变换扩展到任意形状物体的识别，多为圆和椭圆。霍夫变换运用两个坐标空间之间的变换将在一个空间中具有相同形状的曲线或直线映射到另一个坐标空间的一个点上形成峰值，从而把检测任意形状的问题转化为统计峰值问题。</p><p>霍夫变换在OpenCV中分为霍夫线变换和霍夫圆变换两种。<a id="more"></a></p><h1 id="2-霍夫直线检测"><a href="#2-霍夫直线检测" class="headerlink" title="2. 霍夫直线检测"></a>2. 霍夫直线检测</h1><p>Hough直线检测的基本原理在于利用点与线的对偶性，在我们的直线检测任务中，即图像空间中的直线与参数空间中的点是一一对应的，参数空间中的直线与图像空间中的点也是一一对应的。这意味着我们可以得出两个非常有用的结论：</p><p>​    1）图像空间中的每条直线在参数空间中都对应着单独一个点来表示；<br>​    2）图像空间中的直线上任何一部分线段在参数空间对应的是同一个点。</p><p>因此Hough直线检测算法就是把在图像空间中的直线检测问题转换到参数空间中对点的检测问题，通过在参数空间里寻找峰值来完成直线检测任务。</p><h2 id="2-1-霍夫直线检测的具体步骤"><a href="#2-1-霍夫直线检测的具体步骤" class="headerlink" title="2.1 霍夫直线检测的具体步骤"></a>2.1 霍夫直线检测的具体步骤</h2><ol><li>彩色图像-&gt;灰度图；</li><li>去噪（高斯核）；</li><li>边缘提取（梯度算子、拉普拉斯算子、canny、sobel）；</li><li>二值化（判断此处是否为边缘点，就看灰度值==255）；</li><li>映射到霍夫空间（准备两个容器，一个用来展示hough-space概况，一个数组hough-space用来储存voting的值，因为投票过程往往有某个极大值超过阈值，多达几千，不能直接用灰度图来记录投票信息）；</li><li>取局部极大值，设定阈值，过滤干扰直线；</li><li>绘制直线、标定角点。 </li></ol><h2 id="2-2-霍夫直线检测的优缺点"><a href="#2-2-霍夫直线检测的优缺点" class="headerlink" title="2.2 霍夫直线检测的优缺点"></a>2.2 霍夫直线检测的优缺点</h2><p><strong>优点</strong>：Hough直线检测的优点是抗干扰能力强，对图像中直线的残缺部分、噪声以及其它共存的非直线结构不敏感，能容忍特征边界描述中的间隙，并且相对不受图像噪声的影响。</p><p><strong>缺点</strong>：Hough变换算法的特点导致其时间复杂度和空间复杂度都很高，并且在检测过程中只能确定直线方向，丢失了线段的长度信息。由于霍夫检测过程中进行了离散化，因此检测精度受参数离散间隔制约。</p><h2 id="2-3-OpenCV中霍夫直线检测的应用"><a href="#2-3-OpenCV中霍夫直线检测的应用" class="headerlink" title="2.3 OpenCV中霍夫直线检测的应用"></a>2.3 OpenCV中霍夫直线检测的应用</h2><p>OpenCV的中用函数HoughLines(标准)和HoughLinesP(基于统计)来检测图像中的直线，函数原型如下：</p><h3 id="2-3-1-标准霍夫检测"><a href="#2-3-1-标准霍夫检测" class="headerlink" title="2.3.1 标准霍夫检测"></a>2.3.1 标准霍夫检测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lines = cv.HoughLines( image, rho, theta, threshold[, lines[, srn[, stn[, min_theta[, max_theta]]]]] )</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：<br>lines：返回值（ρ,θ），ρ以像素为单位，θ以弧度为单位；<br>rho：累加器的距离分辨率（以像素为单位）；<br>theta：累加器的角度分辨率（弧度）；<br>threshold：阈值，仅大于的才可以被返回；<br>srn：对于多尺度Hough变换，它是距离分辨率rho的除数。粗累加器距离分辨率为rho，精确累加器分辨率为rho/srn。如果srn=0和stn=0，则使用经典Hough变换。否则，这两个参数都应为正值；<br>stn：对于多尺度Hough变换，它是距离分辨率θ的除数；<br>min_theta：对于标准和多尺度Hough变换，检查直线的最小角度。必须介于0和最大θ之间；<br>max_theta：对于标准和多尺度Hough变换，检查直线的最大角度。必须介于min_theta和CV_PI之间。</p></blockquote><h3 id="2-3-2-概率霍夫检测"><a href="#2-3-2-概率霍夫检测" class="headerlink" title="2.3.2 概率霍夫检测"></a>2.3.2 概率霍夫检测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lines = cv.HoughLinesP( image, rho, theta, threshold[, lines[, minLineLength[, maxLineGap]]] )</span><br></pre></td></tr></table></figure><blockquote><p>参数说明：<br>lines：返回两个端点的坐标；<br>rho：累加器的距离分辨率（以像素为单位）；<br>theta：累加器的角度分辨率（弧度）；<br>threshold：阈值，仅大于的才可以被返回；<br>minLineLength：最小行长度，小于该长度的线段将被拒绝；<br>maxLineGap：同一直线上连接点的最大允许间距。</p></blockquote><p> 测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">src = cv.imread(<span class="string">"demo-line.jpg"</span>)</span><br><span class="line">img = src.copy()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 二值化图像（Canny边缘检测）</span></span><br><span class="line">gray_img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line">dst_img = cv.Canny(gray_img, <span class="number">50</span>, <span class="number">150</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 霍夫线变换</span></span><br><span class="line">lines = cv.HoughLines(dst_img, <span class="number">0.5</span>, np.pi / <span class="number">180</span>, <span class="number">300</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将检测的线绘制在原图上（注意是极坐标）</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">    rho, theta = line[<span class="number">0</span>]</span><br><span class="line">    a = np.cos(theta)</span><br><span class="line">    b = np.sin(theta)</span><br><span class="line">    <span class="comment"># 找两个点</span></span><br><span class="line">    x0 = rho * a</span><br><span class="line">    y0 = rho * b</span><br><span class="line">    x1 = int(x0 + <span class="number">1000</span> * (-b))</span><br><span class="line">    y1 = int(y0 + <span class="number">1000</span> * a)</span><br><span class="line">    x2 = int(x0 - <span class="number">1000</span> * (-b))</span><br><span class="line">    y2 = int(y0 - <span class="number">1000</span> * a)</span><br><span class="line">    cv.line(img, (x1, y1), (x2, y2), (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line">plt.subplot(<span class="number">311</span>), plt.imshow(src, <span class="string">'gray'</span>), plt.title(<span class="string">'src_img'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">312</span>), plt.imshow(dst_img, <span class="string">'gray'</span>), plt.title(<span class="string">'canny_img'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">313</span>), plt.imshow(img, <span class="string">'gray'</span>), plt.title(<span class="string">'HoughLines_img'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p> 效果如下：</p><p><img src="/2023/06/11/%E9%9C%8D%E5%A4%AB%E5%8F%98%E6%8D%A2/2.3.2.png" alt="img"> </p><h1 id="3-霍夫圆检测"><a href="#3-霍夫圆检测" class="headerlink" title="3. 霍夫圆检测"></a>3. 霍夫圆检测</h1><p>霍夫圆变换的基本思路是认为图像上每一个非零像素点都有可能是一个潜在的圆上的一点，跟霍夫线变换一样，也是通过投票，生成累积坐标平面，设置一个累积权重来定位圆。</p><p>OpenCV中使用cv2.HoughCircles函数来实现霍夫圆检测，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.HoughCircles(image, method, dp, minDist[, circles[, param1[, param2[, minRadius[, maxRadius]]]]]) → circles</span><br></pre></td></tr></table></figure><blockquote><p>参数说明:<br>image：8位单通道图像，如果使用彩色图像，需要先转换成灰度图像；<br>method：定义检测图像中圆的方法，目前唯一实现的方法是cv2.HOUGH_GRADIENT；<br>dp：图像像素分辨率与参数空间分辨率的比值（官方文档上写的是图像分辨率与累加器分辨率的比值，它把参数空间认为是一个累加器，毕竟里面存储的都是经过的像素点的数量），dp=1，则参数空间与图像像素空间（分辨率）一样大，dp=2，参数空间的分辨率只有像素空间的一半大；<br>minDist：检测到的圆的中心，（x,y）坐标之间的最小距离。如果minDist太小，则可能导致检测到多个相邻的圆。如果minDist太大，则可能导致很多圆检测不到；<br>param1：用于处理边缘检测的梯度值方法；<br>param2：cv2.HOUGH_GRADIENT方法的累加器阈值。阈值越小，检测到的圆越多；<br>minRadius：半径的最小大小（以像素为单位）；<br>maxRadius：半径的最大大小（以像素为单位）。</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">'demo-circle.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">img = cv2.medianBlur(img,<span class="number">5</span>)</span><br><span class="line">cimg = cv2.cvtColor(img,cv2.COLOR_GRAY2BGR)</span><br><span class="line">circles = cv2.HoughCircles(img,cv2.HOUGH_GRADIENT,<span class="number">1</span>,<span class="number">200</span>,param1=<span class="number">50</span>,param2=<span class="number">30</span>,minRadius=<span class="number">0</span>,maxRadius=<span class="number">0</span>)</span><br><span class="line">circles = np.uint16(np.around(circles))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> circles[<span class="number">0</span>,:]:</span><br><span class="line">    <span class="comment"># draw the outer circle</span></span><br><span class="line">    cv2.circle(cimg,(i[<span class="number">0</span>],i[<span class="number">1</span>]),i[<span class="number">2</span>],(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># draw the center of the circle</span></span><br><span class="line">    cv2.circle(cimg,(i[<span class="number">0</span>],i[<span class="number">1</span>]),<span class="number">2</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(img, <span class="string">'gray'</span>), plt.title(<span class="string">'src_img'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(cimg, <span class="string">'gray'</span>), plt.title(<span class="string">'HoughCircles_img'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/11/%E9%9C%8D%E5%A4%AB%E5%8F%98%E6%8D%A2/3.png" alt="img"> </p><h1 id="4-源码仓库地址"><a href="#4-源码仓库地址" class="headerlink" title="4. 源码仓库地址"></a>4. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-什么是霍夫变换&quot;&gt;&lt;a href=&quot;#1-什么是霍夫变换&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是霍夫变换&quot;&gt;&lt;/a&gt;1. 什么是霍夫变换&lt;/h1&gt;&lt;p&gt;霍夫变换(Hough Transform)是图像处理中的一种特征提取技术，该过程在一个参数空间中通过计算累计结果的局部最大值得到一个符合该特定形状的集合作为霍夫变换结果。&lt;/p&gt;
&lt;p&gt;霍夫变换于1962年由PaulHough首次提出，最初的Hough变换是设计用来检测直线和曲线，起初的方法要求知道物体边界线的解析方程，但不需要有关区域位置的先验知识。这种方法的一个突出优点是分割结果的Robustness,即对数据的不完全或噪声不是非常敏感。然而，要获得描述边界的解析表达常常是不可能的。后于1972年由Richard Duda &amp;amp; Peter Hart推广使用，经典霍夫变换用来检测图像中的直线，后来霍夫变换扩展到任意形状物体的识别，多为圆和椭圆。霍夫变换运用两个坐标空间之间的变换将在一个空间中具有相同形状的曲线或直线映射到另一个坐标空间的一个点上形成峰值，从而把检测任意形状的问题转化为统计峰值问题。&lt;/p&gt;
&lt;p&gt;霍夫变换在OpenCV中分为霍夫线变换和霍夫圆变换两种。&lt;/p&gt;</summary>
    
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>图像分割算法</title>
    <link href="http://yoursite.com/2023/06/10/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2023/06/10/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%AE%97%E6%B3%95/</id>
    <published>2023-06-10T06:53:32.000Z</published>
    <updated>2023-06-11T06:11:14.618Z</updated>
    
    <content type="html"><![CDATA[<p>图像分割是指将图像分成若干互不重叠的子区域，使得同一个子区域内的特征具有一定相似性，不同子区域的特征呈现较为明显的差异。之前介绍了基于阈值的分割方法，比如Otsu法等；基于边缘检测的分割方法，比如Sobel算子、Canny算子等。下面介绍<strong>基于区域的分割方法</strong>和<strong>基于图的分割方法</strong>。<a id="more"></a></p><h1 id="1-基于区域的分割方法"><a href="#1-基于区域的分割方法" class="headerlink" title="1. 基于区域的分割方法"></a>1. 基于区域的分割方法</h1><p>基于区域的分割算法将具有相似特征的像素集合聚集构成一个区域，这个区域中的相邻像素之间具有相似的性质，主要包括<strong>区域生长算法、区域分裂合并算法和分水岭算法</strong>等。</p><h2 id="1-1-区域生长算法"><a href="#1-1-区域生长算法" class="headerlink" title="1.1 区域生长算法"></a>1.1 区域生长算法</h2><p>区域生长的基本思想是将具有相似性质的像素集合起来构成区域。具体先对每个需要分割的区域找一个种子像素作为生长起点，然后将种子像素和周围邻域中与种子像素有相同或相似性质的像素（根据某种事先确定的生长或相似准则来判定）合并到种子像素所在的区域中。将这些新像素当作新的种子继续上面的过程，直到没有满足条件的像素可被包括进来，这样一个区域就生长成了。</p><p>区域生长实现步骤如下:</p><ol><li>对图像顺序扫描，找到第1个还没有归属的像素，设该像素为(x0, y0)；</li><li>以(x0,y0)为中心，考虑(x0,y0)的4邻域或者8邻域像素(x,y)与种子像素的灰度值之差的绝对值小于某个阈值T，如果满足条件，将(x,y)与(x0,y0)合并(在同一区域内)，同时将(x,y)压入堆栈；</li><li>从堆栈中取出一个像素，把它当作(x0, y0)返回到步骤2；</li><li>当堆栈为空时，返回到步骤1；</li><li>重复步骤1-4直到图像中的每个点都有归属时，生长结束。</li></ol><p>下图为测试效果（左侧为原图灰度图像，右侧为区域生长图像）： </p><p><img src="/2023/06/10/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%AE%97%E6%B3%95/2.1.png" alt> </p><blockquote><p>需要注意的是：当选取的种子不同时，得到的区域生长图像也会不同。</p></blockquote><h2 id="1-2-区域分裂合并算法"><a href="#1-2-区域分裂合并算法" class="headerlink" title="1.2 区域分裂合并算法"></a>1.2 区域分裂合并算法</h2><p>区域生长是从某个或者某些像素点出发，最后得到整个区域，进而实现目标提取。而分裂合并可以看做是区域生长的逆过程：从整个图像出发，不断分裂得到各个子区域，然后再把前景区域合并，实现目标提取。分裂合并的假设是对于一幅图像，前景区域由一些相互连通的像素组成。因此，如果把一幅图像分裂到像素级，那么就可以判定该像素是否为前景像素。当所有像素点或者子区域完成判断以后，把前景区域或者像素合并就可得到前景目标。</p><p>假定一幅图像分为若干区域，按照有关区域的逻辑词P的性质，各个区域上所有的像素将是一致的。区域分裂合并的算法如下：</p><ol><li>将整幅图像设置为初始区域；</li><li>选择一个区域R，若P（R）错误，则将该区域分为4个子区域；</li><li>考虑图像中任意两个或更多的邻接子区域R1，R2，…,Rn；</li><li>如果P（R1∪R2∪…∪Rn）正确，则将这n个区域合并为一个区域；</li><li>重复上述步骤，直到不能再进行区域分裂合并。</li></ol><p>下图为测试效果：</p><p><img src="/2023/06/10/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%AE%97%E6%B3%95/2.2.png" alt> </p><h2 id="1-3-分水岭算法"><a href="#1-3-分水岭算法" class="headerlink" title="1.3 分水岭算法"></a>1.3 分水岭算法</h2><h3 id="1-3-1-分水岭算法原理"><a href="#1-3-1-分水岭算法原理" class="headerlink" title="1.3.1 分水岭算法原理"></a>1.3.1 分水岭算法原理</h3><p>图像的灰度空间很像地球表面的整个地理结构，每个像素的灰度值代表高度。其中的灰度值较大的像素连成的线可以看做山脊，也就是分水岭。其中的水就是用于二值化的gray threshold level，二值化阈值可以理解为水平面，比水平面低的区域会被淹没，刚开始用水填充每个孤立的山谷(局部最小值)。当水平面上升到一定高度时，水就会溢出当前山谷，可以通过在分水岭上修大坝，从而避免两个山谷的水汇集，这样图像就被分成2个像素集，一个是被水淹没的山谷像素集，一个是分水岭线像素集。最终这些大坝形成的线就对整个图像进行了分区，实现对图像的分割。</p><p>在该算法中，空间上相邻并且灰度值相近的像素被划分为一个区域。</p><p>分水岭算法的整个过程如下：</p><ol><li>把梯度图像中的所有像素按照灰度值进行分类，并设定一个测地距离阈值；</li><li>找到灰度值最小的像素点（默认标记为灰度值最低点），让threshold从最小值开始增长，这些点为起始点；</li><li>水平面在增长的过程中，会碰到周围的邻域像素，测量这些像素到起始点（灰度值最低点）的测地距离，如果小于设定阈值，则将这些像素淹没，否则在这些像素上设置大坝，这样就对这些邻域像素进行了分类；</li><li>随着水平面越来越高，会设置更多更高的大坝，直到灰度值的最大值，所有区域都在分水岭线上相遇，这些大坝就对整个图像像素进行了分区。</li></ol><p>用上面的算法对图像进行分水岭运算，由于噪声点或其它因素的干扰，可能会得到密密麻麻的小区域，即图像被分得太细（over-segmented，过度分割），这因为图像中有非常多的局部极小值点，每个点都会自成一个小区域。</p><p>其中的解决方法有：</p><p>（1）对图像进行高斯平滑操作，抹除很多小的最小值，这些小分区就会合并。</p><p>（2）不从最小值开始增长，可以将相对较高的灰度值像素作为起始点（需要用户手动标记），从标记处开始进行淹没，则很多小区域都会被合并为一个区域，这被称为<strong>基于图像标记(mark)的分水岭算法</strong>。</p><h3 id="1-3-2-opencv-python中分水岭算法的应用"><a href="#1-3-2-opencv-python中分水岭算法的应用" class="headerlink" title="1.3.2 opencv-python中分水岭算法的应用"></a>1.3.2 opencv-python中分水岭算法的应用</h3><p>在OpenCV中，我们需要给不同区域贴上不同的标签。用大于1的整数表示我们确定为前景或对象的区域，用1表示我们确定为背景或非对象的区域，最后用0表示我们无法确定的区域。然后应用分水岭算法，我们的标记图像将被更新，更新后的标记图像的边界像素值为-1。</p><p>具体步骤如下：</p><ol><li>对图进行灰度化和二值化得到二值图像；</li><li>通过膨胀得到确定的背景区域，通过距离转换得到确定的前景区域，剩余部分为不确定区域；</li><li>对确定的前景图像进行连接组件处理，得到标记图像；</li><li>根据标记图像对原图像应用分水岭算法，更新标记图像。</li></ol><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">ret, thresh = cv2.threshold(gray, <span class="number">0</span>, <span class="number">255</span>, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)</span><br><span class="line"></span><br><span class="line"><span class="comment"># noise removal</span></span><br><span class="line">kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">sure_bg = cv2.dilate(opening, kernel, iterations=<span class="number">2</span>)  <span class="comment"># sure background area</span></span><br><span class="line">sure_fg = cv2.erode(opening, kernel, iterations=<span class="number">2</span>)  <span class="comment"># sure foreground area</span></span><br><span class="line">unknown = cv2.subtract(sure_bg, sure_fg)  <span class="comment"># unknown area</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform the distance transform algorithm</span></span><br><span class="line">dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># Normalize the distance image for range = &#123;0.0, 1.0&#125;</span></span><br><span class="line">cv2.normalize(dist_transform, dist_transform, <span class="number">0</span>, <span class="number">1.0</span>, cv2.NORM_MINMAX)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finding sure foreground area</span></span><br><span class="line">ret, sure_fg = cv2.threshold(dist_transform, <span class="number">0.5</span>*dist_transform.max(), <span class="number">255</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finding unknown region</span></span><br><span class="line">sure_fg = np.uint8(sure_fg)</span><br><span class="line">unknown = cv2.subtract(sure_bg,sure_fg)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Marker labelling</span></span><br><span class="line">ret, markers = cv2.connectedComponents(sure_fg)</span><br><span class="line"><span class="comment"># Add one to all labels so that sure background is not 0, but 1</span></span><br><span class="line">markers = markers+<span class="number">1</span></span><br><span class="line"><span class="comment"># Now, mark the region of unknown with zero</span></span><br><span class="line">markers[unknown==<span class="number">255</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">markers_copy = markers.copy()</span><br><span class="line">markers_copy[markers==<span class="number">0</span>] = <span class="number">150</span>  <span class="comment"># 灰色表示背景</span></span><br><span class="line">markers_copy[markers==<span class="number">1</span>] = <span class="number">0</span>    <span class="comment"># 黑色表示背景</span></span><br><span class="line">markers_copy[markers&gt;<span class="number">1</span>] = <span class="number">255</span>   <span class="comment"># 白色表示前景</span></span><br><span class="line"></span><br><span class="line">markers_copy = np.uint8(markers_copy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用分水岭算法执行基于标记的图像分割，将图像中的对象与背景分离</span></span><br><span class="line">markers = cv2.watershed(img, markers)</span><br><span class="line">img[markers==<span class="number">-1</span>] = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>]  <span class="comment"># 将边界标记为红色</span></span><br><span class="line"></span><br><span class="line">plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)), plt.title(<span class="string">'Watershed_image'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/10/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%AE%97%E6%B3%95/2.3.png" alt="img"> </p><h1 id="2-基于图的分割方法"><a href="#2-基于图的分割方法" class="headerlink" title="2. 基于图的分割方法"></a>2. 基于图的分割方法</h1><p>此类方法把图像分割问题与图的最小割（min cut）问题相关联。首先将图像映射为带权无向图G=<V，E>，图中每个节点N∈V对应于图像中的每个像素，每条边∈E连接着一对相邻的像素，边的权值表示了相邻像素之间在灰度、颜色或纹理方面的非负相似度。而对图像的一个分割s就是对图的一个剪切，被分割的每个区域C∈S对应着图中的一个子图。而分割的最优原则就是使划分后的子图在内部保持相似度最大，而子图之间的相似度保持最小。基于图的分割方法的本质就是移除特定的边，将图划分为若干子图从而实现分割，常用的方法有GraphCut，GrabCut和Random Walk等。</V，E></p><h3 id="2-1-Grabcut图像分割"><a href="#2-1-Grabcut图像分割" class="headerlink" title="2.1 Grabcut图像分割"></a>2.1 Grabcut图像分割</h3><p>Grabcut是基于图割(graph cut)实现的图像分割算法，它需要用户输入一个bounding box作为分割目标位置，实现对目标与背景的分离/分割。Grabcut分割速度快，效果好，支持交互操作，因此在很多APP图像分割/背景虚化的软件中经常使用。</p><p>算法流程如下：</p><ol><li>在图片中定义含有（一个或多个）物体的矩形；</li><li>矩形外的区域被自动认为是背景；</li><li>对于用户定义的矩形区域，可用背景中数据来区分是前景还是背景；</li><li>用高斯混合模型（GMM）来对背景和前景建模，并将未定义的像素标记为可能的前景或背景；</li><li>图像中的每一个像素都被看作通过虚拟边与周围像素连接，而每条边都有一个属于前景或背景的概率，这基于它和周围像素颜色上的相似性；</li><li>每一个像素（即算法中的节点）会与一个前景或背景节点连接；</li><li>在节点完成连接后(可能与背景或前景连接)，若节点之间的边属于不同终端(即一个节点属于前景，另一个节点属于背景)，则会切断他们之间的边，这就能将图像各部分分割出来。</li></ol><p>OpenCV中使用cv2.grabCut()函数来实现图像分割，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.grabCut(img, rect, mask, bgdModel, fgdModel, iterCount, mode = GC_EVAL)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li><p>img：输入的三通道图像；</p></li><li><p>rect：表示roi区域；</p></li><li><p>mask：输入的单通道图像，初始化方式为GC_INIT_WITH_RECT表示ROI区域可以被初始化为：</p><p>GC_BGD：定义为明显的背景像素 0</p><p>GC_FGD：定义为明显的前景像素 1</p><p>GC_PR_BGD：定义为可能的背景像素 2</p><p>GC_PR_FGD：定义为可能的前景像素 3</p></li><li><p>bgdModel：表示临时背景模型数组；</p></li><li><p>fgdModel：表示临时前景模型数组；</p></li><li><p>iterCount：表示图割算法迭代次数, 次数越多，效果越好；</p></li><li><p>mode：当使用用户提供的roi时使用GC_INIT_WITH_RECT。</p></li></ul><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">r = cv2.selectROI(<span class="string">'input'</span>, img, <span class="literal">False</span>)  <span class="comment"># 返回 (x_min, y_min, w, h)</span></span><br><span class="line">print(<span class="string">"input:"</span>, r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># roi区域</span></span><br><span class="line">roi = img[int(r[<span class="number">1</span>]):int(r[<span class="number">1</span>] + r[<span class="number">3</span>]), int(r[<span class="number">0</span>]):int(r[<span class="number">0</span>] + r[<span class="number">2</span>])]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 原图mask</span></span><br><span class="line">mask = np.zeros(img.shape[:<span class="number">2</span>], dtype=np.uint8)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩形roi</span></span><br><span class="line">rect = (int(r[<span class="number">0</span>]), int(r[<span class="number">1</span>]), int(r[<span class="number">2</span>]), int(r[<span class="number">3</span>]))  <span class="comment"># 包括前景的矩形，格式为(x,y,w,h)</span></span><br><span class="line"></span><br><span class="line">bgdmodel = np.zeros((<span class="number">1</span>, <span class="number">65</span>), np.float64)  <span class="comment"># bg模型的临时数组</span></span><br><span class="line">fgdmodel = np.zeros((<span class="number">1</span>, <span class="number">65</span>), np.float64)  <span class="comment"># fg模型的临时数组</span></span><br><span class="line"></span><br><span class="line">cv2.grabCut(img, mask, rect, bgdmodel, fgdmodel, <span class="number">11</span>, mode=cv2.GC_INIT_WITH_RECT)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取前景和可能的前景区域</span></span><br><span class="line">mask2 = np.where((mask == <span class="number">1</span>) + (mask == <span class="number">3</span>), <span class="number">255</span>, <span class="number">0</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">print(mask2.shape)</span><br><span class="line"></span><br><span class="line">result = cv2.bitwise_and(img, img, mask=mask2)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)), plt.title(<span class="string">'roi_img'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB)), plt.title(<span class="string">'Grabcut_image'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="/2023/06/10/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%AE%97%E6%B3%95/3.png" alt="img"> </p><h1 id="3-源码仓库地址"><a href="#3-源码仓库地址" class="headerlink" title="3. 源码仓库地址"></a>3. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;图像分割是指将图像分成若干互不重叠的子区域，使得同一个子区域内的特征具有一定相似性，不同子区域的特征呈现较为明显的差异。之前介绍了基于阈值的分割方法，比如Otsu法等；基于边缘检测的分割方法，比如Sobel算子、Canny算子等。下面介绍&lt;strong&gt;基于区域的分割方法&lt;/strong&gt;和&lt;strong&gt;基于图的分割方法&lt;/strong&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    <category term="图像分割" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="opencv" scheme="http://yoursite.com/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>Cmake多级目录编译</title>
    <link href="http://yoursite.com/2023/04/21/Cmake%E5%A4%9A%E7%BA%A7%E7%9B%AE%E5%BD%95%E7%BC%96%E8%AF%91/"/>
    <id>http://yoursite.com/2023/04/21/Cmake%E5%A4%9A%E7%BA%A7%E7%9B%AE%E5%BD%95%E7%BC%96%E8%AF%91/</id>
    <published>2023-04-21T12:20:46.000Z</published>
    <updated>2023-04-21T12:40:09.506Z</updated>
    
    <content type="html"><![CDATA[<p>以下面的C++程序多级目录为例：</p><p><img src="/2023/04/21/Cmake%E5%A4%9A%E7%BA%A7%E7%9B%AE%E5%BD%95%E7%BC%96%E8%AF%91/目录结构.png" alt></p><p>根目录的CMakelists.txt示例代码如下所示：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION <span class="number">3.0</span><span class="number">.0</span>)</span><br><span class="line">project(cppjson VERSION <span class="number">0.1</span><span class="number">.0</span>)</span><br><span class="line"></span><br><span class="line"># The <span class="keyword">default</span> include directory</span><br><span class="line">include_directories(./)</span><br><span class="line"></span><br><span class="line"><span class="meta"># home directory source</span></span><br><span class="line">aux_source_directory(. ROOT_SOURCE)</span><br><span class="line"></span><br><span class="line"><span class="meta"># sub directory source</span></span><br><span class="line">add_subdirectory(json)</span><br><span class="line"></span><br><span class="line"><span class="meta"># build executable file</span></span><br><span class="line">add_executable(main $&#123;ROOT_SOURCE&#125; $&#123;PARSER_SRC&#125;)</span><br></pre></td></tr></table></figure><p>子目录的CMakelists.txt示例代码如下所示：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># sub directory source</span></span><br><span class="line">aux_source_directory($&#123;CMAKE_CURRENT_SOURCE_DIR&#125; PARSER_SRC)</span><br><span class="line"><span class="built_in">set</span>(PARSER_SRC $&#123;PARSER_SRC&#125; PARENT_SCOPE)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;以下面的C++程序多级目录为例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2023/04/21/Cmake%E5%A4%9A%E7%BA%A7%E7%9B%AE%E5%BD%95%E7%BC%96%E8%AF%91/目录结构.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;根目录的CMake</summary>
      
    
    
    
    <category term="C/C++" scheme="http://yoursite.com/categories/C-C/"/>
    
    
    <category term="C/C++ Cmake" scheme="http://yoursite.com/tags/C-C-Cmake/"/>
    
  </entry>
  
  <entry>
    <title>C++：explicit关键字</title>
    <link href="http://yoursite.com/2023/03/30/C-%EF%BC%9Aexplicit%E5%85%B3%E9%94%AE%E5%AD%97/"/>
    <id>http://yoursite.com/2023/03/30/C-%EF%BC%9Aexplicit%E5%85%B3%E9%94%AE%E5%AD%97/</id>
    <published>2023-03-30T06:16:36.000Z</published>
    <updated>2023-03-30T09:14:03.173Z</updated>
    
    <content type="html"><![CDATA[<p>C++中的explicit关键字只能用于修饰只有一个参数的类构造函数，它的作用是表明该构造函数是显示的，而非隐式的，跟它相对应的另一个关键字是implicit，意思是隐藏的，类构造函数默认情况下即声明为implicit(隐式)。<a id="more"></a></p><p>那么显示声明的构造函数和隐式声明的有什么区别呢? 来看下面的例子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CxString</span>  // 没有使用<span class="title">explicit</span>关键字的类声明, 即默认为隐式声明  </span></span><br><span class="line"><span class="class">&#123;</span>  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">    <span class="keyword">char</span> *_pstr;  </span><br><span class="line">    <span class="keyword">int</span> _size;  </span><br><span class="line">    CxString(<span class="keyword">int</span> <span class="built_in">size</span>)  </span><br><span class="line">    &#123;  </span><br><span class="line">        _size = <span class="built_in">size</span>;                <span class="comment">// string的预设大小  </span></span><br><span class="line">        _pstr = <span class="built_in">malloc</span>(<span class="built_in">size</span> + <span class="number">1</span>);    <span class="comment">// 分配string的内存  </span></span><br><span class="line">        <span class="built_in">memset</span>(_pstr, <span class="number">0</span>, <span class="built_in">size</span> + <span class="number">1</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    CxString(<span class="keyword">const</span> <span class="keyword">char</span> *p)  </span><br><span class="line">    &#123;  </span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">size</span> = <span class="built_in">strlen</span>(p);  </span><br><span class="line">        _pstr = <span class="built_in">malloc</span>(<span class="built_in">size</span> + <span class="number">1</span>);    <span class="comment">// 分配string的内存  </span></span><br><span class="line">        <span class="built_in">strcpy</span>(_pstr, p);            <span class="comment">// 复制字符串  </span></span><br><span class="line">        _size = <span class="built_in">strlen</span>(_pstr);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="comment">// 析构函数这里不讨论, 省略...  </span></span><br><span class="line">&#125;;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 下面是调用:  </span></span><br><span class="line">    <span class="function">CxString <span class="title">string1</span><span class="params">(<span class="number">24</span>)</span></span>;     <span class="comment">// 这样是OK的, 为CxString预分配24字节的大小的内存  </span></span><br><span class="line">    CxString string2 = <span class="number">10</span>;    <span class="comment">// 这样是OK的, 为CxString预分配10字节的大小的内存  </span></span><br><span class="line">    CxString string3;         <span class="comment">// 这样是不行的, 因为没有默认构造函数, 错误为: “CxString”: 没有合适的默认构造函数可用  </span></span><br><span class="line">    <span class="function">CxString <span class="title">string4</span><span class="params">(<span class="string">"aaaa"</span>)</span></span>; <span class="comment">// 这样是OK的  </span></span><br><span class="line">    CxString string5 = <span class="string">"bbb"</span>; <span class="comment">// 这样也是OK的, 调用的是CxString(const char *p)  </span></span><br><span class="line">    CxString string6 = <span class="string">'c'</span>;   <span class="comment">// 这样也是OK的, 其实调用的是CxString(int size), 且size等于'c'的ascii码  </span></span><br><span class="line">    string1 = <span class="number">2</span>;              <span class="comment">// 这样也是OK的, 为CxString预分配2字节的大小的内存  </span></span><br><span class="line">    string2 = <span class="number">3</span>;              <span class="comment">// 这样也是OK的, 为CxString预分配3字节的大小的内存  </span></span><br><span class="line">    string3 = string1;        <span class="comment">// 这样也是OK的, 至少编译是没问题的, free释放_pstr内存指针的时候可能会报错, 完整的代码必须重载运算符"=", 并在其中处理内存释放</span></span><br></pre></td></tr></table></figure><p><img src="/2023/03/30/C-%EF%BC%9Aexplicit%E5%85%B3%E9%94%AE%E5%AD%97/pic1.png" alt></p><p>上面的代码中, “CxString string2 = 10;” 这句为什么是可以的呢？</p><p>在C++中, 如果的构造函数只有一个参数时, 那么在编译的时候就会有一个缺省的转换操作：将该构造函数对应数据类型的数据转换为该类对象。也就是说 “CxString string2 = 10;” 这段代码，编译器自动将整型转换为CxString类对象，实际上等同于下面的操作：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CxString <span class="title">string2</span><span class="params">(<span class="number">10</span>)</span></span>;  </span><br><span class="line"><span class="comment">// 或如下代码</span></span><br><span class="line"><span class="function">CxString <span class="title">temp</span><span class="params">(<span class="number">10</span>)</span></span>;  </span><br><span class="line">CxString string2 = temp;</span><br></pre></td></tr></table></figure><p>但是，上面的代码中的_size代表的是字符串内存分配的大小，那么调用的第二句 “CxString string2 = 10;” 和第六句 “CxString string6 = ‘c’;” 就显得不伦不类，而且容易让人疑惑。有什么办法阻止这种用法呢？答案就是使用explicit关键字。我们把上面的代码修改一下，如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CxString</span>  // 使用关键字<span class="title">explicit</span>的类声明, 显示转换  </span></span><br><span class="line"><span class="class">&#123;</span>  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">    <span class="keyword">char</span> *_pstr;  </span><br><span class="line">    <span class="keyword">int</span> _size;  </span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">CxString</span><span class="params">(<span class="keyword">int</span> size)</span>  </span></span><br><span class="line"><span class="function">    </span>&#123;  </span><br><span class="line">        _size = size;  </span><br><span class="line">        <span class="comment">// 代码同上, 省略...  </span></span><br><span class="line">    &#125;  </span><br><span class="line">    CxString(<span class="keyword">const</span> <span class="keyword">char</span> *p)  </span><br><span class="line">    &#123;  </span><br><span class="line">        <span class="comment">// 代码同上, 省略...  </span></span><br><span class="line">    &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 下面是调用:  </span></span><br><span class="line">    <span class="function">CxString <span class="title">string1</span><span class="params">(<span class="number">24</span>)</span></span>;     <span class="comment">// 这样是OK的  </span></span><br><span class="line">    CxString string2 = <span class="number">10</span>;    <span class="comment">// 这样是不行的, 因为explicit关键字取消了隐式转换  </span></span><br><span class="line">    CxString string3;         <span class="comment">// 这样是不行的, 因为没有默认构造函数  </span></span><br><span class="line">    <span class="function">CxString <span class="title">string4</span><span class="params">(<span class="string">"aaaa"</span>)</span></span>; <span class="comment">// 这样是OK的  </span></span><br><span class="line">    CxString string5 = <span class="string">"bbb"</span>; <span class="comment">// 这样也是OK的, 调用的是CxString(const char *p)  </span></span><br><span class="line">    CxString string6 = <span class="string">'c'</span>;   <span class="comment">// 这样是不行的, 其实调用的是CxString(int size), 且size等于'c'的ascii码, 但explicit关键字取消了隐式转换  </span></span><br><span class="line">    string1 = <span class="number">2</span>;              <span class="comment">// 这样也是不行的, 因为取消了隐式转换  </span></span><br><span class="line">    string2 = <span class="number">3</span>;              <span class="comment">// 这样也是不行的, 因为取消了隐式转换  </span></span><br><span class="line">    string3 = string1;        <span class="comment">// 这样也是不行的, 因为取消了隐式转换, 除非类实现操作符"="的重载</span></span><br></pre></td></tr></table></figure><p><img src="/2023/03/30/C-%EF%BC%9Aexplicit%E5%85%B3%E9%94%AE%E5%AD%97/pic2.png" alt></p><p>explicit关键字的作用就是防止类构造函数的隐式自动转换。</p><p>如上面所说， explicit关键字只对有一个参数的类构造函数有效，如果类构造函数参数大于或等于两个时，是不会产生隐式转换的，所以explicit关键字也就无效了。例如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CxString</span>  // <span class="title">explicit</span>关键字在类构造函数参数大于或等于两个时无效  </span></span><br><span class="line"><span class="class">&#123;</span>  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">    <span class="keyword">char</span> *_pstr;  </span><br><span class="line">    <span class="keyword">int</span> _age;  </span><br><span class="line">    <span class="keyword">int</span> _size;  </span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">CxString</span><span class="params">(<span class="keyword">int</span> age, <span class="keyword">int</span> size)</span>  </span></span><br><span class="line"><span class="function">    </span>&#123;  </span><br><span class="line">        _age = age;  </span><br><span class="line">        _size = size;  </span><br><span class="line">        <span class="comment">// 代码同上, 省略...  </span></span><br><span class="line">    &#125;  </span><br><span class="line">    CxString(<span class="keyword">const</span> <span class="keyword">char</span> *p)  </span><br><span class="line">    &#123;  </span><br><span class="line">        <span class="comment">// 代码同上, 省略...  </span></span><br><span class="line">    &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 这个时候有没有explicit关键字都是一样的</span></span><br></pre></td></tr></table></figure><p>但是, 也有一个例外, 就是当除了第一个参数以外的其他参数都有默认值的时候, explicit关键字依然有效, 此时, 当调用构造函数时只传入一个参数, 等效于只有一个参数的类构造函数, 例子如下:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CxString</span>  // 使用关键字<span class="title">explicit</span>声明  </span></span><br><span class="line"><span class="class">&#123;</span>  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">    <span class="keyword">int</span> _age;  </span><br><span class="line">    <span class="keyword">int</span> _size;  </span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">CxString</span><span class="params">(<span class="keyword">int</span> age, <span class="keyword">int</span> size = <span class="number">0</span>)</span>  </span></span><br><span class="line"><span class="function">    </span>&#123;  </span><br><span class="line">        _age = age;  </span><br><span class="line">        _size = size;  </span><br><span class="line">        <span class="comment">// 代码同上, 省略...  </span></span><br><span class="line">    &#125;  </span><br><span class="line">    CxString(<span class="keyword">const</span> <span class="keyword">char</span> *p)  </span><br><span class="line">    &#123;  </span><br><span class="line">        <span class="comment">// 代码同上, 省略...  </span></span><br><span class="line">    &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 下面是调用:  </span></span><br><span class="line">    <span class="function">CxString <span class="title">string1</span><span class="params">(<span class="number">24</span>)</span></span>;     <span class="comment">// 这样是OK的  </span></span><br><span class="line">    CxString string2 = <span class="number">10</span>;    <span class="comment">// 这样是不行的, 因为explicit关键字取消了隐式转换  </span></span><br><span class="line">    CxString string3;         <span class="comment">// 这样是不行的, 因为没有默认构造函数  </span></span><br><span class="line">    string1 = <span class="number">2</span>;              <span class="comment">// 这样也是不行的, 因为取消了隐式转换  </span></span><br><span class="line">    string2 = <span class="number">3</span>;              <span class="comment">// 这样也是不行的, 因为取消了隐式转换  </span></span><br><span class="line">    string3 = string1;        <span class="comment">// 这样也是不行的, 因为取消了隐式转换, 除非类实现操作符"="的重载</span></span><br></pre></td></tr></table></figure><p>总结：<br>explicit关键字只需用于类内的单参数构造函数前面。由于无参数的构造函数和多参数的构造函数总是显示调用，这种情况在构造函数前加explicit无意义。</p><p>google的c++规范中提到：explicit的优点是可以避免不合时宜的类型变换，缺点无。所以google约定所有单参数的构造函数都必须是显示的，只有极少数情况下拷贝构造函数可以不声明称explicit，例如作为其他类的透明包装器的类。<br>effective c++中说：被声明为explicit的构造函数通常比其non-explicit兄弟更受欢迎。因为它们禁止编译器执行非预期（往往也不被期望）的类型转换。除非我有一个好理由允许构造函数被用于隐式类型转换，否则我会把它声明为explicit，鼓励大家遵循相同的政策。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;C++中的explicit关键字只能用于修饰只有一个参数的类构造函数，它的作用是表明该构造函数是显示的，而非隐式的，跟它相对应的另一个关键字是implicit，意思是隐藏的，类构造函数默认情况下即声明为implicit(隐式)。&lt;/p&gt;</summary>
    
    
    
    <category term="C/C++" scheme="http://yoursite.com/categories/C-C/"/>
    
    
    <category term="C/C++" scheme="http://yoursite.com/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>寻梦环游记</title>
    <link href="http://yoursite.com/2023/03/06/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0/"/>
    <id>http://yoursite.com/2023/03/06/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0/</id>
    <published>2023-03-06T15:53:26.000Z</published>
    <updated>2023-06-10T08:28:18.837Z</updated>
    
    <content type="html"><![CDATA[<p>看了这部早有耳闻的电影，原因可能是开学前给去世的爷爷上坟和最近身体不好的奶奶，但直到前两天我才知道这是部关于死亡的电影。故事可以说是多级反转，不过结局是美好的：一家人在一起，载歌载舞，无论他们是生是死。<a id="more"></a></p><p>不久前的寒假，我回到儿时的故乡。儿时热闹的村庄，早已没有了往日的喧嚣。那时，无论盛夏烈日炎炎，还是冬天白雪皑皑，总有我们一群嬉笑打闹的孩子，大人们忙碌谈笑，老人们家长里短。然而，在我长大的短短二十年间，老人们陆续离去，我们也离开家乡，或是上学或是工作或是在外地买房。这个村庄，只剩一群不愿离开，也离不开的中老年人守护。他们保留着传统的生活习惯，随着时光流逝，最后也会像这可能不知多久后消失的村庄，埋入祖辈的墓地里，隐入历史的尘烟中。</p><p>我生于旧世纪的尾巴，成长在由贫穷迈向小康的转折点，农村和城市的印记都刻在我的脑海里。虽然我现在在大城市学习生活，以后也大概率会在大城市工作生活。然而一直以来，我做梦的场景好像很多时候都是故乡的村庄和老房子，甚至都不是现在翻新的房子，虽然这房子也翻新得有十年了吧。这是什么原因？我一直苦苦思索而不得。可能儿时的我，儿时的村庄才是我潜意识中真正幸福的时光吧，在那个记忆里，我才真正放松和自由。有时我也想，等我老了也回来吧，那时的我也没有了欲望和抱负，坐在宁静的村庄晒着太阳，回忆往事，静静等待死亡，最后叶落归根。</p><p>奶奶身体不好，可能随时离我们而去，爸爸妈妈也步入中年。曾经我在想，他们一生的意义是什么呢？他们没有知识，不知道文化的魅力和宇宙的浩瀚；他们没去过大城市，没见过高楼大厦，去过风景名胜；他们没有以后……后来我想通了，他们的一生在他们眼中未必不是精彩的，他们从乱世中走来，度过苦难，却也见过现代社会的繁荣，我们不必为他们悲哀，因为一代人自有一代人的命运，我们最终也会如此。</p><p>时间缓缓流淌，死是我们的归宿，就像电影中的那样，谁也不能幸免，所以我们不必害怕，但我们现在得认真地活，认真地陪伴家人，认真学习工作，感受人生。这样，在我们离开这个世界时，才能更加坦然，让活着的人记住我们。到那时，在另一个世界的我们和这个世界的人们，虽然肉体无法接触，但灵魂一直相伴，直到最终和我们最亲近的人在另一个世界重逢。</p><p><img src="/2023/03/06/%E5%AF%BB%E6%A2%A6%E7%8E%AF%E6%B8%B8%E8%AE%B0/寻梦环游记.jpg" alt></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;看了这部早有耳闻的电影，原因可能是开学前给去世的爷爷上坟和最近身体不好的奶奶，但直到前两天我才知道这是部关于死亡的电影。故事可以说是多级反转，不过结局是美好的：一家人在一起，载歌载舞，无论他们是生是死。&lt;/p&gt;</summary>
    
    
    
    <category term="电影" scheme="http://yoursite.com/categories/%E7%94%B5%E5%BD%B1/"/>
    
    
    <category term="电影" scheme="http://yoursite.com/tags/%E7%94%B5%E5%BD%B1/"/>
    
  </entry>
  
  <entry>
    <title>大顶堆和小顶堆</title>
    <link href="http://yoursite.com/2023/02/21/%E5%A4%A7%E9%A1%B6%E5%A0%86%E5%92%8C%E5%B0%8F%E9%A1%B6%E5%A0%86/"/>
    <id>http://yoursite.com/2023/02/21/%E5%A4%A7%E9%A1%B6%E5%A0%86%E5%92%8C%E5%B0%8F%E9%A1%B6%E5%A0%86/</id>
    <published>2023-02-21T14:32:32.000Z</published>
    <updated>2023-06-13T03:20:40.707Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-什么是堆、大顶堆和小顶堆"><a href="#1-什么是堆、大顶堆和小顶堆" class="headerlink" title="1. 什么是堆、大顶堆和小顶堆"></a>1. 什么是堆、大顶堆和小顶堆</h2><p>堆是一种非线性结构，可以把堆看作一棵二叉树，也可以看作一个数组，即：堆就是利用完全二叉树的结构来维护的一维数组。</p><p>堆可以分为大顶堆和小顶堆：<br>大顶堆：每个结点的值都大于或等于其左右孩子结点的值。<br>小顶堆：每个结点的值都小于或等于其左右孩子结点的值。<br>用简单的公式来描述一下堆的定义就是：</p><ul><li><p>大顶堆<strong>：</strong>arr[i] &gt;= arr[2i+1] &amp;&amp; arr[i] &gt;= arr[2i+2] </p></li><li><p>小顶堆<strong>：</strong>arr[i] &lt;= arr[2i+1] &amp;&amp; arr[i] &lt;= arr[2i+2]</p></li></ul><p>如果是排序，求升序用大顶堆，求降序用小顶堆。一般我们说 topK 问题，就可以用大顶堆或小顶堆来实现，即最大的 K 个：小顶堆/最小的 K 个：大顶堆。</p><a id="more"></a><h2 id="2-大顶堆的构建过程"><a href="#2-大顶堆的构建过程" class="headerlink" title="2. 大顶堆的构建过程"></a>2. 大顶堆的构建过程</h2><p>大顶堆的构建过程就是<strong>从最后一个非叶子结点开始从下往上调整</strong>。</p><p>最后一个非叶子节点怎么找？这里我们用数组表示待排序序列，则最后一个非叶子结点的位置是：<strong>数组长度/2-1</strong>。假如数组长度为9，则最后一个非叶子结点位置是 9/2-1=3。</p><ol><li>比较当前结点的值和左子树的值，如果当前节点小于左子树的值，就交换当前节点和左子树；<br>交换完后要检查左子树是否满足大顶堆的性质，不满足则重新调整子树结构；</li><li>再比较当前结点的值和右子树的值，如果当前节点小于右子树的值，就交换当前节点和右子树；<br>交换完后要检查右子树是否满足大顶堆的性质，不满足则重新调整子树结构；</li><li>无需交换调整的时候，则大顶堆构建完成。</li></ol><p>画个图理解下，以 [3, 7, 16, 10, 21, 23] 为例：</p><p><img src="/2023/02/21/%E5%A4%A7%E9%A1%B6%E5%A0%86%E5%92%8C%E5%B0%8F%E9%A1%B6%E5%A0%86/2.png" alt></p><h2 id="3-大顶堆的排序过程"><a href="#3-大顶堆的排序过程" class="headerlink" title="3. 大顶堆的排序过程"></a>3. 大顶堆的排序过程</h2><p>将待排序序列构造成一个大顶堆，此时，整个序列的最大值就是堆顶的根节点。将其与末尾元素进行交换，此时末尾就为最大值。然后将剩余n-1个元素重新构造成一个堆，这样会得到n个元素的次小值，如此反复执行，便能得到一个有序序列了。</p><p>该排序过程可以用下面 4 步概括：<br>第 1 步：先 n 个元素的无序序列，构建成大顶堆；<br>第 2 步：将根节点与最后一个元素交换位置，（将最大元素”沉”到数组末端）；<br>第 3 步：交换过后可能不再满足大顶堆的条件，所以需要将剩下的 n-1 个元素重新构建成大顶堆；<br>第 4 步：重复第 2 步、第 3 步直到整个数组排序完成。</p><p>同样以 [3, 7, 16, 10, 21, 23] 为例：</p><p><img src="/2023/02/21/%E5%A4%A7%E9%A1%B6%E5%A0%86%E5%92%8C%E5%B0%8F%E9%A1%B6%E5%A0%86/3.png" alt></p><h2 id="4-程序实例"><a href="#4-程序实例" class="headerlink" title="4. 程序实例"></a>4. 程序实例</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">g1</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> n, <span class="keyword">int</span> i)</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">2</span> * i &lt;= n)&#123;</span><br><span class="line">        <span class="keyword">int</span> j = <span class="number">2</span> * i;</span><br><span class="line">        <span class="keyword">int</span> v = a[j - <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">if</span> (j &lt; n &amp;&amp; v &lt; a[j])&#123;</span><br><span class="line">            v = a[j];</span><br><span class="line">            j += <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (a[i - <span class="number">1</span>] &lt; v)&#123;</span><br><span class="line">            <span class="keyword">int</span> tmp = a[i - <span class="number">1</span>];</span><br><span class="line">            a[i - <span class="number">1</span>] = v;</span><br><span class="line">            a[j - <span class="number">1</span>] = tmp;</span><br><span class="line">            i = j;</span><br><span class="line">        &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">g2</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> n, <span class="keyword">int</span> m)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i = n / <span class="number">2</span>; i &gt; <span class="number">0</span>; --i)</span><br><span class="line">        g1(a, n, i);</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; n &amp;&amp; a[i] != m; ++i);</span><br><span class="line">    <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (++i; i &gt; <span class="number">0</span>; i /= <span class="number">2</span>)</span><br><span class="line">        ++j;</span><br><span class="line">    <span class="keyword">return</span> j;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a[] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="keyword">sizeof</span>(a) / <span class="keyword">sizeof</span>(a[<span class="number">0</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d"</span>, g2(a, n, <span class="number">8</span>));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>g1 函数的作用即为构建大顶堆，代码第28行：当 i = 7 时，a[i] = 8，退出循环，程序执行后控制台输出 j 的值为：4。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-什么是堆、大顶堆和小顶堆&quot;&gt;&lt;a href=&quot;#1-什么是堆、大顶堆和小顶堆&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是堆、大顶堆和小顶堆&quot;&gt;&lt;/a&gt;1. 什么是堆、大顶堆和小顶堆&lt;/h2&gt;&lt;p&gt;堆是一种非线性结构，可以把堆看作一棵二叉树，也可以看作一个数组，即：堆就是利用完全二叉树的结构来维护的一维数组。&lt;/p&gt;
&lt;p&gt;堆可以分为大顶堆和小顶堆：&lt;br&gt;大顶堆：每个结点的值都大于或等于其左右孩子结点的值。&lt;br&gt;小顶堆：每个结点的值都小于或等于其左右孩子结点的值。&lt;br&gt;用简单的公式来描述一下堆的定义就是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;大顶堆&lt;strong&gt;：&lt;/strong&gt;arr[i] &amp;gt;= arr[2i+1] &amp;amp;&amp;amp; arr[i] &amp;gt;= arr[2i+2] &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;小顶堆&lt;strong&gt;：&lt;/strong&gt;arr[i] &amp;lt;= arr[2i+1] &amp;amp;&amp;amp; arr[i] &amp;lt;= arr[2i+2]&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果是排序，求升序用大顶堆，求降序用小顶堆。一般我们说 topK 问题，就可以用大顶堆或小顶堆来实现，即最大的 K 个：小顶堆/最小的 K 个：大顶堆。&lt;/p&gt;</summary>
    
    
    
    <category term="数据结构与算法" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="C/C++" scheme="http://yoursite.com/tags/C-C/"/>
    
    <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode-21.合并两个有序链表</title>
    <link href="http://yoursite.com/2023/01/29/LeetCode-21-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E9%93%BE%E8%A1%A8/"/>
    <id>http://yoursite.com/2023/01/29/LeetCode-21-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E9%93%BE%E8%A1%A8/</id>
    <published>2023-01-29T12:26:08.000Z</published>
    <updated>2023-01-29T12:36:02.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="21-合并两个有序链表（Merge-Two-Sorted-Lists）"><a href="#21-合并两个有序链表（Merge-Two-Sorted-Lists）" class="headerlink" title="21. 合并两个有序链表（Merge Two Sorted Lists）"></a>21. 合并两个有序链表（Merge Two Sorted Lists）</h1><p>将两个升序链表合并为一个新的 <strong>升序</strong> 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">示例1：</span><br><span class="line">输入：l1 &#x3D; [1,2,4], l2 &#x3D; [1,3,4]</span><br><span class="line">输出：[1,1,2,3,4,4]</span><br><span class="line"></span><br><span class="line">示例2：</span><br><span class="line">输入：l1 &#x3D; [], l2 &#x3D; []</span><br><span class="line">输出：[]</span><br><span class="line"></span><br><span class="line">示例3：</span><br><span class="line">输入：l1 &#x3D; [], l2 &#x3D; [0]</span><br><span class="line">输出：[0]</span><br></pre></td></tr></table></figure><p><strong>提示：</strong></p><ul><li>两个链表的节点数目范围是 <code>[0, 50]</code></li><li><code>-100 &lt;= Node.val &lt;= 100</code></li><li><code>l1</code> 和 <code>l2</code> 均按 <strong>非递减顺序</strong> 排列</li></ul><a id="more"></a><h2 id="方法1：递归"><a href="#方法1：递归" class="headerlink" title="方法1：递归"></a>方法1：递归</h2><h3 id="思路与算法"><a href="#思路与算法" class="headerlink" title="思路与算法"></a>思路与算法</h3><p>我们可以如下递归地定义两个链表里的 merge 操作（忽略边界情况，比如空链表等）：</p><p><img src="/2023/01/29/LeetCode-21-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E9%93%BE%E8%A1%A8/递归公式.png" alt></p><p>也就是说，两个链表头部值较小的一个节点与剩下元素的 merge 操作结果合并。</p><p>我们直接将以上递归过程建模，同时需要考虑边界情况。如果 l1 或者 l2 一开始就是空链表 ，那么没有任何操作需要合并，所以我们只需要返回非空链表。否则，我们要判断 l1 和 l2 哪一个链表的头节点的值更小，然后递归地决定下一个添加到结果里的节点。如果两个链表有一个为空，递归结束。</p><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><p><img src="/2023/01/29/LeetCode-21-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E9%93%BE%E8%A1%A8/复杂度分析1.png" alt></p><h3 id="C-解法一"><a href="#C-解法一" class="headerlink" title="C++ 解法一"></a>C++ 解法一</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义链表节点</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> val;    <span class="comment">// 节点值</span></span><br><span class="line">    ListNode *next; <span class="comment">// 节点存储的next节点地址</span></span><br><span class="line">    ListNode() : val(<span class="number">0</span>), next(<span class="literal">nullptr</span>) &#123;&#125;   <span class="comment">// 无参构造函数：节点值为0，next节点地址为空</span></span><br><span class="line">    ListNode(<span class="keyword">int</span> x) : val(x), next(<span class="literal">nullptr</span>) &#123;&#125;  <span class="comment">// 有参构造函数：节点值为x，next节点地址为空</span></span><br><span class="line">    ListNode(<span class="keyword">int</span> x, ListNode *next) : val(x), next(next) &#123;&#125; <span class="comment">// 有参构造函数：节点值为x，next节点地址为next</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">ListNode* <span class="title">createList</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123; <span class="comment">// 使用&amp;：因为无需拷贝nums；使用const：因为无需改变nums中元素的值</span></span><br><span class="line">    <span class="function">ListNode <span class="title">dummy</span><span class="params">(<span class="number">0</span>)</span></span>; <span class="comment">// 定义哑结点（方式一）：在栈上开辟空间，由系统自动分配内存并销毁</span></span><br><span class="line">    <span class="comment">//ListNode* dummy = new ListNode(); // 定义哑结点（方式二）：在堆上开辟空间，手动分配内存并需要手动销毁</span></span><br><span class="line">    <span class="comment">// 为什么在这里要采用方式一：因为后面在销毁链表时，并没有处理哑结点。如果采用方式二，需要在后面处理哑结点。</span></span><br><span class="line">    ListNode* node = &amp;dummy;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">int</span>&amp; n: nums) &#123;</span><br><span class="line">        node-&gt;next = <span class="keyword">new</span> ListNode(n);</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dummy.next;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (head) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; head-&gt;val &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">destroyList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (head) &#123;</span><br><span class="line">        ListNode* delNode = head;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> delNode;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">mergeTwoLists</span><span class="params">(ListNode* l1, ListNode* l2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (l1 == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> l2;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (l2 == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> l1;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (l1-&gt;val &lt; l2-&gt;val) &#123;</span><br><span class="line">            l1-&gt;next = mergeTwoLists(l1-&gt;next, l2);</span><br><span class="line">            <span class="keyword">return</span> l1;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            l2-&gt;next = mergeTwoLists(l1, l2-&gt;next);</span><br><span class="line">            <span class="keyword">return</span> l2;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nums1&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    ListNode* l1 = createList(nums1);</span><br><span class="line">    printList(l1);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nums2&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    ListNode* l2 = createList(nums2);</span><br><span class="line">    printList(l2);</span><br><span class="line"></span><br><span class="line">    Solution s;</span><br><span class="line">    printList(s.mergeTwoLists(l1, l2));</span><br><span class="line"></span><br><span class="line">    destroyList(l1);</span><br><span class="line">    destroyList(l2);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="方法2：迭代"><a href="#方法2：迭代" class="headerlink" title="方法2：迭代"></a>方法2：迭代</h2><h3 id="思路与算法-1"><a href="#思路与算法-1" class="headerlink" title="思路与算法"></a>思路与算法</h3><p>我们可以用迭代的方法来实现上述算法。当 l1 和 l2 都不是空链表时，判断 l1 和 l2 哪一个链表的头节点的值更小，将较小值的节点添加到结果里，当一个节点被添加到结果里之后，将对应链表中的节点向后移一位。</p><p>首先，我们设定一个哨兵节点 prehead ，这可以在最后让我们比较容易地返回合并后的链表。我们维护一个 prev 指针，我们需要做的是调整它的 next 指针。然后，我们重复以下过程，直到 l1 或者 l2 指向了 null ：如果 l1 当前节点的值小于等于 l2 ，我们就把 l1 当前的节点接在 prev 节点的后面同时将 l1 指针往后移一位。否则，我们对 l2 做同样的操作。不管我们将哪一个元素接在了后面，我们都需要把 prev 向后移一位。</p><p>在循环终止的时候， l1 和 l2 至多有一个是非空的。由于输入的两个链表都是有序的，所以不管哪个链表是非空的，它包含的所有元素都比前面已经合并链表中的所有元素都要大。这意味着我们只需要简单地将非空链表接在合并链表的后面，并返回合并链表即可。</p><h3 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><p><img src="/2023/01/29/LeetCode-21-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E9%93%BE%E8%A1%A8/复杂度分析2.png" alt></p><h3 id="C-解法二"><a href="#C-解法二" class="headerlink" title="C++ 解法二"></a>C++ 解法二</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义链表节点</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> val;    <span class="comment">// 节点值</span></span><br><span class="line">    ListNode *next; <span class="comment">// 节点存储的next节点地址</span></span><br><span class="line">    ListNode() : val(<span class="number">0</span>), next(<span class="literal">nullptr</span>) &#123;&#125;   <span class="comment">// 无参构造函数：节点值为0，next节点地址为空</span></span><br><span class="line">    ListNode(<span class="keyword">int</span> x) : val(x), next(<span class="literal">nullptr</span>) &#123;&#125;  <span class="comment">// 有参构造函数：节点值为x，next节点地址为空</span></span><br><span class="line">    ListNode(<span class="keyword">int</span> x, ListNode *next) : val(x), next(next) &#123;&#125; <span class="comment">// 有参构造函数：节点值为x，next节点地址为next</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">ListNode* <span class="title">createList</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123; <span class="comment">// 使用&amp;：因为无需拷贝nums；使用const：因为无需改变nums中元素的值</span></span><br><span class="line">    <span class="function">ListNode <span class="title">dummy</span><span class="params">(<span class="number">0</span>)</span></span>; <span class="comment">// 定义哑结点（方式一）：在栈上开辟空间，由系统自动分配内存并销毁</span></span><br><span class="line">    <span class="comment">//ListNode* dummy = new ListNode(); // 定义哑结点（方式二）：在堆上开辟空间，手动分配内存并需要手动销毁</span></span><br><span class="line">    <span class="comment">// 为什么在这里要采用方式一：因为后面在销毁链表时，并没有处理哑结点。如果采用方式二，需要在后面处理哑结点。</span></span><br><span class="line">    ListNode* node = &amp;dummy;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">int</span>&amp; n: nums) &#123;</span><br><span class="line">        node-&gt;next = <span class="keyword">new</span> ListNode(n);</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dummy.next;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (head) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; head-&gt;val &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">destroyList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (head) &#123;</span><br><span class="line">        ListNode* delNode = head;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> delNode;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">mergeTwoLists</span><span class="params">(ListNode* l1, ListNode* l2)</span> </span>&#123;</span><br><span class="line">        ListNode* preHead = <span class="keyword">new</span> ListNode(<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line">        ListNode* prev = preHead;</span><br><span class="line">        <span class="keyword">while</span> (l1 != <span class="literal">nullptr</span> &amp;&amp; l2 != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (l1-&gt;val &lt; l2-&gt;val) &#123;</span><br><span class="line">                prev-&gt;next = l1;</span><br><span class="line">                l1 = l1-&gt;next;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                prev-&gt;next = l2;</span><br><span class="line">                l2 = l2-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            prev = prev-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并后 l1 和 l2 最多只有一个还未被合并完，我们直接将链表末尾指向未合并完的链表即可</span></span><br><span class="line">        prev-&gt;next = l1 == <span class="literal">nullptr</span> ? l2 : l1;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> preHead-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nums1&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    ListNode* l1 = createList(nums1);</span><br><span class="line">    printList(l1);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; nums2&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    ListNode* l2 = createList(nums2);</span><br><span class="line">    printList(l2);</span><br><span class="line"></span><br><span class="line">    Solution s;</span><br><span class="line">    printList(s.mergeTwoLists(l1, l2));</span><br><span class="line"></span><br><span class="line">    destroyList(l1);</span><br><span class="line">    destroyList(l2);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;21-合并两个有序链表（Merge-Two-Sorted-Lists）&quot;&gt;&lt;a href=&quot;#21-合并两个有序链表（Merge-Two-Sorted-Lists）&quot; class=&quot;headerlink&quot; title=&quot;21. 合并两个有序链表（Merge Two Sorted Lists）&quot;&gt;&lt;/a&gt;21. 合并两个有序链表（Merge Two Sorted Lists）&lt;/h1&gt;&lt;p&gt;将两个升序链表合并为一个新的 &lt;strong&gt;升序&lt;/strong&gt; 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 &lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;示例1：&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;输入：l1 &amp;#x3D; [1,2,4], l2 &amp;#x3D; [1,3,4]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;输出：[1,1,2,3,4,4]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;示例2：&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;输入：l1 &amp;#x3D; [], l2 &amp;#x3D; []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;输出：[]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;示例3：&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;输入：l1 &amp;#x3D; [], l2 &amp;#x3D; [0]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;输出：[0]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;提示：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;两个链表的节点数目范围是 &lt;code&gt;[0, 50]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-100 &amp;lt;= Node.val &amp;lt;= 100&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;l1&lt;/code&gt; 和 &lt;code&gt;l2&lt;/code&gt; 均按 &lt;strong&gt;非递减顺序&lt;/strong&gt; 排列&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="LeetCode" scheme="http://yoursite.com/categories/LeetCode/"/>
    
    
    <category term="LeetCode" scheme="http://yoursite.com/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>图像归一化</title>
    <link href="http://yoursite.com/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <id>http://yoursite.com/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/</id>
    <published>2023-01-09T01:17:09.000Z</published>
    <updated>2023-06-11T06:12:48.275Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-图像归一化"><a href="#1-图像归一化" class="headerlink" title="1. 图像归一化"></a>1. 图像归一化</h2><p>图像归一化是指对图像进行了一系列标准的处理变换，使之变换为一固定标准形式的过程，该标准图像称作归一化图像。</p><p>在机器学习中，不同评价指标（即特征向量中的不同特征，就是所述的不同评价指标）往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果。为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。其中，最典型的就是数据的归一化处理。简而言之，归一化的目的就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响。</p><p>在深度学习中，通常在模型训练前都会对图像进行归一化处理，而对图像进行归一化处理是将特征值大小调整到相近的范围，不归一化处理时，如果特征值较大时，梯度值也会较大，特征值较小时，梯度值也会较小。在模型反向传播时，梯度值更新与学习率一样，当学习率较小时，梯度值较小会导致更新缓慢，当学习率较大时，梯度值较大会导致模型不易收敛，因此为了使模型训练收敛平稳，对图像进行归一化操作，把不同维度的特征值调整到相近的范围内，就可以采用统一的学习率加速模型训练。<a id="more"></a></p><h2 id="2-图像归一化的常用方法及Python应用"><a href="#2-图像归一化的常用方法及Python应用" class="headerlink" title="2. 图像归一化的常用方法及Python应用"></a>2. 图像归一化的常用方法及Python应用</h2><h3 id="2-1-Min-Max归一化"><a href="#2-1-Min-Max归一化" class="headerlink" title="2.1 Min-Max归一化"></a>2.1 Min-Max归一化</h3><p>通过遍历图像矩阵中的每一个像素，设定max和min，进行数据的归一化处理，公式如下：</p><script type="math/tex; mode=display">x'=(x-min⁡(x))/(max⁡(x)-min⁡(x))</script><p>（1）线性函数将原始数据用线性化的方法转换到[0,1]的范围，计算结果x’为归一化后的数据，x为原始数据。（2）Min-Max归一化方法比较适用在数值比较集中的情况。<br>（3）缺点：如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量来替代max和min。</p><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">h,w = gray.shape</span><br><span class="line">mn = np.min(gray)</span><br><span class="line">mx = np.max(gray)</span><br><span class="line">norm = np.zeros((h,w),dtype=np.float32) <span class="comment"># 自定义空白单通道图像，用于存放归一化图像</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(h):</span><br><span class="line">   <span class="keyword">for</span> j <span class="keyword">in</span> range(w):</span><br><span class="line">       norm[i,j] = (gray[i,j] - mn) / (mx - mn)</span><br><span class="line">       <span class="comment">#norm[i,j] = gray[i,j] / 255</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'归一化前：'</span>)</span><br><span class="line">print(gray)</span><br><span class="line">print(<span class="string">'归一化后：'</span>)</span><br><span class="line">print(norm)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(gray, <span class="string">'gray'</span>), plt.title(<span class="string">'gray'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(norm, <span class="string">'gray'</span>), plt.title(<span class="string">'normalization'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>归一化前后灰度图像矩阵如下所示：</p><p> <img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-1.png" alt></p><p>归一化前后灰度图像对比如下所示：</p><p><img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-2.png" alt></p><h3 id="2-2-z-score标准化"><a href="#2-2-z-score标准化" class="headerlink" title="2.2 z-score标准化"></a>2.2 z-score标准化</h3><p>z-score标准化公式如下：</p><script type="math/tex; mode=display">x'=(x-μ)/σ</script><p>其中，μ、σ分别为原始数据集的均值和方法。<br>（1）将原始数据集归一化为均值为0、方差1的数据集。<br>（2）该种归一化方式要求原始数据的分布可以近似为高斯分布，否则归一化的效果会变得很糟糕。<br>（3）应用场景：在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，z-score standardization表现更好。</p><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">h,w = gray.shape</span><br><span class="line">x_mean = np.mean(gray)</span><br><span class="line">vari = np.sqrt((np.sum((gray-x_mean)**<span class="number">2</span>))/(h*w))</span><br><span class="line">norm = np.zeros((h,w),dtype=np.float32) <span class="comment"># 自定义空白单通道图像，用于存放归一化图像</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(h):</span><br><span class="line">   <span class="keyword">for</span> j <span class="keyword">in</span> range(w):</span><br><span class="line">       norm[i,j] = (gray[i,j] - x_mean) / vari</span><br><span class="line">       <span class="comment">#norm[i,j] = gray[i,j] / 127.5 - 1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'归一化前：'</span>)</span><br><span class="line">print(gray)</span><br><span class="line">print(<span class="string">'归一化后：'</span>)</span><br><span class="line">print(norm)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(gray, <span class="string">'gray'</span>), plt.title(<span class="string">'gray'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(norm, <span class="string">'gray'</span>), plt.title(<span class="string">'normalization'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>归一化前后灰度图像矩阵如下所示：</p><p> <img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-3.png" alt></p><p>归一化前后灰度图像对比如下所示：</p><p> <img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-4.png" alt></p><h3 id="2-3-神经网络归一化"><a href="#2-3-神经网络归一化" class="headerlink" title="2.3 神经网络归一化"></a>2.3 神经网络归一化</h3><p>该归一化方法经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括log，反正切等，需要根据数据分布的情况，决定非线性函数的曲线。</p><h4 id="2-3-1-log对数函数归一化"><a href="#2-3-1-log对数函数归一化" class="headerlink" title="2.3.1 log对数函数归一化"></a>2.3.1 log对数函数归一化</h4><p>y = log10(x)，即以10为底的对数转换函数，对应的归一化方法为：</p><script type="math/tex; mode=display">x' = log10(x)/log10(max)</script><p>其中max表示样本数据的最大值，并且所有样本数据均要大于等于1。</p><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">h,w = gray.shape</span><br><span class="line">norm = np.zeros((h,w),dtype=np.float32) <span class="comment"># 自定义空白单通道图像，用于存放归一化图像</span></span><br><span class="line">norm = np.log10(gray) / np.log10(gray.max())</span><br><span class="line"></span><br><span class="line">print(<span class="string">'归一化前：'</span>)</span><br><span class="line">print(gray)</span><br><span class="line">print(<span class="string">'归一化后：'</span>)</span><br><span class="line">print(norm)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(gray, <span class="string">'gray'</span>), plt.title(<span class="string">'gray'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(norm, <span class="string">'gray'</span>), plt.title(<span class="string">'normalization'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>归一化前后灰度图像矩阵如下所示：</p><p> <img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-5.png" alt></p><p>归一化前后灰度图像对比如下所示：</p><p><img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-6.png" alt></p><h4 id="2-3-2-反正切函数归一化"><a href="#2-3-2-反正切函数归一化" class="headerlink" title="2.3.2 反正切函数归一化"></a>2.3.2 反正切函数归一化</h4><p>对应的归一化方法为：x’ = atan(x)*(2/pi)</p><p>使用这个方法需要注意的是如果想映射的区间为[0,1]，则数据都应该大于等于0，小于0的数据将被映射到[－1,0]区间上。</p><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">h,w = gray.shape</span><br><span class="line">norm = np.zeros((h,w),dtype=np.float32) <span class="comment"># 自定义空白单通道图像，用于存放归一化图像</span></span><br><span class="line">norm = np.arctan(gray) * (<span class="number">2</span> / np.pi)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'归一化前：'</span>)</span><br><span class="line">print(gray)</span><br><span class="line">print(<span class="string">'归一化后：'</span>)</span><br><span class="line">print(norm)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(gray, <span class="string">'gray'</span>), plt.title(<span class="string">'gray'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(norm, <span class="string">'gray'</span>), plt.title(<span class="string">'normalization'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>归一化前后灰度图像矩阵如下所示：</p><p><img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-7.png" alt></p><p>归一化前后灰度图像对比如下所示：</p><p><img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-8.png" alt></p><h3 id="2-4-L2范数归一化"><a href="#2-4-L2范数归一化" class="headerlink" title="2.4 L2范数归一化"></a>2.4 L2范数归一化</h3><p>定义：特征向量中每个元素均除以向量的范数，即如下公式：</p><script type="math/tex; mode=display">x_i'=x_i/(norm(x))</script><p>向量x(x1,x2,…,xn)的L2范数定义为：</p><script type="math/tex; mode=display">norm(x)=\sqrt(x_1^2+x_2^2+⋯+x_n^2)</script><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">h,w = gray.shape</span><br><span class="line">norm = np.zeros((h,w),dtype=np.float32) <span class="comment"># 自定义空白单通道图像，用于存放归一化图像</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(h):</span><br><span class="line">   <span class="keyword">for</span> j <span class="keyword">in</span> range(w):</span><br><span class="line">       norm_x = <span class="number">0.0</span> + gray[i,j]**<span class="number">2</span></span><br><span class="line">norm_x = np.sqrt(norm_x)</span><br><span class="line">norm = gray / norm_x</span><br><span class="line"></span><br><span class="line">print(<span class="string">'归一化前：'</span>)</span><br><span class="line">print(gray)</span><br><span class="line">print(<span class="string">'归一化后：'</span>)</span><br><span class="line">print(norm)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(gray, <span class="string">'gray'</span>), plt.title(<span class="string">'gray'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(norm, <span class="string">'gray'</span>), plt.title(<span class="string">'normalization'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>归一化前后灰度图像矩阵如下所示：</p><p> <img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-9.png" alt></p><p>归一化前后灰度图像对比如下所示：</p><p><img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-10.png" alt></p><h2 id="3-opencv-python中归一化方法的应用"><a href="#3-opencv-python中归一化方法的应用" class="headerlink" title="3. opencv-python中归一化方法的应用"></a>3. opencv-python中归一化方法的应用</h2><p>opencv-python中使用cv2.normalize()函数实现归一化，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.normalize(src[, dst[, alpha[, beta[, norm_type[, dtype[, mask]]]]]]) → dst</span><br></pre></td></tr></table></figure><p>参数说明：<br>src：输入数组；<br>dst：输出数组，数组的大小和原数组一致；<br>alpha：1.用来规范值。2.规范范围，并且是下限；<br>beta：只用来规范范围并且是上限；<br>norm_type：归一化选择的数学公式类型；<br>dtype：当为负，输出在大小深度通道数都等于输入，当为正，输出只在深度与输入不同，不同的地方由dtype决定；<br>mark：掩码。选择感兴趣区域，选定后只能对该区域进行操作。</p><p>归一化选择的数学公式类型有如下几种：<br>NORM_MINMAX：数组的数值被平移或缩放到一个指定的范围，线性归一化，一般较常用；<br>NORM_INF：矩阵中绝对值的最大值；<br>NORM_L1：归一化数组的L1-范数(绝对值的和)；<br>NORM_L2：归一化数组的(欧几里德)L2-范数。</p><p>测试代码如下（以NORM_MINMAX为例）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)</span><br><span class="line">h,w = gray.shape</span><br><span class="line">norm = np.zeros((h,w),dtype=np.float32) <span class="comment"># 自定义空白单通道图像，用于存放归一化图像</span></span><br><span class="line">cv.normalize(gray, norm, alpha=<span class="number">0</span>, beta=<span class="number">1</span>, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)</span><br><span class="line"></span><br><span class="line"><span class="comment"># norm = np.uint8(norm*255.0)</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'归一化前：'</span>)</span><br><span class="line">print(gray)</span><br><span class="line">print(<span class="string">'归一化后：'</span>)</span><br><span class="line">print(norm)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(gray, <span class="string">'gray'</span>), plt.title(<span class="string">'gray'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(norm, <span class="string">'gray'</span>), plt.title(<span class="string">'normalization'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>归一化前后灰度图像矩阵如下所示，可以发现与2.1节Min-Max归一化处理结果一致：</p><p><img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-11.png" alt></p><p>归一化前后灰度图像对比如下所示：</p><p> <img src="/2023/01/09/%E5%9B%BE%E5%83%8F%E5%BD%92%E4%B8%80%E5%8C%96/2-12.png" alt></p><h2 id="4-源码仓库地址"><a href="#4-源码仓库地址" class="headerlink" title="4. 源码仓库地址"></a>4. 源码仓库地址</h2><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-图像归一化&quot;&gt;&lt;a href=&quot;#1-图像归一化&quot; class=&quot;headerlink&quot; title=&quot;1. 图像归一化&quot;&gt;&lt;/a&gt;1. 图像归一化&lt;/h2&gt;&lt;p&gt;图像归一化是指对图像进行了一系列标准的处理变换，使之变换为一固定标准形式的过程，该标准图像称作归一化图像。&lt;/p&gt;
&lt;p&gt;在机器学习中，不同评价指标（即特征向量中的不同特征，就是所述的不同评价指标）往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果。为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。其中，最典型的就是数据的归一化处理。简而言之，归一化的目的就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响。&lt;/p&gt;
&lt;p&gt;在深度学习中，通常在模型训练前都会对图像进行归一化处理，而对图像进行归一化处理是将特征值大小调整到相近的范围，不归一化处理时，如果特征值较大时，梯度值也会较大，特征值较小时，梯度值也会较小。在模型反向传播时，梯度值更新与学习率一样，当学习率较小时，梯度值较小会导致更新缓慢，当学习率较大时，梯度值较大会导致模型不易收敛，因此为了使模型训练收敛平稳，对图像进行归一化操作，把不同维度的特征值调整到相近的范围内，就可以采用统一的学习率加速模型训练。&lt;/p&gt;</summary>
    
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>直方图均衡化</title>
    <link href="http://yoursite.com/2023/01/08/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/"/>
    <id>http://yoursite.com/2023/01/08/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/</id>
    <published>2023-01-08T12:53:39.000Z</published>
    <updated>2023-06-10T08:27:54.698Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-图像直方图"><a href="#1-图像直方图" class="headerlink" title="1. 图像直方图"></a>1. 图像直方图</h1><p>图像直方图，是指对整个图像在灰度范围内的像素值(0-255)统计出现频率次数，据此生成的直方图，称为图像直方图或直方图。直方图反映了图像灰度的分布情况，是图像的统计学特征。也可以说，直方图是图像中像素强度分布的图形表达方式，它统计了每一个强度值所具有的像素个数。</p><h1 id="2-直方图均衡化"><a href="#2-直方图均衡化" class="headerlink" title="2. 直方图均衡化"></a>2. 直方图均衡化</h1><p>直方图均衡化是以累计分布函数为核心，将原始图像灰度直方图从比较集中的某个灰度区间，非线性地映射为在全部灰度范围内的较均匀分布，从而增强对比度。</p><a id="more"></a><p>直方图均衡化的数学原理如下：<br>首先作原始图像灰度的概率直方图, 然后设输入像素灰度值为r<sub>k</sub>，累计分布函数为</p><p><img src="/2023/01/08/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/2-1.png" alt></p><p>其中n<sub>i</sub>为图像中灰度值为r<sub>i</sub>的像素频数，n为图像像素总数。设输出像素灰度值为s<sub>k</sub>，像素范围为s<sub>min</sub>-s<sub>max</sub>。期望输出灰度直方图是均匀分布，即</p><p> <img src="/2023/01/08/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/2-2.png" alt></p><p>令C(s<sub>k</sub>)=C(r<sub>k</sub>)，即得</p><p> <img src="/2023/01/08/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/2-3.png" alt></p><p>所以最终直方图均衡化的点算子为</p><p> <img src="/2023/01/08/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/2-4.png" alt></p><h1 id="3-直方图均衡化的Python实现"><a href="#3-直方图均衡化的Python实现" class="headerlink" title="3. 直方图均衡化的Python实现"></a>3. 直方图均衡化的Python实现</h1><p>根据直方图均衡化的数学原理，用Python实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算累计分布函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">C</span><span class="params">(rk)</span>:</span></span><br><span class="line">    <span class="comment"># 读取图片灰度直方图</span></span><br><span class="line">    <span class="comment"># bins为直方图直方柱的取值向量，hist为bins各取值区间上的频数取值</span></span><br><span class="line">    hist, bins = np.histogram(rk, <span class="number">256</span>, [<span class="number">0</span>, <span class="number">256</span>])</span><br><span class="line">    <span class="comment"># 计算累计分布函数</span></span><br><span class="line">    <span class="keyword">return</span> hist.cumsum()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算灰度均衡化映射</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">T</span><span class="params">(rk)</span>:</span></span><br><span class="line">    cdf = C(rk)</span><br><span class="line">    <span class="comment"># 均衡化</span></span><br><span class="line">    cdf = (cdf - cdf.min()) * (<span class="number">255</span> - <span class="number">0</span>) / (cdf.max() - cdf.min()) + <span class="number">0</span></span><br><span class="line">    <span class="comment">#cdf = 255.0 * cdf / cdf[-1]</span></span><br><span class="line">    <span class="keyword">return</span> cdf.astype(<span class="string">'uint8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line">img = cv.imread(<span class="string">'lenna.jpg'</span>, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 将二维数字图像矩阵转变为一维向量</span></span><br><span class="line">rk = img.flatten()</span><br><span class="line"><span class="comment"># 原始图像灰度直方图</span></span><br><span class="line">plt.hist(rk, <span class="number">256</span>, [<span class="number">0</span>, <span class="number">255</span>], color = <span class="string">'r'</span>)</span><br><span class="line"><span class="comment"># 直方图均衡化</span></span><br><span class="line">imgDst = T(rk)[img]     </span><br><span class="line">plt.hist(imgDst.flatten(), <span class="number">256</span>, [<span class="number">0</span>, <span class="number">255</span>], color = <span class="string">'b'</span>)</span><br><span class="line">plt.legend([<span class="string">'Before Equalization'</span>,<span class="string">'Equalization'</span>]) </span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 展示前后对比图像</span></span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(img, cmap=<span class="string">'gray'</span>), plt.title(<span class="string">'Original Gray'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(imgDst, cmap=<span class="string">'gray'</span>), plt.title(<span class="string">'Histogram Equalization'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>均衡前后图像灰度直方图如下所示：</p><p> <img src="/2023/01/08/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/3-1.png" alt></p><p>直方图均衡化前后的图像对比如下所示：</p><p><img src="/2023/01/08/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/3-2.png" alt></p><h1 id="4-opencv-python中直方图均衡化的应用"><a href="#4-opencv-python中直方图均衡化的应用" class="headerlink" title="4. opencv-python中直方图均衡化的应用"></a>4. opencv-python中直方图均衡化的应用</h1><p>opencv-python中使用cv.equalizeHist函数即可实现直方图均衡化，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dst=cv.equalizeHist(src[, dst])</span><br></pre></td></tr></table></figure><blockquote><p>注意：输入需是灰度图像。</p></blockquote><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img = cv.imread(<span class="string">'lenna.jpg'</span>, <span class="number">0</span>)</span><br><span class="line">dst = cv.equalizeHist(img)</span><br><span class="line">plt.subplot(<span class="number">121</span>), plt.imshow(img, cmap=<span class="string">'gray'</span>), plt.title(<span class="string">'Original Gray'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>), plt.imshow(dst, cmap=<span class="string">'gray'</span>), plt.title(<span class="string">'Histogram Equalization'</span>), plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p> <img src="/2023/01/08/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/4.png" alt></p><h1 id="源码仓库地址"><a href="#源码仓库地址" class="headerlink" title="源码仓库地址"></a>源码仓库地址</h1><p>🌼<a href="https://github.com/crossoverpptx/ImageProcessing" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-图像直方图&quot;&gt;&lt;a href=&quot;#1-图像直方图&quot; class=&quot;headerlink&quot; title=&quot;1. 图像直方图&quot;&gt;&lt;/a&gt;1. 图像直方图&lt;/h1&gt;&lt;p&gt;图像直方图，是指对整个图像在灰度范围内的像素值(0-255)统计出现频率次数，据此生成的直方图，称为图像直方图或直方图。直方图反映了图像灰度的分布情况，是图像的统计学特征。也可以说，直方图是图像中像素强度分布的图形表达方式，它统计了每一个强度值所具有的像素个数。&lt;/p&gt;
&lt;h1 id=&quot;2-直方图均衡化&quot;&gt;&lt;a href=&quot;#2-直方图均衡化&quot; class=&quot;headerlink&quot; title=&quot;2. 直方图均衡化&quot;&gt;&lt;/a&gt;2. 直方图均衡化&lt;/h1&gt;&lt;p&gt;直方图均衡化是以累计分布函数为核心，将原始图像灰度直方图从比较集中的某个灰度区间，非线性地映射为在全部灰度范围内的较均匀分布，从而增强对比度。&lt;/p&gt;</summary>
    
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>图像缩放、旋转、翻转、平移</title>
    <link href="http://yoursite.com/2023/01/07/%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E3%80%81%E6%97%8B%E8%BD%AC%E3%80%81%E7%BF%BB%E8%BD%AC%E3%80%81%E5%B9%B3%E7%A7%BB/"/>
    <id>http://yoursite.com/2023/01/07/%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E3%80%81%E6%97%8B%E8%BD%AC%E3%80%81%E7%BF%BB%E8%BD%AC%E3%80%81%E5%B9%B3%E7%A7%BB/</id>
    <published>2023-01-07T05:05:28.000Z</published>
    <updated>2023-06-11T06:13:29.430Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍几种常见的图像几何变换方法。<a id="more"></a></p><h1 id="1-图像缩放"><a href="#1-图像缩放" class="headerlink" title="1. 图像缩放"></a>1. 图像缩放</h1><p>图像缩放就是将源图像中的像素点经过算法映射到目标图像的像素点的过程，即找出目标图像中的像素点Pd(Xd，Yd)对应的源图像的像素点Ps(Xs，Ys)，然后将源图像像素点填充到对应目标图像的像素点，最终形成目标图像。常见的图像缩放算法有最邻近点插值法、双线性插值法和BiCubic卷积插值法等。</p><p>在OpenCV中提供函数cv2.resize()实现对图像的缩放，该函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dst = cv2.resize( src, dsize[, fx[, fy[, interpolation]]] )</span><br></pre></td></tr></table></figure><p>参数说明：<br>dst：输出的目标图像，其类型与src相同，大小为dsize（当该值非零时），或者可以通过src.size()、fx、fy计算得到；<br>src：需要进行缩放的原始图像；<br>dsize：输出图像的大小；<br>x：水平方向的缩放比例；<br>y：垂直方向的缩放比例；<br>interpolation：插值方式。</p><p>插值是指图像在进行几何处理时，给无法直接通过映射得到的值的像素点赋值。比如，将原始图像放大为原来的2倍，必然会多出一些无法被直接映射值的像素点，对于这些像素点，插值方式决定了如何确定它们的值。当缩小图像时，使用区域插值方式（INTER_AREA）能够得到最好的效果；当放大图像时，使用三次样条插值（INTER_CUBIC）方式和双线性插值（INTER_LINEAR）方式都能够取得较好的效果。三次样条插值方式速度较慢，双线性插值方式速度相对较快且效果并不逊色。</p><p> 测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">"lenna.jpg"</span>)</span><br><span class="line">height, width = img.shape[:<span class="number">2</span>]  <span class="comment"># 获取图像的高度和宽度</span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>, img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 缩放到原来的二分之一</span></span><br><span class="line">img_test1 = cv2.resize(img, (int(height / <span class="number">2</span>), int(width / <span class="number">2</span>)))</span><br><span class="line">cv2.imshow(<span class="string">'resize1'</span>, img_test1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最近邻插值法缩放，缩放到原来的四分之一</span></span><br><span class="line">img_test2 = cv2.resize(img, (<span class="number">0</span>, <span class="number">0</span>), fx=<span class="number">0.25</span>, fy=<span class="number">0.25</span>, interpolation=cv2.INTER_NEAREST)</span><br><span class="line">cv2.imshow(<span class="string">'resize2'</span>, img_test2)</span><br><span class="line">cv2.waitKey()</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p> <img src="/2023/01/07/%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E3%80%81%E6%97%8B%E8%BD%AC%E3%80%81%E7%BF%BB%E8%BD%AC%E3%80%81%E5%B9%B3%E7%A7%BB/1.png" alt></p><h1 id="2-图像旋转"><a href="#2-图像旋转" class="headerlink" title="2. 图像旋转"></a>2. 图像旋转</h1><p>图像旋转是指将图像绕某个中心点旋转一定角度后，得到一幅新的图像。</p><p>Python中imutils工具包提供了如下函数实现图像旋转：<br>imutils.rotate：实现了在旋转完成图分辨率不调整的情况下，对原图像内容旋转，可能局部丢失，缺失部分用黑色填充。<br>imutils.rotate_bound：保持原图完整，旋转完成图分辨率会改变。</p><p>测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">import</span> imutils</span><br><span class="line">img = cv2.imread(<span class="string">"lenna.jpg"</span>)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>, img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 旋转45度，可能局部丢失，缺失部分用黑色填充</span></span><br><span class="line"></span><br><span class="line">rot1 = imutils.rotate(img, angle=<span class="number">45</span>)</span><br><span class="line">cv2.imshow(<span class="string">"Rotated1"</span>, rot1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 旋转45度，保持原图完整，旋转完成图分辨率会改变</span></span><br><span class="line"></span><br><span class="line">rot2 = imutils.rotate_bound(img, angle=<span class="number">45</span>)</span><br><span class="line">cv2.imshow(<span class="string">"Rotated2"</span>, rot2)</span><br><span class="line">cv2.waitKey()</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p> <img src="/2023/01/07/%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E3%80%81%E6%97%8B%E8%BD%AC%E3%80%81%E7%BF%BB%E8%BD%AC%E3%80%81%E5%B9%B3%E7%A7%BB/2.png" alt></p><h1 id="3-图像翻转"><a href="#3-图像翻转" class="headerlink" title="3. 图像翻转"></a>3. 图像翻转</h1><p>翻转也称镜像，是指将图像沿轴线进行轴对称变换。水平镜像是将图像沿垂直中轴线进行左右翻转，垂直镜像是将图像沿水平中轴线进行上下翻转，水平垂直镜像是水平镜像和垂直镜像的叠加。</p><p>OpenCV提供了cv2.flip函数，可以将图像沿水平方向、垂直方向、或水平/垂直方向同时进行翻转。其函数原型如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.flip(src, flipCode[, dst]) -&gt; dst</span><br></pre></td></tr></table></figure><p>参数说明：<br>scr：变换操作的输入图像；<br>flipCode：控制参数，整型（int），flipCode&gt;0 水平翻转，flipCode=0 垂直翻转，flipCode&lt;0 水平和垂直翻转；<br>dst：变换操作的输出图像，可选项。</p><p> 测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">"lenna.jpg"</span>)  <span class="comment"># 读取彩色图像(BGR)</span></span><br><span class="line"></span><br><span class="line">imgFlip1 = cv2.flip(img, <span class="number">0</span>)  <span class="comment"># 垂直翻转</span></span><br><span class="line">imgFlip2 = cv2.flip(img, <span class="number">1</span>)  <span class="comment"># 水平翻转</span></span><br><span class="line">imgFlip3 = cv2.flip(img, <span class="number">-1</span>)  <span class="comment"># 水平和垂直翻转</span></span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">221</span>), plt.axis(<span class="string">'off'</span>), plt.title(<span class="string">"Original"</span>)</span><br><span class="line">plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  <span class="comment"># 原始图像</span></span><br><span class="line">plt.subplot(<span class="number">222</span>), plt.axis(<span class="string">'off'</span>), plt.title(<span class="string">"Flipped Horizontally"</span>)</span><br><span class="line">plt.imshow(cv2.cvtColor(imgFlip2, cv2.COLOR_BGR2RGB))  <span class="comment"># 水平翻转</span></span><br><span class="line">plt.subplot(<span class="number">223</span>), plt.axis(<span class="string">'off'</span>), plt.title(<span class="string">"Flipped Vertically"</span>)</span><br><span class="line">plt.imshow(cv2.cvtColor(imgFlip1, cv2.COLOR_BGR2RGB))  <span class="comment"># 垂直翻转</span></span><br><span class="line">plt.subplot(<span class="number">224</span>), plt.axis(<span class="string">'off'</span>), plt.title(<span class="string">"Flipped Horizontally &amp; Vertically"</span>)</span><br><span class="line">plt.imshow(cv2.cvtColor(imgFlip3, cv2.COLOR_BGR2RGB))  <span class="comment"># 水平垂直翻转</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p> <img src="/2023/01/07/%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E3%80%81%E6%97%8B%E8%BD%AC%E3%80%81%E7%BF%BB%E8%BD%AC%E3%80%81%E5%B9%B3%E7%A7%BB/3.png" alt></p><h2 id="4-图像平移"><a href="#4-图像平移" class="headerlink" title="4. 图像平移"></a>4. 图像平移</h2><p>图像的平移就是将图像上的像素点整体移动。图像平移首先定义平移矩阵M，再调用warpAffine()函数实现平移，函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">M = np.float32([[<span class="number">1</span>, <span class="number">0</span>, x], [<span class="number">0</span>, <span class="number">1</span>, y]])</span><br><span class="line">shifted = cv2.warpAffine(image, M, (image.shape[<span class="number">1</span>], image.shape[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><p> 测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'lenna.jpg'</span>)</span><br><span class="line">image = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像下、上、右、左平移</span></span><br><span class="line"></span><br><span class="line">M = np.float32([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>]])</span><br><span class="line">img1 = cv2.warpAffine(image, M, (image.shape[<span class="number">1</span>], image.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">M = np.float32([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">-100</span>]])</span><br><span class="line">img2 = cv2.warpAffine(image, M, (image.shape[<span class="number">1</span>], image.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">M = np.float32([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">100</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">img3 = cv2.warpAffine(image, M, (image.shape[<span class="number">1</span>], image.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">M = np.float32([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">-100</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">img4 = cv2.warpAffine(image, M, (image.shape[<span class="number">1</span>], image.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图形</span></span><br><span class="line"></span><br><span class="line">titles = [ <span class="string">'Image-down'</span>, <span class="string">'Image-up'</span>, <span class="string">'Image-right'</span>, <span class="string">'Image-left'</span>]  </span><br><span class="line">images = [img1, img2, img3, img4]  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):  </span><br><span class="line">   plt.subplot(<span class="number">2</span>,<span class="number">2</span>,i+<span class="number">1</span>), plt.imshow(images[i]), plt.title(titles[i])  </span><br><span class="line">   plt.xticks([]),plt.yticks([])  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p> 效果如下：</p><p> <img src="/2023/01/07/%E5%9B%BE%E5%83%8F%E7%BC%A9%E6%94%BE%E3%80%81%E6%97%8B%E8%BD%AC%E3%80%81%E7%BF%BB%E8%BD%AC%E3%80%81%E5%B9%B3%E7%A7%BB/4.png" alt></p><h1 id="4-源码仓库地址"><a href="#4-源码仓库地址" class="headerlink" title="4. 源码仓库地址"></a>4. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文介绍几种常见的图像几何变换方法。&lt;/p&gt;</summary>
    
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>图像边缘检测</title>
    <link href="http://yoursite.com/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/</id>
    <published>2023-01-06T01:02:17.000Z</published>
    <updated>2023-06-11T06:12:11.341Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-什么是边缘检测"><a href="#1-什么是边缘检测" class="headerlink" title="1. 什么是边缘检测"></a>1. 什么是边缘检测</h1><p>边缘检测是图像处理与计算机视觉中的重要技术之一。其目的是检测识别出图像中亮度变化剧烈的像素点构成的集合。图像边缘的正确检测对于分析图像中的内容、实现图像中物体的分割、定位等具有重要的作用。边缘检测大大减少了源图像的数据量，剔除了与目标不相干的信息，保留了图像重要的结构属性。</p><p>图像的边缘指的是图像中像素灰度值突然发生变化的区域，如果将图像的每一行像素和每一列像素都描述成一个关于灰度值的函数，那么图像的边缘对应在灰度值函数中是函数值突然变大的区域。函数值的变化趋势可以用函数的导数描述。当函数值突然变大时，导数也必然会变大，而函数值变化较为平缓区域，导数值也比较小，因此可以通过寻找导数值较大的区域去寻找函数中突然变化的区域，进而确定图像中的边缘位置。<a id="more"></a></p><h1 id="2-边缘检测的常用方法及Python应用"><a href="#2-边缘检测的常用方法及Python应用" class="headerlink" title="2. 边缘检测的常用方法及Python应用"></a>2. 边缘检测的常用方法及Python应用</h1><p>边缘检测的方法大致可分为两类：基于搜索和基于零交叉。</p><p>基于搜索的边缘检测方法：首先计算边缘强度，通常用一阶导数表示，例如梯度模，然后，计算估计边缘的局部方向，通常采用梯度的方向，并利用此方向找到局部梯度模的最大值。</p><p>基于零交叉的边缘检测方法：找到由图像得到的二阶导数的零交叉点来定位边缘，通常用拉普拉斯算子或非线性微分方程的零交叉点。</p><p>滤波作为边缘检测的预处理通常是必要的，通常采用高斯滤波。</p><h2 id="2-1-一阶微分算子"><a href="#2-1-一阶微分算子" class="headerlink" title="2.1 一阶微分算子"></a>2.1 一阶微分算子</h2><p>一阶微分为基础的边缘检测，通过计算图像的梯度值来检测图像的边缘，如Roberts算子、Prewitt算子和Sobel算子等。</p><h3 id="2-1-1-Roberts算子"><a href="#2-1-1-Roberts算子" class="headerlink" title="2.1.1 Roberts算子"></a>2.1.1 Roberts算子</h3><p>Roberts算子是一种最简单的算子，它利用局部差分算子寻找边缘。采用对角线相邻两像素之差近似梯度幅值检测边缘，检测垂直边缘的效果比斜向边缘要好，定位精度高，但对噪声比较敏感，无法抑制噪声的影响。</p><p>Roberts算子是一个2x2的模板，采用的是对角方向相邻的两个像素之差，如下的2个卷积核形成了Roberts算子，图像中的每一个点都用这2个核做卷积：</p><p><img src="/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/2.1.1-1.png" alt></p><p>若对于输入图像f(x,y),使用Roberts算子后输出的目标图像为g(x,y),则</p><p> <img src="/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/2.1.1-2.png" alt></p><p>在Python中，Roberts算子主要是通过Numpy定义模板，再调用OpenCV的filter2D()函数实现边缘提取。该函数主要是利用内核实现对图像的卷积运算，其函数原型如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dst = filter2D(src, ddepth, kernel, dts, anchor,delta, borderType)</span><br></pre></td></tr></table></figure><p>参数说明：<br>src：表示输入图像；<br>ddepth: 表示目标图像所需的深度；<br>kernel: 表示卷积核,一个单通道浮点型矩阵；<br>anchor： 表示内核的基准点，其默认值为(-1, -1)，位于中心位置；<br>delta：表示在存储目标图像前可选的添加到像素的值，默认值为0；<br>borderType：表示边框模式。</p><p> 实验代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Roberts</span><span class="params">(srcImg_path)</span>:</span></span><br><span class="line">   img = cv2.imread(srcImg_path)</span><br><span class="line">   img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">   grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">   <span class="comment"># Roberts算子</span></span><br><span class="line">   kernelx = np.array([[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">-1</span>]], dtype=int)</span><br><span class="line">   kernely = np.array([[<span class="number">0</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">0</span>]], dtype=int)</span><br><span class="line">   x = cv2.filter2D(grayImage, cv2.CV_16S, kernelx)</span><br><span class="line">   y = cv2.filter2D(grayImage, cv2.CV_16S, kernely)</span><br><span class="line">   <span class="comment"># 转成uint8</span></span><br><span class="line">   absX = cv2.convertScaleAbs(x)</span><br><span class="line">   absY = cv2.convertScaleAbs(y)</span><br><span class="line">   Roberts = cv2.addWeighted(absX, <span class="number">0.5</span>, absY, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">   <span class="comment"># 显示图形</span></span><br><span class="line">   titles = [<span class="string">"Original Image"</span>, <span class="string">"Roberts Image"</span>]</span><br><span class="line">   images = [img, Roberts]</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">       plt.subplot(<span class="number">1</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">       plt.imshow(images[i], <span class="string">"gray"</span>)</span><br><span class="line">       plt.title(titles[i])</span><br><span class="line">       plt.axis(<span class="string">'off'</span>)</span><br><span class="line">   plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p> <img src="/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/2.1.1-3.png" alt></p><h3 id="2-1-2-Prewitt算子"><a href="#2-1-2-Prewitt算子" class="headerlink" title="2.1.2 Prewitt算子"></a>2.1.2 Prewitt算子</h3><p>Prewitt是一种图像边缘检测的微分算子，其原理是利用特定区域内像素值产生的差分实现边缘检测。由于Prewitt算子采用3x3模板对区域内的像素值进行计算，而Roberts算子的模板为2x2，故Prewitt算子的边缘检测结果在水平和垂直方向均比Roberts算子更加明显。Prewitt算子适合用来识别噪声较多，灰度渐变的图像。</p><p>Prewitt算子卷积核如下：</p><p> <img src="/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/2.1.2-1.png" alt></p><p> 实验代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Prewitt</span><span class="params">(srcImg_path)</span>:</span></span><br><span class="line">   img = cv2.imread(srcImg_path)</span><br><span class="line">   img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">   grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">   <span class="comment"># Prewitt算子</span></span><br><span class="line">   kernelx = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>]], dtype=int)</span><br><span class="line">   kernely = np.array([[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>]], dtype=int)</span><br><span class="line">   x = cv2.filter2D(grayImage, cv2.CV_16S, kernelx)</span><br><span class="line">   y = cv2.filter2D(grayImage, cv2.CV_16S, kernely)</span><br><span class="line">   <span class="comment"># 转成uint8</span></span><br><span class="line">   absX = cv2.convertScaleAbs(x)</span><br><span class="line">   absY = cv2.convertScaleAbs(y)</span><br><span class="line">   Prewitt = cv2.addWeighted(absX, <span class="number">0.5</span>, absY, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">   <span class="comment"># 显示图形</span></span><br><span class="line">   titles = [<span class="string">"Original Image"</span>, <span class="string">"Prewitt Image"</span>]</span><br><span class="line">   images = [img, Prewitt]</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">       plt.subplot(<span class="number">1</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">       plt.imshow(images[i], <span class="string">"gray"</span>)</span><br><span class="line">       plt.title(titles[i])</span><br><span class="line">       plt.axis(<span class="string">'off'</span>)</span><br><span class="line">   plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p>  <img src="/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/2.1.2-2.png" alt></p><h3 id="2-1-3-Sobel算子"><a href="#2-1-3-Sobel算子" class="headerlink" title="2.1.3 Sobel算子"></a>2.1.3 Sobel算子</h3><p>在边缘检测中，常用的一种模板是Sobel算子。Sobel算子有两个卷积核，一个是检测水平边缘的；另一个是检测垂直边缘的。与Prewitt算子相比，Sobel算子对于像素的位置的影响做了加权，可以降低边缘模糊程度，因此效果更好。</p><p>Sobel算子卷积核如下：</p><p> <img src="/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/2.1.3-1.png" alt></p><p> 在opencv-python中定义了Sobel算子，其函数原型如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dst = Sobel(src, ddepth, dx, dy, dst,ksize, scale, delta, borderType)</span><br></pre></td></tr></table></figure><p>参数说明：<br>src：表示输入图像；<br>dst：表示输出的边缘图，其大小和通道数与输入图像相同；<br>ddepth：表示目标图像所需的深度，针对不同的输入图像，输出目标图像有不同的深度；<br>dx：表示x方向上的差分阶数，取值1或0；<br>dy：表示y方向上的差分阶数，取值1或0；<br>ksize：表示Sobel算子的大小，其值必须是正数和奇数；<br>scale：表示缩放导数的比例常数，默认情况下没有伸缩系数；<br>delta：表示将结果存入目标图像之前，添加到结果中的可选增量值。</p><p> 实验代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Sobel_demo</span><span class="params">(srcImg_path)</span>:</span></span><br><span class="line">   img = cv2.imread(srcImg_path)</span><br><span class="line">   img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">   grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">   <span class="comment"># Sobel算子</span></span><br><span class="line">   x = cv2.Sobel(grayImage, cv2.CV_16S, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">   y = cv2.Sobel(grayImage, cv2.CV_16S, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">   <span class="comment"># 转成uint8</span></span><br><span class="line">   absX = cv2.convertScaleAbs(x)</span><br><span class="line">   absY = cv2.convertScaleAbs(y)</span><br><span class="line">   Sobel = cv2.addWeighted(absX, <span class="number">0.5</span>, absY, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">   <span class="comment"># 显示图形</span></span><br><span class="line">   titles = [<span class="string">"Original Image"</span>, <span class="string">"Sobel Image"</span>]</span><br><span class="line">   images = [img, Sobel]</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">       plt.subplot(<span class="number">1</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">       plt.imshow(images[i], <span class="string">"gray"</span>)</span><br><span class="line">       plt.title(titles[i])</span><br><span class="line">       plt.axis(<span class="string">'off'</span>)</span><br><span class="line">   plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p> <img src="/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/2.1.3-2.png" alt></p><h2 id="2-2-二阶微分算子"><a href="#2-2-二阶微分算子" class="headerlink" title="2.2 二阶微分算子"></a>2.2 二阶微分算子</h2><p>二阶微分为基础的边缘检测，通过寻求二阶导数中的过零点来检测边缘，如Laplacian算子和Canny算子等。</p><h3 id="2-2-1-Laplacian算子"><a href="#2-2-1-Laplacian算子" class="headerlink" title="2.2.1 Laplacian算子"></a>2.2.1 Laplacian算子</h3><p>Laplacian算子是n维欧几里德空间中的一个二阶微分算子，常用于图像增强和边缘提取。它通过灰度差分计算邻域内的像素，基本流程是：判断图像中心像素灰度值与它周围其他像素的灰度值，如果中心像素的灰度更高，则提升中心像素的灰度；反之降低中心像素的灰度，从而实现图像锐化操作。在算法实现过程中，Laplacian算子通过对邻域中心像素的四方向或八方向求梯度，再将梯度相加起来判断中心像素灰度与邻域内其他像素灰度的关系，最后通过梯度运算的结果对像素灰度进行调整。</p><p>在opencv-python中，Laplacian算子封装在Laplacian()函数中，其函数原型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dst = Laplacian(src, ddepth[, dst[, ksize[, scale[, delta[, borderType]]]]])</span><br></pre></td></tr></table></figure><p>参数说明：<br>src：表示输入图像；<br>dst：表示输出的边缘图，其大小和通道数与输入图像相同；<br>ddepth：表示目标图像所需的深度；<br>ksize：表示用于计算二阶导数的滤波器的孔径大小，其值必须是正数和奇数，且默认值为1；<br>scale：表示计算拉普拉斯算子值的可选比例因子，默认值为1；<br>delta：表示将结果存入目标图像之前，添加到结果中的可选增量值，默认值为0；<br>borderType：表示边框模式。</p><p> 当ksize=1时，Laplacian()函数采用3x3模板（四邻域）进行变换处理。下面的实验代码是采用ksize=3的Laplacian算子进行图像锐化处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Laplacian_demo</span><span class="params">(srcImg_path)</span>:</span></span><br><span class="line">    img = cv2.imread(srcImg_path)</span><br><span class="line">    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">    grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">    <span class="comment"># Laplacian算子</span></span><br><span class="line">    Laplacian = cv2.Laplacian(grayImage, cv2.CV_16S, ksize=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 转成uint8</span></span><br><span class="line">    Laplacian = cv2.convertScaleAbs(Laplacian)</span><br><span class="line">    <span class="comment"># 显示图形</span></span><br><span class="line">    titles = [<span class="string">"Original Image"</span>, <span class="string">"Laplacian Image"</span>]</span><br><span class="line">    images = [img, Laplacian]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">        plt.imshow(images[i], <span class="string">"gray"</span>)</span><br><span class="line">        plt.title(titles[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p> 效果如下：</p><p>  <img src="/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/2.2.1-1.png" alt></p><h3 id="2-2-2-Canny算子"><a href="#2-2-2-Canny算子" class="headerlink" title="2.2.2 Canny算子"></a>2.2.2 Canny算子</h3><p>Canny算子由John F. Canny在1986年提出，由于它出色的检测和容错能力，至今一直被广泛使用。Canny边缘检测具有以下特点：<br>较低的错误率 - 只有真实存在的边缘才会被检测到。<br>较好的边缘定位 - 检测出来的结果和图像中真实的边缘在距离上的误差很小。<br>没有重复的检测 - 对于每一条边缘，只会返回一个与之对应的结果。</p><p> Canny算子的计算步骤大概分成以下几步：</p><p>1.图像灰度化<br>2.用高斯滤波去噪：目的是平滑一些纹理较弱的非边缘区域，以得到更准确的边缘。<br>3.计算梯度方向和大小：图像梯度表达的是各个像素点之间，像素值大小的变化幅度大小，变化较大，则可以认为是处于边缘位置。<br>4.非极大值抑制：在获得梯度的方向和大小之后，应该对整幅图像做一个扫描，去除那些非边界上的点，即对每一个像素进行检查，看这个点的梯度是不是周围具有相同梯度方向的点中最大的。<br>5.双阈值选取和滞后边界跟踪：确定哪些边界才是真正的边界。这时我们需要设置两个阈值：minVal和maxVal。当图像的灰度梯度高于maxVal时被认为是真的边界，那些低于minVal的边界会被抛弃。如果介于两者之间的话，就要看这个点是否与某个被确定为真正的边界点相连，如果是就认为它也是边界点，如果不是就抛弃。</p><p> 在Python Opencv接口中，提供了Canny函数，其函数原型如下： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">canny = cv2.Canny(image,threshold1,threshold2)</span><br></pre></td></tr></table></figure><p>参数说明：<br>image：灰度图；<br>threshold1：minval，较小的阈值将间断的边缘连接起来；<br>threshold2：maxval，较大的阈值检测图像中明显的边缘。</p><p> 实验代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Canny_demo</span><span class="params">(srcImg_path)</span>:</span></span><br><span class="line">    img = cv2.imread(srcImg_path)</span><br><span class="line">    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">    <span class="comment"># 高斯滤波</span></span><br><span class="line">    img_GaussianBlur = cv2.GaussianBlur(gray, (<span class="number">3</span>,<span class="number">3</span>), <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># Canny算子</span></span><br><span class="line">    Canny = cv2.Canny(img_GaussianBlur, <span class="number">0</span>, <span class="number">100</span>)</span><br><span class="line">    <span class="comment"># 显示图形</span></span><br><span class="line">    titles = [<span class="string">"Original Image"</span>, <span class="string">"Canny Image"</span>]</span><br><span class="line">    images = [img, Canny]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">        plt.imshow(images[i], <span class="string">"gray"</span>)</span><br><span class="line">        plt.title(titles[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>效果如下：</p><p> <img src="/2023/01/06/%E5%9B%BE%E5%83%8F%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/2.2.2-1.png" alt></p><h1 id="3-源码仓库地址"><a href="#3-源码仓库地址" class="headerlink" title="3. 源码仓库地址"></a>3. 源码仓库地址</h1><p>🌼 <a href="https://github.com/crossoverpptx/ImageProcessing" target="_blank" rel="noopener">图像处理、机器学习的常用算法汇总</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;1-什么是边缘检测&quot;&gt;&lt;a href=&quot;#1-什么是边缘检测&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是边缘检测&quot;&gt;&lt;/a&gt;1. 什么是边缘检测&lt;/h1&gt;&lt;p&gt;边缘检测是图像处理与计算机视觉中的重要技术之一。其目的是检测识别出图像中亮度变化剧烈的像素点构成的集合。图像边缘的正确检测对于分析图像中的内容、实现图像中物体的分割、定位等具有重要的作用。边缘检测大大减少了源图像的数据量，剔除了与目标不相干的信息，保留了图像重要的结构属性。&lt;/p&gt;
&lt;p&gt;图像的边缘指的是图像中像素灰度值突然发生变化的区域，如果将图像的每一行像素和每一列像素都描述成一个关于灰度值的函数，那么图像的边缘对应在灰度值函数中是函数值突然变大的区域。函数值的变化趋势可以用函数的导数描述。当函数值突然变大时，导数也必然会变大，而函数值变化较为平缓区域，导数值也比较小，因此可以通过寻找导数值较大的区域去寻找函数中突然变化的区域，进而确定图像中的边缘位置。&lt;/p&gt;</summary>
    
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像处理" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
</feed>
